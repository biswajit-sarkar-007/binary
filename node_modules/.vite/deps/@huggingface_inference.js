import {
  __publicField
} from "./chunk-2TUXWMP5.js";

// node_modules/@huggingface/tasks/dist/esm/library-to-tasks.js
var LIBRARY_TASK_MAPPING = {
  "adapter-transformers": ["question-answering", "text-classification", "token-classification"],
  allennlp: ["question-answering"],
  asteroid: [
    // "audio-source-separation",
    "audio-to-audio"
  ],
  bertopic: ["text-classification"],
  diffusers: ["image-to-image", "text-to-image"],
  doctr: ["object-detection"],
  espnet: ["text-to-speech", "automatic-speech-recognition"],
  fairseq: ["text-to-speech", "audio-to-audio"],
  fastai: ["image-classification"],
  fasttext: ["feature-extraction", "text-classification"],
  flair: ["token-classification"],
  k2: ["automatic-speech-recognition"],
  keras: ["image-classification"],
  nemo: ["automatic-speech-recognition"],
  open_clip: ["zero-shot-classification", "zero-shot-image-classification"],
  paddlenlp: ["fill-mask", "summarization", "zero-shot-classification"],
  peft: ["text-generation"],
  "pyannote-audio": ["automatic-speech-recognition"],
  "sentence-transformers": ["feature-extraction", "sentence-similarity"],
  setfit: ["text-classification"],
  sklearn: ["tabular-classification", "tabular-regression", "text-classification"],
  spacy: ["token-classification", "text-classification", "sentence-similarity"],
  "span-marker": ["token-classification"],
  speechbrain: [
    "audio-classification",
    "audio-to-audio",
    "automatic-speech-recognition",
    "text-to-speech",
    "text2text-generation"
  ],
  stanza: ["token-classification"],
  timm: ["image-classification", "image-feature-extraction"],
  transformers: [
    "audio-classification",
    "automatic-speech-recognition",
    "depth-estimation",
    "document-question-answering",
    "feature-extraction",
    "fill-mask",
    "image-classification",
    "image-feature-extraction",
    "image-segmentation",
    "image-to-image",
    "image-to-text",
    "image-text-to-text",
    "mask-generation",
    "object-detection",
    "question-answering",
    "summarization",
    "table-question-answering",
    "text2text-generation",
    "text-classification",
    "text-generation",
    "text-to-audio",
    "text-to-speech",
    "token-classification",
    "translation",
    "video-classification",
    "visual-question-answering",
    "zero-shot-classification",
    "zero-shot-image-classification",
    "zero-shot-object-detection"
  ],
  mindspore: ["image-classification"]
};

// node_modules/@huggingface/tasks/dist/esm/default-widget-inputs.js
var MAPPING_FA = /* @__PURE__ */ new Map([
  [
    "text-classification",
    [`پروژه به موقع تحویل شد و همه چیز خوب بود.`, `سیب‌زمینی بی‌کیفیت بود.`, `قیمت و کیفیت عالی`, `خوب نبود اصلا`]
  ],
  [
    "token-classification",
    [
      `این سریال به صورت رسمی در تاریخ دهم می ۲۰۱۱ توسط شبکه فاکس برای پخش رزرو شد.`,
      `دفتر مرکزی شرکت پارس‌مینو در شهر اراک در استان مرکزی قرار دارد.`,
      `وی در سال ۲۰۱۳ درگذشت و مسئول خاکسپاری و اقوامش برای او مراسم یادبود گرفتند.`
    ]
  ],
  [
    "question-answering",
    [
      {
        text: `من کجا زندگی میکنم؟`,
        context: `نام من پژمان است و در گرگان زندگی میکنم.`
      },
      {
        text: `نامم چیست و کجا زندگی می‌کنم؟`,
        context: `اسمم سارا است و در آفریقای جنوبی زندگی میکنم.`
      },
      {
        text: `نام من چیست؟`,
        context: `من مریم هستم و در تبریز زندگی می‌کنم.`
      },
      {
        text: `بیشترین مساحت جنگل آمازون در کدام کشور است؟`,
        context: [
          "آمازون نام بزرگ‌ترین جنگل بارانی جهان است که در شمال آمریکای جنوبی قرار گرفته و بیشتر آن در خاک برزیل و پرو",
          "جای دارد. بیش از نیمی از همه جنگل‌های بارانی باقی‌مانده در جهان در آمازون قرار دارد.",
          "مساحت جنگل‌های آمازون ۵٫۵ میلیون کیلومتر مربع است که بین ۹ کشور تقسیم شده‌است."
        ].join("\n")
      }
    ]
  ],
  [
    "translation",
    [
      "بیشتر مساحت جنگل‌های آمازون در حوضه آبریز رود آمازون و ۱۱۰۰ شاخه آن واقع شده‌است.",
      "مردمان نَبَطی از هزاره‌های یکم و دوم پیش از میلاد در این منطقه زندگی می‌کردند."
    ]
  ],
  [
    "summarization",
    [
      [
        "شاهنامه اثر حکیم ابوالقاسم فردوسی توسی، حماسه‌ای منظوم، بر حسب دست نوشته‌های ",
        "موجود دربرگیرنده نزدیک به ۵۰٬۰۰۰ بیت تا نزدیک به ۶۱٬۰۰۰ بیت و یکی از ",
        "بزرگ‌ترین و برجسته‌ترین سروده‌های حماسی جهان است که سرایش آن دست‌آوردِ ",
        "دست‌کم سی سال کارِ پیوستهٔ این سخن‌سرای نامدار ایرانی است. موضوع این شاهکار ادبی،",
        " افسانه‌ها و تاریخ ایران از آغاز تا حملهٔ عرب‌ها به ایران در سدهٔ هفتم میلادی است",
        "  (شاهنامه از سه بخش اسطوره، پهلوانی و تاریخی تشکیل شده‌است) که در چهار",
        "   دودمان پادشاهیِ پیشدادیان، کیانیان، اشکانیان و ساسانیان گنجانده می‌شود.",
        "    شاهنامه بر وزن «فَعولُن فعولن فعولن فَعَلْ»، در بحرِ مُتَقارِبِ مثمَّنِ محذوف نگاشته شده‌است.",
        "هنگامی که زبان دانش و ادبیات در ایران زبان عربی بود، فردوسی، با سرودن شاهنامه",
        " با ویژگی‌های هدف‌مندی که داشت، زبان پارسی را زنده و پایدار کرد. یکی از ",
        " بن‌مایه‌های مهمی که فردوسی برای سرودن شاهنامه از آن استفاده کرد،",
        "  شاهنامهٔ ابومنصوری بود. شاهنامه نفوذ بسیاری در جهت‌گیری ",
        "  فرهنگ فارسی و نیز بازتاب‌های شکوه‌مندی در ادبیات جهان داشته‌است و شاعران ",
        "  بزرگی مانند گوته و ویکتور هوگو از آن به نیکی یاد کرده‌اند."
      ].join("\n")
    ]
  ],
  ["text-generation", ["اسم من نازنین است و من", "روزی روزگاری"]],
  [
    "fill-mask",
    [
      `زندگی یک سوال است و این که چگونه <mask> کنیم پاسخ این سوال!`,
      `زندگی از مرگ پرسید: چرا همه من را <mask> دارند اما از تو متنفرند؟`
    ]
  ]
]);

// node_modules/@huggingface/tasks/dist/esm/pipelines.js
var PIPELINE_DATA = {
  "text-classification": {
    name: "Text Classification",
    subtasks: [
      {
        type: "acceptability-classification",
        name: "Acceptability Classification"
      },
      {
        type: "entity-linking-classification",
        name: "Entity Linking Classification"
      },
      {
        type: "fact-checking",
        name: "Fact Checking"
      },
      {
        type: "intent-classification",
        name: "Intent Classification"
      },
      {
        type: "language-identification",
        name: "Language Identification"
      },
      {
        type: "multi-class-classification",
        name: "Multi Class Classification"
      },
      {
        type: "multi-label-classification",
        name: "Multi Label Classification"
      },
      {
        type: "multi-input-text-classification",
        name: "Multi-input Text Classification"
      },
      {
        type: "natural-language-inference",
        name: "Natural Language Inference"
      },
      {
        type: "semantic-similarity-classification",
        name: "Semantic Similarity Classification"
      },
      {
        type: "sentiment-classification",
        name: "Sentiment Classification"
      },
      {
        type: "topic-classification",
        name: "Topic Classification"
      },
      {
        type: "semantic-similarity-scoring",
        name: "Semantic Similarity Scoring"
      },
      {
        type: "sentiment-scoring",
        name: "Sentiment Scoring"
      },
      {
        type: "sentiment-analysis",
        name: "Sentiment Analysis"
      },
      {
        type: "hate-speech-detection",
        name: "Hate Speech Detection"
      },
      {
        type: "text-scoring",
        name: "Text Scoring"
      }
    ],
    modality: "nlp",
    color: "orange"
  },
  "token-classification": {
    name: "Token Classification",
    subtasks: [
      {
        type: "named-entity-recognition",
        name: "Named Entity Recognition"
      },
      {
        type: "part-of-speech",
        name: "Part of Speech"
      },
      {
        type: "parsing",
        name: "Parsing"
      },
      {
        type: "lemmatization",
        name: "Lemmatization"
      },
      {
        type: "word-sense-disambiguation",
        name: "Word Sense Disambiguation"
      },
      {
        type: "coreference-resolution",
        name: "Coreference-resolution"
      }
    ],
    modality: "nlp",
    color: "blue"
  },
  "table-question-answering": {
    name: "Table Question Answering",
    modality: "nlp",
    color: "green"
  },
  "question-answering": {
    name: "Question Answering",
    subtasks: [
      {
        type: "extractive-qa",
        name: "Extractive QA"
      },
      {
        type: "open-domain-qa",
        name: "Open Domain QA"
      },
      {
        type: "closed-domain-qa",
        name: "Closed Domain QA"
      }
    ],
    modality: "nlp",
    color: "blue"
  },
  "zero-shot-classification": {
    name: "Zero-Shot Classification",
    modality: "nlp",
    color: "yellow"
  },
  translation: {
    name: "Translation",
    modality: "nlp",
    color: "green"
  },
  summarization: {
    name: "Summarization",
    subtasks: [
      {
        type: "news-articles-summarization",
        name: "News Articles Summarization"
      },
      {
        type: "news-articles-headline-generation",
        name: "News Articles Headline Generation"
      }
    ],
    modality: "nlp",
    color: "indigo"
  },
  "feature-extraction": {
    name: "Feature Extraction",
    modality: "nlp",
    color: "red"
  },
  "text-generation": {
    name: "Text Generation",
    subtasks: [
      {
        type: "dialogue-modeling",
        name: "Dialogue Modeling"
      },
      {
        type: "dialogue-generation",
        name: "Dialogue Generation"
      },
      {
        type: "conversational",
        name: "Conversational"
      },
      {
        type: "language-modeling",
        name: "Language Modeling"
      }
    ],
    modality: "nlp",
    color: "indigo"
  },
  "text2text-generation": {
    name: "Text2Text Generation",
    subtasks: [
      {
        type: "text-simplification",
        name: "Text simplification"
      },
      {
        type: "explanation-generation",
        name: "Explanation Generation"
      },
      {
        type: "abstractive-qa",
        name: "Abstractive QA"
      },
      {
        type: "open-domain-abstractive-qa",
        name: "Open Domain Abstractive QA"
      },
      {
        type: "closed-domain-qa",
        name: "Closed Domain QA"
      },
      {
        type: "open-book-qa",
        name: "Open Book QA"
      },
      {
        type: "closed-book-qa",
        name: "Closed Book QA"
      }
    ],
    modality: "nlp",
    color: "indigo"
  },
  "fill-mask": {
    name: "Fill-Mask",
    subtasks: [
      {
        type: "slot-filling",
        name: "Slot Filling"
      },
      {
        type: "masked-language-modeling",
        name: "Masked Language Modeling"
      }
    ],
    modality: "nlp",
    color: "red"
  },
  "sentence-similarity": {
    name: "Sentence Similarity",
    modality: "nlp",
    color: "yellow"
  },
  "text-to-speech": {
    name: "Text-to-Speech",
    modality: "audio",
    color: "yellow"
  },
  "text-to-audio": {
    name: "Text-to-Audio",
    modality: "audio",
    color: "yellow"
  },
  "automatic-speech-recognition": {
    name: "Automatic Speech Recognition",
    modality: "audio",
    color: "yellow"
  },
  "audio-to-audio": {
    name: "Audio-to-Audio",
    modality: "audio",
    color: "blue"
  },
  "audio-classification": {
    name: "Audio Classification",
    subtasks: [
      {
        type: "keyword-spotting",
        name: "Keyword Spotting"
      },
      {
        type: "speaker-identification",
        name: "Speaker Identification"
      },
      {
        type: "audio-intent-classification",
        name: "Audio Intent Classification"
      },
      {
        type: "audio-emotion-recognition",
        name: "Audio Emotion Recognition"
      },
      {
        type: "audio-language-identification",
        name: "Audio Language Identification"
      }
    ],
    modality: "audio",
    color: "green"
  },
  "audio-text-to-text": {
    name: "Audio-Text-to-Text",
    modality: "multimodal",
    color: "red",
    hideInDatasets: true
  },
  "voice-activity-detection": {
    name: "Voice Activity Detection",
    modality: "audio",
    color: "red"
  },
  "depth-estimation": {
    name: "Depth Estimation",
    modality: "cv",
    color: "yellow"
  },
  "image-classification": {
    name: "Image Classification",
    subtasks: [
      {
        type: "multi-label-image-classification",
        name: "Multi Label Image Classification"
      },
      {
        type: "multi-class-image-classification",
        name: "Multi Class Image Classification"
      }
    ],
    modality: "cv",
    color: "blue"
  },
  "object-detection": {
    name: "Object Detection",
    subtasks: [
      {
        type: "face-detection",
        name: "Face Detection"
      },
      {
        type: "vehicle-detection",
        name: "Vehicle Detection"
      }
    ],
    modality: "cv",
    color: "yellow"
  },
  "image-segmentation": {
    name: "Image Segmentation",
    subtasks: [
      {
        type: "instance-segmentation",
        name: "Instance Segmentation"
      },
      {
        type: "semantic-segmentation",
        name: "Semantic Segmentation"
      },
      {
        type: "panoptic-segmentation",
        name: "Panoptic Segmentation"
      }
    ],
    modality: "cv",
    color: "green"
  },
  "text-to-image": {
    name: "Text-to-Image",
    modality: "cv",
    color: "yellow"
  },
  "image-to-text": {
    name: "Image-to-Text",
    subtasks: [
      {
        type: "image-captioning",
        name: "Image Captioning"
      }
    ],
    modality: "cv",
    color: "red"
  },
  "image-to-image": {
    name: "Image-to-Image",
    subtasks: [
      {
        type: "image-inpainting",
        name: "Image Inpainting"
      },
      {
        type: "image-colorization",
        name: "Image Colorization"
      },
      {
        type: "super-resolution",
        name: "Super Resolution"
      }
    ],
    modality: "cv",
    color: "indigo"
  },
  "image-to-video": {
    name: "Image-to-Video",
    modality: "cv",
    color: "indigo"
  },
  "unconditional-image-generation": {
    name: "Unconditional Image Generation",
    modality: "cv",
    color: "green"
  },
  "video-classification": {
    name: "Video Classification",
    modality: "cv",
    color: "blue"
  },
  "reinforcement-learning": {
    name: "Reinforcement Learning",
    modality: "rl",
    color: "red"
  },
  robotics: {
    name: "Robotics",
    modality: "rl",
    subtasks: [
      {
        type: "grasping",
        name: "Grasping"
      },
      {
        type: "task-planning",
        name: "Task Planning"
      }
    ],
    color: "blue"
  },
  "tabular-classification": {
    name: "Tabular Classification",
    modality: "tabular",
    subtasks: [
      {
        type: "tabular-multi-class-classification",
        name: "Tabular Multi Class Classification"
      },
      {
        type: "tabular-multi-label-classification",
        name: "Tabular Multi Label Classification"
      }
    ],
    color: "blue"
  },
  "tabular-regression": {
    name: "Tabular Regression",
    modality: "tabular",
    subtasks: [
      {
        type: "tabular-single-column-regression",
        name: "Tabular Single Column Regression"
      }
    ],
    color: "blue"
  },
  "tabular-to-text": {
    name: "Tabular to Text",
    modality: "tabular",
    subtasks: [
      {
        type: "rdf-to-text",
        name: "RDF to text"
      }
    ],
    color: "blue",
    hideInModels: true
  },
  "table-to-text": {
    name: "Table to Text",
    modality: "nlp",
    color: "blue",
    hideInModels: true
  },
  "multiple-choice": {
    name: "Multiple Choice",
    subtasks: [
      {
        type: "multiple-choice-qa",
        name: "Multiple Choice QA"
      },
      {
        type: "multiple-choice-coreference-resolution",
        name: "Multiple Choice Coreference Resolution"
      }
    ],
    modality: "nlp",
    color: "blue",
    hideInModels: true
  },
  "text-ranking": {
    name: "Text Ranking",
    modality: "nlp",
    color: "red"
  },
  "text-retrieval": {
    name: "Text Retrieval",
    subtasks: [
      {
        type: "document-retrieval",
        name: "Document Retrieval"
      },
      {
        type: "utterance-retrieval",
        name: "Utterance Retrieval"
      },
      {
        type: "entity-linking-retrieval",
        name: "Entity Linking Retrieval"
      },
      {
        type: "fact-checking-retrieval",
        name: "Fact Checking Retrieval"
      }
    ],
    modality: "nlp",
    color: "indigo",
    hideInModels: true
  },
  "time-series-forecasting": {
    name: "Time Series Forecasting",
    modality: "tabular",
    subtasks: [
      {
        type: "univariate-time-series-forecasting",
        name: "Univariate Time Series Forecasting"
      },
      {
        type: "multivariate-time-series-forecasting",
        name: "Multivariate Time Series Forecasting"
      }
    ],
    color: "blue"
  },
  "text-to-video": {
    name: "Text-to-Video",
    modality: "cv",
    color: "green"
  },
  "image-text-to-text": {
    name: "Image-Text-to-Text",
    modality: "multimodal",
    color: "red",
    hideInDatasets: true
  },
  "visual-question-answering": {
    name: "Visual Question Answering",
    subtasks: [
      {
        type: "visual-question-answering",
        name: "Visual Question Answering"
      }
    ],
    modality: "multimodal",
    color: "red"
  },
  "document-question-answering": {
    name: "Document Question Answering",
    subtasks: [
      {
        type: "document-question-answering",
        name: "Document Question Answering"
      }
    ],
    modality: "multimodal",
    color: "blue",
    hideInDatasets: true
  },
  "zero-shot-image-classification": {
    name: "Zero-Shot Image Classification",
    modality: "cv",
    color: "yellow"
  },
  "graph-ml": {
    name: "Graph Machine Learning",
    modality: "other",
    color: "green"
  },
  "mask-generation": {
    name: "Mask Generation",
    modality: "cv",
    color: "indigo"
  },
  "zero-shot-object-detection": {
    name: "Zero-Shot Object Detection",
    modality: "cv",
    color: "yellow"
  },
  "text-to-3d": {
    name: "Text-to-3D",
    modality: "cv",
    color: "yellow"
  },
  "image-to-3d": {
    name: "Image-to-3D",
    modality: "cv",
    color: "green"
  },
  "image-feature-extraction": {
    name: "Image Feature Extraction",
    modality: "cv",
    color: "indigo"
  },
  "video-text-to-text": {
    name: "Video-Text-to-Text",
    modality: "multimodal",
    color: "blue",
    hideInDatasets: false
  },
  "keypoint-detection": {
    name: "Keypoint Detection",
    subtasks: [
      {
        type: "pose-estimation",
        name: "Pose Estimation"
      }
    ],
    modality: "cv",
    color: "red",
    hideInDatasets: true
  },
  "visual-document-retrieval": {
    name: "Visual Document Retrieval",
    modality: "multimodal",
    color: "yellow",
    hideInDatasets: true
  },
  "any-to-any": {
    name: "Any-to-Any",
    modality: "multimodal",
    color: "yellow",
    hideInDatasets: true
  },
  other: {
    name: "Other",
    modality: "other",
    color: "blue",
    hideInModels: true,
    hideInDatasets: true
  }
};
var PIPELINE_TYPES = Object.keys(PIPELINE_DATA);
var SUBTASK_TYPES = Object.values(PIPELINE_DATA).flatMap((data) => "subtasks" in data ? data.subtasks : []).map((s) => s.type);
var PIPELINE_TYPES_SET = new Set(PIPELINE_TYPES);

// node_modules/@huggingface/tasks/dist/esm/tasks/audio-classification/data.js
var taskData = {
  datasets: [
    {
      description: "A benchmark of 10 different audio tasks.",
      id: "s3prl/superb"
    },
    {
      description: "A dataset of YouTube clips and their sound categories.",
      id: "agkphysics/AudioSet"
    }
  ],
  demo: {
    inputs: [
      {
        filename: "audio.wav",
        type: "audio"
      }
    ],
    outputs: [
      {
        data: [
          {
            label: "Up",
            score: 0.2
          },
          {
            label: "Down",
            score: 0.8
          }
        ],
        type: "chart"
      }
    ]
  },
  metrics: [
    {
      description: "",
      id: "accuracy"
    },
    {
      description: "",
      id: "recall"
    },
    {
      description: "",
      id: "precision"
    },
    {
      description: "",
      id: "f1"
    }
  ],
  models: [
    {
      description: "An easy-to-use model for command recognition.",
      id: "speechbrain/google_speech_command_xvector"
    },
    {
      description: "An emotion recognition model.",
      id: "ehcalabres/wav2vec2-lg-xlsr-en-speech-emotion-recognition"
    },
    {
      description: "A language identification model.",
      id: "facebook/mms-lid-126"
    }
  ],
  spaces: [
    {
      description: "An application that can classify music into different genre.",
      id: "kurianbenoy/audioclassification"
    }
  ],
  summary: "Audio classification is the task of assigning a label or class to a given audio. It can be used for recognizing which command a user is giving or the emotion of a statement, as well as identifying a speaker.",
  widgetModels: ["MIT/ast-finetuned-audioset-10-10-0.4593"],
  youtubeId: "KWwzcmG98Ds"
};
var data_default = taskData;

// node_modules/@huggingface/tasks/dist/esm/tasks/audio-to-audio/data.js
var taskData2 = {
  datasets: [
    {
      description: "512-element X-vector embeddings of speakers from CMU ARCTIC dataset.",
      id: "Matthijs/cmu-arctic-xvectors"
    }
  ],
  demo: {
    inputs: [
      {
        filename: "input.wav",
        type: "audio"
      }
    ],
    outputs: [
      {
        filename: "label-0.wav",
        type: "audio"
      },
      {
        filename: "label-1.wav",
        type: "audio"
      }
    ]
  },
  metrics: [
    {
      description: "The Signal-to-Noise ratio is the relationship between the target signal level and the background noise level. It is calculated as the logarithm of the target signal divided by the background noise, in decibels.",
      id: "snri"
    },
    {
      description: "The Signal-to-Distortion ratio is the relationship between the target signal and the sum of noise, interference, and artifact errors",
      id: "sdri"
    }
  ],
  models: [
    {
      description: "A speech enhancement model.",
      id: "ResembleAI/resemble-enhance"
    },
    {
      description: "A model that can change the voice in a speech recording.",
      id: "microsoft/speecht5_vc"
    }
  ],
  spaces: [
    {
      description: "An application for speech separation.",
      id: "younver/speechbrain-speech-separation"
    },
    {
      description: "An application for audio style transfer.",
      id: "nakas/audio-diffusion_style_transfer"
    }
  ],
  summary: "Audio-to-Audio is a family of tasks in which the input is an audio and the output is one or multiple generated audios. Some example tasks are speech enhancement and source separation.",
  widgetModels: ["speechbrain/sepformer-wham"],
  youtubeId: "iohj7nCCYoM"
};
var data_default2 = taskData2;

// node_modules/@huggingface/tasks/dist/esm/tasks/automatic-speech-recognition/data.js
var taskData3 = {
  datasets: [
    {
      description: "31,175 hours of multilingual audio-text dataset in 108 languages.",
      id: "mozilla-foundation/common_voice_17_0"
    },
    {
      description: "Multilingual and diverse audio dataset with 101k hours of audio.",
      id: "amphion/Emilia-Dataset"
    },
    {
      description: "A dataset with 44.6k hours of English speaker data and 6k hours of other language speakers.",
      id: "parler-tts/mls_eng"
    },
    {
      description: "A multilingual audio dataset with 370K hours of audio.",
      id: "espnet/yodas"
    }
  ],
  demo: {
    inputs: [
      {
        filename: "input.flac",
        type: "audio"
      }
    ],
    outputs: [
      {
        /// GOING ALONG SLUSHY COUNTRY ROADS AND SPEAKING TO DAMP AUDIENCES I
        label: "Transcript",
        content: "Going along slushy country roads and speaking to damp audiences in...",
        type: "text"
      }
    ]
  },
  metrics: [
    {
      description: "",
      id: "wer"
    },
    {
      description: "",
      id: "cer"
    }
  ],
  models: [
    {
      description: "A powerful ASR model by OpenAI.",
      id: "openai/whisper-large-v3"
    },
    {
      description: "A good generic speech model by MetaAI for fine-tuning.",
      id: "facebook/w2v-bert-2.0"
    },
    {
      description: "An end-to-end model that performs ASR and Speech Translation by MetaAI.",
      id: "facebook/seamless-m4t-v2-large"
    },
    {
      description: "A powerful multilingual ASR and Speech Translation model by Nvidia.",
      id: "nvidia/canary-1b"
    },
    {
      description: "Powerful speaker diarization model.",
      id: "pyannote/speaker-diarization-3.1"
    }
  ],
  spaces: [
    {
      description: "A powerful general-purpose speech recognition application.",
      id: "hf-audio/whisper-large-v3"
    },
    {
      description: "Latest ASR model from Useful Sensors.",
      id: "mrfakename/Moonshinex"
    },
    {
      description: "A high quality speech and text translation model by Meta.",
      id: "facebook/seamless_m4t"
    },
    {
      description: "A powerful multilingual ASR and Speech Translation model by Nvidia",
      id: "nvidia/canary-1b"
    }
  ],
  summary: "Automatic Speech Recognition (ASR), also known as Speech to Text (STT), is the task of transcribing a given audio to text. It has many applications, such as voice user interfaces.",
  widgetModels: ["openai/whisper-large-v3"],
  youtubeId: "TksaY_FDgnk"
};
var data_default3 = taskData3;

// node_modules/@huggingface/tasks/dist/esm/tasks/document-question-answering/data.js
var taskData4 = {
  datasets: [
    {
      description: "Largest document understanding dataset.",
      id: "HuggingFaceM4/Docmatix"
    },
    {
      description: "Dataset from the 2020 DocVQA challenge. The documents are taken from the UCSF Industry Documents Library.",
      id: "eliolio/docvqa"
    }
  ],
  demo: {
    inputs: [
      {
        label: "Question",
        content: "What is the idea behind the consumer relations efficiency team?",
        type: "text"
      },
      {
        filename: "document-question-answering-input.png",
        type: "img"
      }
    ],
    outputs: [
      {
        label: "Answer",
        content: "Balance cost efficiency with quality customer service",
        type: "text"
      }
    ]
  },
  metrics: [
    {
      description: "The evaluation metric for the DocVQA challenge is the Average Normalized Levenshtein Similarity (ANLS). This metric is flexible to character regognition errors and compares the predicted answer with the ground truth answer.",
      id: "anls"
    },
    {
      description: "Exact Match is a metric based on the strict character match of the predicted answer and the right answer. For answers predicted correctly, the Exact Match will be 1. Even if only one character is different, Exact Match will be 0",
      id: "exact-match"
    }
  ],
  models: [
    {
      description: "A robust document question answering model.",
      id: "impira/layoutlm-document-qa"
    },
    {
      description: "A document question answering model specialized in invoices.",
      id: "impira/layoutlm-invoices"
    },
    {
      description: "A special model for OCR-free document question answering.",
      id: "microsoft/udop-large"
    },
    {
      description: "A powerful model for document question answering.",
      id: "google/pix2struct-docvqa-large"
    }
  ],
  spaces: [
    {
      description: "A robust document question answering application.",
      id: "impira/docquery"
    },
    {
      description: "An application that can answer questions from invoices.",
      id: "impira/invoices"
    },
    {
      description: "An application to compare different document question answering models.",
      id: "merve/compare_docvqa_models"
    }
  ],
  summary: "Document Question Answering (also known as Document Visual Question Answering) is the task of answering questions on document images. Document question answering models take a (document, question) pair as input and return an answer in natural language. Models usually rely on multi-modal features, combining text, position of words (bounding-boxes) and image.",
  widgetModels: ["impira/layoutlm-invoices"],
  youtubeId: ""
};
var data_default4 = taskData4;

// node_modules/@huggingface/tasks/dist/esm/tasks/feature-extraction/data.js
var taskData5 = {
  datasets: [
    {
      description: "Wikipedia dataset containing cleaned articles of all languages. Can be used to train `feature-extraction` models.",
      id: "wikipedia"
    }
  ],
  demo: {
    inputs: [
      {
        label: "Input",
        content: "India, officially the Republic of India, is a country in South Asia.",
        type: "text"
      }
    ],
    outputs: [
      {
        table: [
          ["Dimension 1", "Dimension 2", "Dimension 3"],
          ["2.583383083343506", "2.757075071334839", "0.9023529887199402"],
          ["8.29393482208252", "1.1071064472198486", "2.03399395942688"],
          ["-0.7754912972450256", "-1.647324562072754", "-0.6113331913948059"],
          ["0.07087723910808563", "1.5942802429199219", "1.4610432386398315"]
        ],
        type: "tabular"
      }
    ]
  },
  metrics: [],
  models: [
    {
      description: "A powerful feature extraction model for natural language processing tasks.",
      id: "thenlper/gte-large"
    },
    {
      description: "A strong feature extraction model for retrieval.",
      id: "Alibaba-NLP/gte-Qwen1.5-7B-instruct"
    }
  ],
  spaces: [
    {
      description: "A leaderboard to rank text feature extraction models based on a benchmark.",
      id: "mteb/leaderboard"
    },
    {
      description: "A leaderboard to rank best feature extraction models based on human feedback.",
      id: "mteb/arena"
    }
  ],
  summary: "Feature extraction is the task of extracting features learnt in a model.",
  widgetModels: ["facebook/bart-base"]
};
var data_default5 = taskData5;

// node_modules/@huggingface/tasks/dist/esm/tasks/fill-mask/data.js
var taskData6 = {
  datasets: [
    {
      description: "A common dataset that is used to train models for many languages.",
      id: "wikipedia"
    },
    {
      description: "A large English dataset with text crawled from the web.",
      id: "c4"
    }
  ],
  demo: {
    inputs: [
      {
        label: "Input",
        content: "The <mask> barked at me",
        type: "text"
      }
    ],
    outputs: [
      {
        type: "chart",
        data: [
          {
            label: "wolf",
            score: 0.487
          },
          {
            label: "dog",
            score: 0.061
          },
          {
            label: "cat",
            score: 0.058
          },
          {
            label: "fox",
            score: 0.047
          },
          {
            label: "squirrel",
            score: 0.025
          }
        ]
      }
    ]
  },
  metrics: [
    {
      description: "Cross Entropy is a metric that calculates the difference between two probability distributions. Each probability distribution is the distribution of predicted words",
      id: "cross_entropy"
    },
    {
      description: "Perplexity is the exponential of the cross-entropy loss. It evaluates the probabilities assigned to the next word by the model. Lower perplexity indicates better performance",
      id: "perplexity"
    }
  ],
  models: [
    {
      description: "State-of-the-art masked language model.",
      id: "answerdotai/ModernBERT-large"
    },
    {
      description: "A multilingual model trained on 100 languages.",
      id: "FacebookAI/xlm-roberta-base"
    }
  ],
  spaces: [],
  summary: "Masked language modeling is the task of masking some of the words in a sentence and predicting which words should replace those masks. These models are useful when we want to get a statistical understanding of the language in which the model is trained in.",
  widgetModels: ["distilroberta-base"],
  youtubeId: "mqElG5QJWUg"
};
var data_default6 = taskData6;

// node_modules/@huggingface/tasks/dist/esm/tasks/image-classification/data.js
var taskData7 = {
  datasets: [
    {
      // TODO write proper description
      description: "Benchmark dataset used for image classification with images that belong to 100 classes.",
      id: "cifar100"
    },
    {
      // TODO write proper description
      description: "Dataset consisting of images of garments.",
      id: "fashion_mnist"
    }
  ],
  demo: {
    inputs: [
      {
        filename: "image-classification-input.jpeg",
        type: "img"
      }
    ],
    outputs: [
      {
        type: "chart",
        data: [
          {
            label: "Egyptian cat",
            score: 0.514
          },
          {
            label: "Tabby cat",
            score: 0.193
          },
          {
            label: "Tiger cat",
            score: 0.068
          }
        ]
      }
    ]
  },
  metrics: [
    {
      description: "",
      id: "accuracy"
    },
    {
      description: "",
      id: "recall"
    },
    {
      description: "",
      id: "precision"
    },
    {
      description: "",
      id: "f1"
    }
  ],
  models: [
    {
      description: "A strong image classification model.",
      id: "google/vit-base-patch16-224"
    },
    {
      description: "A robust image classification model.",
      id: "facebook/deit-base-distilled-patch16-224"
    },
    {
      description: "A strong image classification model.",
      id: "facebook/convnext-large-224"
    }
  ],
  spaces: [
    {
      description: "A leaderboard to evaluate different image classification models.",
      id: "timm/leaderboard"
    }
  ],
  summary: "Image classification is the task of assigning a label or class to an entire image. Images are expected to have only one class for each image. Image classification models take an image as input and return a prediction about which class the image belongs to.",
  widgetModels: ["google/vit-base-patch16-224"],
  youtubeId: "tjAIM7BOYhw"
};
var data_default7 = taskData7;

// node_modules/@huggingface/tasks/dist/esm/tasks/image-feature-extraction/data.js
var taskData8 = {
  datasets: [
    {
      description: "ImageNet-1K is a image classification dataset in which images are used to train image-feature-extraction models.",
      id: "imagenet-1k"
    }
  ],
  demo: {
    inputs: [
      {
        filename: "mask-generation-input.png",
        type: "img"
      }
    ],
    outputs: [
      {
        table: [
          ["Dimension 1", "Dimension 2", "Dimension 3"],
          ["0.21236686408519745", "1.0919708013534546", "0.8512550592422485"],
          ["0.809657871723175", "-0.18544459342956543", "-0.7851548194885254"],
          ["1.3103108406066895", "-0.2479034662246704", "-0.9107287526130676"],
          ["1.8536205291748047", "-0.36419737339019775", "0.09717650711536407"]
        ],
        type: "tabular"
      }
    ]
  },
  metrics: [],
  models: [
    {
      description: "A powerful image feature extraction model.",
      id: "timm/vit_large_patch14_dinov2.lvd142m"
    },
    {
      description: "A strong image feature extraction model.",
      id: "nvidia/MambaVision-T-1K"
    },
    {
      description: "A robust image feature extraction model.",
      id: "facebook/dino-vitb16"
    },
    {
      description: "Cutting-edge image feature extraction model.",
      id: "apple/aimv2-large-patch14-336-distilled"
    },
    {
      description: "Strong image feature extraction model that can be used on images and documents.",
      id: "OpenGVLab/InternViT-6B-448px-V1-2"
    }
  ],
  spaces: [
    {
      description: "A leaderboard to evaluate different image-feature-extraction models on classification performances",
      id: "timm/leaderboard"
    }
  ],
  summary: "Image feature extraction is the task of extracting features learnt in a computer vision model.",
  widgetModels: []
};
var data_default8 = taskData8;

// node_modules/@huggingface/tasks/dist/esm/tasks/image-to-image/data.js
var taskData9 = {
  datasets: [
    {
      description: "Synthetic dataset, for image relighting",
      id: "VIDIT"
    },
    {
      description: "Multiple images of celebrities, used for facial expression translation",
      id: "huggan/CelebA-faces"
    },
    {
      description: "12M image-caption pairs.",
      id: "Spawning/PD12M"
    }
  ],
  demo: {
    inputs: [
      {
        filename: "image-to-image-input.jpeg",
        type: "img"
      }
    ],
    outputs: [
      {
        filename: "image-to-image-output.png",
        type: "img"
      }
    ]
  },
  isPlaceholder: false,
  metrics: [
    {
      description: "Peak Signal to Noise Ratio (PSNR) is an approximation of the human perception, considering the ratio of the absolute intensity with respect to the variations. Measured in dB, a high value indicates a high fidelity.",
      id: "PSNR"
    },
    {
      description: "Structural Similarity Index (SSIM) is a perceptual metric which compares the luminance, contrast and structure of two images. The values of SSIM range between -1 and 1, and higher values indicate closer resemblance to the original image.",
      id: "SSIM"
    },
    {
      description: "Inception Score (IS) is an analysis of the labels predicted by an image classification model when presented with a sample of the generated images.",
      id: "IS"
    }
  ],
  models: [
    {
      description: "An image-to-image model to improve image resolution.",
      id: "fal/AuraSR-v2"
    },
    {
      description: "A model that increases the resolution of an image.",
      id: "keras-io/super-resolution"
    },
    {
      description: "A model for applying edits to images through image controls.",
      id: "Yuanshi/OminiControl"
    },
    {
      description: "A model that generates images based on segments in the input image and the text prompt.",
      id: "mfidabel/controlnet-segment-anything"
    },
    {
      description: "Strong model for inpainting and outpainting.",
      id: "black-forest-labs/FLUX.1-Fill-dev"
    },
    {
      description: "Strong model for image editing using depth maps.",
      id: "black-forest-labs/FLUX.1-Depth-dev-lora"
    }
  ],
  spaces: [
    {
      description: "Image enhancer application for low light.",
      id: "keras-io/low-light-image-enhancement"
    },
    {
      description: "Style transfer application.",
      id: "keras-io/neural-style-transfer"
    },
    {
      description: "An application that generates images based on segment control.",
      id: "mfidabel/controlnet-segment-anything"
    },
    {
      description: "Image generation application that takes image control and text prompt.",
      id: "hysts/ControlNet"
    },
    {
      description: "Colorize any image using this app.",
      id: "ioclab/brightness-controlnet"
    },
    {
      description: "Edit images with instructions.",
      id: "timbrooks/instruct-pix2pix"
    }
  ],
  summary: "Image-to-image is the task of transforming an input image through a variety of possible manipulations and enhancements, such as super-resolution, image inpainting, colorization, and more.",
  widgetModels: ["stabilityai/stable-diffusion-2-inpainting"],
  youtubeId: ""
};
var data_default9 = taskData9;

// node_modules/@huggingface/tasks/dist/esm/tasks/image-to-text/data.js
var taskData10 = {
  datasets: [
    {
      // TODO write proper description
      description: "Dataset from 12M image-text of Reddit",
      id: "red_caps"
    },
    {
      // TODO write proper description
      description: "Dataset from 3.3M images of Google",
      id: "datasets/conceptual_captions"
    }
  ],
  demo: {
    inputs: [
      {
        filename: "savanna.jpg",
        type: "img"
      }
    ],
    outputs: [
      {
        label: "Detailed description",
        content: "a herd of giraffes and zebras grazing in a field",
        type: "text"
      }
    ]
  },
  metrics: [],
  models: [
    {
      description: "A robust image captioning model.",
      id: "Salesforce/blip2-opt-2.7b"
    },
    {
      description: "A powerful and accurate image-to-text model that can also localize concepts in images.",
      id: "microsoft/kosmos-2-patch14-224"
    },
    {
      description: "A strong optical character recognition model.",
      id: "facebook/nougat-base"
    },
    {
      description: "A powerful model that lets you have a conversation with the image.",
      id: "llava-hf/llava-1.5-7b-hf"
    }
  ],
  spaces: [
    {
      description: "An application that compares various image captioning models.",
      id: "nielsr/comparing-captioning-models"
    },
    {
      description: "A robust image captioning application.",
      id: "flax-community/image-captioning"
    },
    {
      description: "An application that transcribes handwritings into text.",
      id: "nielsr/TrOCR-handwritten"
    },
    {
      description: "An application that can caption images and answer questions about a given image.",
      id: "Salesforce/BLIP"
    },
    {
      description: "An application that can caption images and answer questions with a conversational agent.",
      id: "Salesforce/BLIP2"
    },
    {
      description: "An image captioning application that demonstrates the effect of noise on captions.",
      id: "johko/capdec-image-captioning"
    }
  ],
  summary: "Image to text models output a text from a given image. Image captioning or optical character recognition can be considered as the most common applications of image to text.",
  widgetModels: ["Salesforce/blip-image-captioning-large"],
  youtubeId: ""
};
var data_default10 = taskData10;

// node_modules/@huggingface/tasks/dist/esm/tasks/image-text-to-text/data.js
var taskData11 = {
  datasets: [
    {
      description: "Instructions composed of image and text.",
      id: "liuhaotian/LLaVA-Instruct-150K"
    },
    {
      description: "Collection of image-text pairs on scientific topics.",
      id: "DAMO-NLP-SG/multimodal_textbook"
    },
    {
      description: "A collection of datasets made for model fine-tuning.",
      id: "HuggingFaceM4/the_cauldron"
    },
    {
      description: "Screenshots of websites with their HTML/CSS codes.",
      id: "HuggingFaceM4/WebSight"
    }
  ],
  demo: {
    inputs: [
      {
        filename: "image-text-to-text-input.png",
        type: "img"
      },
      {
        label: "Text Prompt",
        content: "Describe the position of the bee in detail.",
        type: "text"
      }
    ],
    outputs: [
      {
        label: "Answer",
        content: "The bee is sitting on a pink flower, surrounded by other flowers. The bee is positioned in the center of the flower, with its head and front legs sticking out.",
        type: "text"
      }
    ]
  },
  metrics: [],
  models: [
    {
      description: "Small and efficient yet powerful vision language model.",
      id: "HuggingFaceTB/SmolVLM-Instruct"
    },
    {
      description: "A screenshot understanding model used to control computers.",
      id: "microsoft/OmniParser-v2.0"
    },
    {
      description: "Cutting-edge vision language model.",
      id: "allenai/Molmo-7B-D-0924"
    },
    {
      description: "Small yet powerful model.",
      id: "vikhyatk/moondream2"
    },
    {
      description: "Strong image-text-to-text model.",
      id: "Qwen/Qwen2.5-VL-7B-Instruct"
    },
    {
      description: "Image-text-to-text model with agentic capabilities.",
      id: "microsoft/Magma-8B"
    },
    {
      description: "Strong image-text-to-text model focused on documents.",
      id: "allenai/olmOCR-7B-0225-preview"
    },
    {
      description: "Small yet strong image-text-to-text model.",
      id: "ibm-granite/granite-vision-3.2-2b"
    }
  ],
  spaces: [
    {
      description: "Leaderboard to evaluate vision language models.",
      id: "opencompass/open_vlm_leaderboard"
    },
    {
      description: "Vision language models arena, where models are ranked by votes of users.",
      id: "WildVision/vision-arena"
    },
    {
      description: "Powerful vision-language model assistant.",
      id: "akhaliq/Molmo-7B-D-0924"
    },
    {
      description: "Powerful vision language assistant that can understand multiple images.",
      id: "HuggingFaceTB/SmolVLM2"
    },
    {
      description: "An application for chatting with an image-text-to-text model.",
      id: "GanymedeNil/Qwen2-VL-7B"
    },
    {
      description: "An application that parses screenshots into actions.",
      id: "showlab/ShowUI"
    },
    {
      description: "An application that detects gaze.",
      id: "moondream/gaze-demo"
    }
  ],
  summary: "Image-text-to-text models take in an image and text prompt and output text. These models are also called vision-language models, or VLMs. The difference from image-to-text models is that these models take an additional text input, not restricting the model to certain use cases like image captioning, and may also be trained to accept a conversation as input.",
  widgetModels: ["Qwen/Qwen2-VL-7B-Instruct"],
  youtubeId: "IoGaGfU1CIg"
};
var data_default11 = taskData11;

// node_modules/@huggingface/tasks/dist/esm/tasks/image-segmentation/data.js
var taskData12 = {
  datasets: [
    {
      description: "Scene segmentation dataset.",
      id: "scene_parse_150"
    }
  ],
  demo: {
    inputs: [
      {
        filename: "image-segmentation-input.jpeg",
        type: "img"
      }
    ],
    outputs: [
      {
        filename: "image-segmentation-output.png",
        type: "img"
      }
    ]
  },
  metrics: [
    {
      description: "Average Precision (AP) is the Area Under the PR Curve (AUC-PR). It is calculated for each semantic class separately",
      id: "Average Precision"
    },
    {
      description: "Mean Average Precision (mAP) is the overall average of the AP values",
      id: "Mean Average Precision"
    },
    {
      description: "Intersection over Union (IoU) is the overlap of segmentation masks. Mean IoU is the average of the IoU of all semantic classes",
      id: "Mean Intersection over Union"
    },
    {
      description: "APα is the Average Precision at the IoU threshold of a α value, for example, AP50 and AP75",
      id: "APα"
    }
  ],
  models: [
    {
      // TO DO: write description
      description: "Solid semantic segmentation model trained on ADE20k.",
      id: "openmmlab/upernet-convnext-small"
    },
    {
      description: "Background removal model.",
      id: "briaai/RMBG-1.4"
    },
    {
      description: "A multipurpose image segmentation model for high resolution images.",
      id: "ZhengPeng7/BiRefNet"
    },
    {
      description: "Powerful human-centric image segmentation model.",
      id: "facebook/sapiens-seg-1b"
    },
    {
      description: "Panoptic segmentation model trained on the COCO (common objects) dataset.",
      id: "facebook/mask2former-swin-large-coco-panoptic"
    }
  ],
  spaces: [
    {
      description: "A semantic segmentation application that can predict unseen instances out of the box.",
      id: "facebook/ov-seg"
    },
    {
      description: "One of the strongest segmentation applications.",
      id: "jbrinkma/segment-anything"
    },
    {
      description: "A human-centric segmentation model.",
      id: "facebook/sapiens-pose"
    },
    {
      description: "An instance segmentation application to predict neuronal cell types from microscopy images.",
      id: "rashmi/sartorius-cell-instance-segmentation"
    },
    {
      description: "An application that segments videos.",
      id: "ArtGAN/Segment-Anything-Video"
    },
    {
      description: "An panoptic segmentation application built for outdoor environments.",
      id: "segments/panoptic-segment-anything"
    }
  ],
  summary: "Image Segmentation divides an image into segments where each pixel in the image is mapped to an object. This task has multiple variants such as instance segmentation, panoptic segmentation and semantic segmentation.",
  widgetModels: ["nvidia/segformer-b0-finetuned-ade-512-512"],
  youtubeId: "dKE8SIt9C-w"
};
var data_default12 = taskData12;

// node_modules/@huggingface/tasks/dist/esm/tasks/mask-generation/data.js
var taskData13 = {
  datasets: [
    {
      description: "Widely used benchmark dataset for multiple Vision tasks.",
      id: "merve/coco2017"
    },
    {
      description: "Medical Imaging dataset of the Human Brain for segmentation and mask generating tasks",
      id: "rocky93/BraTS_segmentation"
    }
  ],
  demo: {
    inputs: [
      {
        filename: "mask-generation-input.png",
        type: "img"
      }
    ],
    outputs: [
      {
        filename: "mask-generation-output.png",
        type: "img"
      }
    ]
  },
  metrics: [
    {
      description: "IoU is used to measure the overlap between predicted mask and the ground truth mask.",
      id: "Intersection over Union (IoU)"
    }
  ],
  models: [
    {
      description: "Small yet powerful mask generation model.",
      id: "Zigeng/SlimSAM-uniform-50"
    },
    {
      description: "Very strong mask generation model.",
      id: "facebook/sam2-hiera-large"
    }
  ],
  spaces: [
    {
      description: "An application that combines a mask generation model with a zero-shot object detection model for text-guided image segmentation.",
      id: "merve/OWLSAM2"
    },
    {
      description: "An application that compares the performance of a large and a small mask generation model.",
      id: "merve/slimsam"
    },
    {
      description: "An application based on an improved mask generation model.",
      id: "SkalskiP/segment-anything-model-2"
    },
    {
      description: "An application to remove objects from videos using mask generation models.",
      id: "SkalskiP/SAM_and_ProPainter"
    }
  ],
  summary: "Mask generation is the task of generating masks that identify a specific object or region of interest in a given image. Masks are often used in segmentation tasks, where they provide a precise way to isolate the object of interest for further processing or analysis.",
  widgetModels: [],
  youtubeId: ""
};
var data_default13 = taskData13;

// node_modules/@huggingface/tasks/dist/esm/tasks/object-detection/data.js
var taskData14 = {
  datasets: [
    {
      description: "Widely used benchmark dataset for multiple vision tasks.",
      id: "merve/coco2017"
    },
    {
      description: "Multi-task computer vision benchmark.",
      id: "merve/pascal-voc"
    }
  ],
  demo: {
    inputs: [
      {
        filename: "object-detection-input.jpg",
        type: "img"
      }
    ],
    outputs: [
      {
        filename: "object-detection-output.jpg",
        type: "img"
      }
    ]
  },
  metrics: [
    {
      description: "The Average Precision (AP) metric is the Area Under the PR Curve (AUC-PR). It is calculated for each class separately",
      id: "Average Precision"
    },
    {
      description: "The Mean Average Precision (mAP) metric is the overall average of the AP values",
      id: "Mean Average Precision"
    },
    {
      description: "The APα metric is the Average Precision at the IoU threshold of a α value, for example, AP50 and AP75",
      id: "APα"
    }
  ],
  models: [
    {
      description: "Solid object detection model pre-trained on the COCO 2017 dataset.",
      id: "facebook/detr-resnet-50"
    },
    {
      description: "Accurate object detection model.",
      id: "IDEA-Research/dab-detr-resnet-50"
    },
    {
      description: "Fast and accurate object detection model.",
      id: "PekingU/rtdetr_v2_r50vd"
    },
    {
      description: "Object detection model for low-lying objects.",
      id: "StephanST/WALDO30"
    }
  ],
  spaces: [
    {
      description: "Leaderboard to compare various object detection models across several metrics.",
      id: "hf-vision/object_detection_leaderboard"
    },
    {
      description: "An application that contains various object detection models to try from.",
      id: "Gradio-Blocks/Object-Detection-With-DETR-and-YOLOS"
    },
    {
      description: "A cutting-edge object detection application.",
      id: "sunsmarterjieleaf/yolov12"
    },
    {
      description: "An object tracking, segmentation and inpainting application.",
      id: "VIPLab/Track-Anything"
    },
    {
      description: "Very fast object tracking application based on object detection.",
      id: "merve/RT-DETR-tracking-coco"
    }
  ],
  summary: "Object Detection models allow users to identify objects of certain defined classes. Object detection models receive an image as input and output the images with bounding boxes and labels on detected objects.",
  widgetModels: ["facebook/detr-resnet-50"],
  youtubeId: "WdAeKSOpxhw"
};
var data_default14 = taskData14;

// node_modules/@huggingface/tasks/dist/esm/tasks/depth-estimation/data.js
var taskData15 = {
  datasets: [
    {
      description: "NYU Depth V2 Dataset: Video dataset containing both RGB and depth sensor data.",
      id: "sayakpaul/nyu_depth_v2"
    },
    {
      description: "Monocular depth estimation benchmark based without noise and errors.",
      id: "depth-anything/DA-2K"
    }
  ],
  demo: {
    inputs: [
      {
        filename: "depth-estimation-input.jpg",
        type: "img"
      }
    ],
    outputs: [
      {
        filename: "depth-estimation-output.png",
        type: "img"
      }
    ]
  },
  metrics: [],
  models: [
    {
      description: "Cutting-edge depth estimation model.",
      id: "depth-anything/Depth-Anything-V2-Large"
    },
    {
      description: "A strong monocular depth estimation model.",
      id: "jingheya/lotus-depth-g-v1-0"
    },
    {
      description: "A depth estimation model that predicts depth in videos.",
      id: "tencent/DepthCrafter"
    },
    {
      description: "A robust depth estimation model.",
      id: "apple/DepthPro-hf"
    }
  ],
  spaces: [
    {
      description: "An application that predicts the depth of an image and then reconstruct the 3D model as voxels.",
      id: "radames/dpt-depth-estimation-3d-voxels"
    },
    {
      description: "An application for bleeding-edge depth estimation.",
      id: "akhaliq/depth-pro"
    },
    {
      description: "An application on cutting-edge depth estimation in videos.",
      id: "tencent/DepthCrafter"
    },
    {
      description: "A human-centric depth estimation application.",
      id: "facebook/sapiens-depth"
    }
  ],
  summary: "Depth estimation is the task of predicting depth of the objects present in an image.",
  widgetModels: [""],
  youtubeId: ""
};
var data_default15 = taskData15;

// node_modules/@huggingface/tasks/dist/esm/tasks/placeholder/data.js
var taskData16 = {
  datasets: [],
  demo: {
    inputs: [],
    outputs: []
  },
  isPlaceholder: true,
  metrics: [],
  models: [],
  spaces: [],
  summary: "",
  widgetModels: [],
  youtubeId: void 0,
  /// If this is a subtask, link to the most general task ID
  /// (eg, text2text-generation is the canonical ID of translation)
  canonicalId: void 0
};
var data_default16 = taskData16;

// node_modules/@huggingface/tasks/dist/esm/tasks/reinforcement-learning/data.js
var taskData17 = {
  datasets: [
    {
      description: "A curation of widely used datasets for Data Driven Deep Reinforcement Learning (D4RL)",
      id: "edbeeching/decision_transformer_gym_replay"
    }
  ],
  demo: {
    inputs: [
      {
        label: "State",
        content: "Red traffic light, pedestrians are about to pass.",
        type: "text"
      }
    ],
    outputs: [
      {
        label: "Action",
        content: "Stop the car.",
        type: "text"
      },
      {
        label: "Next State",
        content: "Yellow light, pedestrians have crossed.",
        type: "text"
      }
    ]
  },
  metrics: [
    {
      description: "Accumulated reward across all time steps discounted by a factor that ranges between 0 and 1 and determines how much the agent optimizes for future relative to immediate rewards. Measures how good is the policy ultimately found by a given algorithm considering uncertainty over the future.",
      id: "Discounted Total Reward"
    },
    {
      description: "Average return obtained after running the policy for a certain number of evaluation episodes. As opposed to total reward, mean reward considers how much reward a given algorithm receives while learning.",
      id: "Mean Reward"
    },
    {
      description: "Measures how good a given algorithm is after a predefined time. Some algorithms may be guaranteed to converge to optimal behavior across many time steps. However, an agent that reaches an acceptable level of optimality after a given time horizon may be preferable to one that ultimately reaches optimality but takes a long time.",
      id: "Level of Performance After Some Time"
    }
  ],
  models: [
    {
      description: "A Reinforcement Learning model trained on expert data from the Gym Hopper environment",
      id: "edbeeching/decision-transformer-gym-hopper-expert"
    },
    {
      description: "A PPO agent playing seals/CartPole-v0 using the stable-baselines3 library and the RL Zoo.",
      id: "HumanCompatibleAI/ppo-seals-CartPole-v0"
    }
  ],
  spaces: [
    {
      description: "An application for a cute puppy agent learning to catch a stick.",
      id: "ThomasSimonini/Huggy"
    },
    {
      description: "An application to play Snowball Fight with a reinforcement learning agent.",
      id: "ThomasSimonini/SnowballFight"
    }
  ],
  summary: "Reinforcement learning is the computational approach of learning from action by interacting with an environment through trial and error and receiving rewards (negative or positive) as feedback",
  widgetModels: [],
  youtubeId: "q0BiUn5LiBc"
};
var data_default17 = taskData17;

// node_modules/@huggingface/tasks/dist/esm/tasks/question-answering/data.js
var taskData18 = {
  datasets: [
    {
      // TODO write proper description
      description: "A famous question answering dataset based on English articles from Wikipedia.",
      id: "squad_v2"
    },
    {
      // TODO write proper description
      description: "A dataset of aggregated anonymized actual queries issued to the Google search engine.",
      id: "natural_questions"
    }
  ],
  demo: {
    inputs: [
      {
        label: "Question",
        content: "Which name is also used to describe the Amazon rainforest in English?",
        type: "text"
      },
      {
        label: "Context",
        content: "The Amazon rainforest, also known in English as Amazonia or the Amazon Jungle",
        type: "text"
      }
    ],
    outputs: [
      {
        label: "Answer",
        content: "Amazonia",
        type: "text"
      }
    ]
  },
  metrics: [
    {
      description: "Exact Match is a metric based on the strict character match of the predicted answer and the right answer. For answers predicted correctly, the Exact Match will be 1. Even if only one character is different, Exact Match will be 0",
      id: "exact-match"
    },
    {
      description: " The F1-Score metric is useful if we value both false positives and false negatives equally. The F1-Score is calculated on each word in the predicted sequence against the correct answer",
      id: "f1"
    }
  ],
  models: [
    {
      description: "A robust baseline model for most question answering domains.",
      id: "deepset/roberta-base-squad2"
    },
    {
      description: "Small yet robust model that can answer questions.",
      id: "distilbert/distilbert-base-cased-distilled-squad"
    },
    {
      description: "A special model that can answer questions from tables.",
      id: "google/tapas-base-finetuned-wtq"
    }
  ],
  spaces: [
    {
      description: "An application that can answer a long question from Wikipedia.",
      id: "deepset/wikipedia-assistant"
    }
  ],
  summary: "Question Answering models can retrieve the answer to a question from a given text, which is useful for searching for an answer in a document. Some question answering models can generate answers without context!",
  widgetModels: ["deepset/roberta-base-squad2"],
  youtubeId: "ajPx5LwJD-I"
};
var data_default18 = taskData18;

// node_modules/@huggingface/tasks/dist/esm/tasks/sentence-similarity/data.js
var taskData19 = {
  datasets: [
    {
      description: "Bing queries with relevant passages from various web sources.",
      id: "microsoft/ms_marco"
    }
  ],
  demo: {
    inputs: [
      {
        label: "Source sentence",
        content: "Machine learning is so easy.",
        type: "text"
      },
      {
        label: "Sentences to compare to",
        content: "Deep learning is so straightforward.",
        type: "text"
      },
      {
        label: "",
        content: "This is so difficult, like rocket science.",
        type: "text"
      },
      {
        label: "",
        content: "I can't believe how much I struggled with this.",
        type: "text"
      }
    ],
    outputs: [
      {
        type: "chart",
        data: [
          {
            label: "Deep learning is so straightforward.",
            score: 0.623
          },
          {
            label: "This is so difficult, like rocket science.",
            score: 0.413
          },
          {
            label: "I can't believe how much I struggled with this.",
            score: 0.256
          }
        ]
      }
    ]
  },
  metrics: [
    {
      description: "Reciprocal Rank is a measure used to rank the relevancy of documents given a set of documents. Reciprocal Rank is the reciprocal of the rank of the document retrieved, meaning, if the rank is 3, the Reciprocal Rank is 0.33. If the rank is 1, the Reciprocal Rank is 1",
      id: "Mean Reciprocal Rank"
    },
    {
      description: "The similarity of the embeddings is evaluated mainly on cosine similarity. It is calculated as the cosine of the angle between two vectors. It is particularly useful when your texts are not the same length",
      id: "Cosine Similarity"
    }
  ],
  models: [
    {
      description: "This model works well for sentences and paragraphs and can be used for clustering/grouping and semantic searches.",
      id: "sentence-transformers/all-mpnet-base-v2"
    },
    {
      description: "A multilingual robust sentence similarity model.",
      id: "BAAI/bge-m3"
    },
    {
      description: "A robust sentence similarity model.",
      id: "HIT-TMG/KaLM-embedding-multilingual-mini-instruct-v1.5"
    }
  ],
  spaces: [
    {
      description: "An application that leverages sentence similarity to answer questions from YouTube videos.",
      id: "Gradio-Blocks/Ask_Questions_To_YouTube_Videos"
    },
    {
      description: "An application that retrieves relevant PubMed abstracts for a given online article which can be used as further references.",
      id: "Gradio-Blocks/pubmed-abstract-retriever"
    },
    {
      description: "An application that leverages sentence similarity to summarize text.",
      id: "nickmuchi/article-text-summarizer"
    },
    {
      description: "A guide that explains how Sentence Transformers can be used for semantic search.",
      id: "sentence-transformers/Sentence_Transformers_for_semantic_search"
    }
  ],
  summary: "Sentence Similarity is the task of determining how similar two texts are. Sentence similarity models convert input texts into vectors (embeddings) that capture semantic information and calculate how close (similar) they are between them. This task is particularly useful for information retrieval and clustering/grouping.",
  widgetModels: ["BAAI/bge-small-en-v1.5"],
  youtubeId: "VCZq5AkbNEU"
};
var data_default19 = taskData19;

// node_modules/@huggingface/tasks/dist/esm/tasks/summarization/data.js
var taskData20 = {
  canonicalId: "text2text-generation",
  datasets: [
    {
      description: "News articles in five different languages along with their summaries. Widely used for benchmarking multilingual summarization models.",
      id: "mlsum"
    },
    {
      description: "English conversations and their summaries. Useful for benchmarking conversational agents.",
      id: "samsum"
    }
  ],
  demo: {
    inputs: [
      {
        label: "Input",
        content: "The tower is 324 metres (1,063 ft) tall, about the same height as an 81-storey building, and the tallest structure in Paris. Its base is square, measuring 125 metres (410 ft) on each side. It was the first structure to reach a height of 300 metres. Excluding transmitters, the Eiffel Tower is the second tallest free-standing structure in France after the Millau Viaduct.",
        type: "text"
      }
    ],
    outputs: [
      {
        label: "Output",
        content: "The tower is 324 metres (1,063 ft) tall, about the same height as an 81-storey building. It was the first structure to reach a height of 300 metres.",
        type: "text"
      }
    ]
  },
  metrics: [
    {
      description: "The generated sequence is compared against its summary, and the overlap of tokens are counted. ROUGE-N refers to overlap of N subsequent tokens, ROUGE-1 refers to overlap of single tokens and ROUGE-2 is the overlap of two subsequent tokens.",
      id: "rouge"
    }
  ],
  models: [
    {
      description: "A strong summarization model trained on English news articles. Excels at generating factual summaries.",
      id: "facebook/bart-large-cnn"
    },
    {
      description: "A summarization model trained on medical articles.",
      id: "Falconsai/medical_summarization"
    }
  ],
  spaces: [
    {
      description: "An application that can summarize long paragraphs.",
      id: "pszemraj/summarize-long-text"
    },
    {
      description: "A much needed summarization application for terms and conditions.",
      id: "ml6team/distilbart-tos-summarizer-tosdr"
    },
    {
      description: "An application that summarizes long documents.",
      id: "pszemraj/document-summarization"
    },
    {
      description: "An application that can detect errors in abstractive summarization.",
      id: "ml6team/post-processing-summarization"
    }
  ],
  summary: "Summarization is the task of producing a shorter version of a document while preserving its important information. Some models can extract text from the original input, while other models can generate entirely new text.",
  widgetModels: ["facebook/bart-large-cnn"],
  youtubeId: "yHnr5Dk2zCI"
};
var data_default20 = taskData20;

// node_modules/@huggingface/tasks/dist/esm/tasks/table-question-answering/data.js
var taskData21 = {
  datasets: [
    {
      description: "The WikiTableQuestions dataset is a large-scale dataset for the task of question answering on semi-structured tables.",
      id: "wikitablequestions"
    },
    {
      description: "WikiSQL is a dataset of 80654 hand-annotated examples of questions and SQL queries distributed across 24241 tables from Wikipedia.",
      id: "wikisql"
    }
  ],
  demo: {
    inputs: [
      {
        table: [
          ["Rank", "Name", "No.of reigns", "Combined days"],
          ["1", "lou Thesz", "3", "3749"],
          ["2", "Ric Flair", "8", "3103"],
          ["3", "Harley Race", "7", "1799"]
        ],
        type: "tabular"
      },
      { label: "Question", content: "What is the number of reigns for Harley Race?", type: "text" }
    ],
    outputs: [{ label: "Result", content: "7", type: "text" }]
  },
  metrics: [
    {
      description: "Checks whether the predicted answer(s) is the same as the ground-truth answer(s).",
      id: "Denotation Accuracy"
    }
  ],
  models: [
    {
      description: "A table question answering model that is capable of neural SQL execution, i.e., employ TAPEX to execute a SQL query on a given table.",
      id: "microsoft/tapex-base"
    },
    {
      description: "A robust table question answering model.",
      id: "google/tapas-base-finetuned-wtq"
    }
  ],
  spaces: [
    {
      description: "An application that answers questions based on table CSV files.",
      id: "katanaml/table-query"
    }
  ],
  summary: "Table Question Answering (Table QA) is the answering a question about an information on a given table.",
  widgetModels: ["google/tapas-base-finetuned-wtq"]
};
var data_default21 = taskData21;

// node_modules/@huggingface/tasks/dist/esm/tasks/tabular-classification/data.js
var taskData22 = {
  datasets: [
    {
      description: "A comprehensive curation of datasets covering all benchmarks.",
      id: "inria-soda/tabular-benchmark"
    }
  ],
  demo: {
    inputs: [
      {
        table: [
          ["Glucose", "Blood Pressure ", "Skin Thickness", "Insulin", "BMI"],
          ["148", "72", "35", "0", "33.6"],
          ["150", "50", "30", "0", "35.1"],
          ["141", "60", "29", "1", "39.2"]
        ],
        type: "tabular"
      }
    ],
    outputs: [
      {
        table: [["Diabetes"], ["1"], ["1"], ["0"]],
        type: "tabular"
      }
    ]
  },
  metrics: [
    {
      description: "",
      id: "accuracy"
    },
    {
      description: "",
      id: "recall"
    },
    {
      description: "",
      id: "precision"
    },
    {
      description: "",
      id: "f1"
    }
  ],
  models: [
    {
      description: "Breast cancer prediction model based on decision trees.",
      id: "scikit-learn/cancer-prediction-trees"
    }
  ],
  spaces: [
    {
      description: "An application that can predict defective products on a production line.",
      id: "scikit-learn/tabular-playground"
    },
    {
      description: "An application that compares various tabular classification techniques on different datasets.",
      id: "scikit-learn/classification"
    }
  ],
  summary: "Tabular classification is the task of classifying a target category (a group) based on set of attributes.",
  widgetModels: ["scikit-learn/tabular-playground"],
  youtubeId: ""
};
var data_default22 = taskData22;

// node_modules/@huggingface/tasks/dist/esm/tasks/tabular-regression/data.js
var taskData23 = {
  datasets: [
    {
      description: "A comprehensive curation of datasets covering all benchmarks.",
      id: "inria-soda/tabular-benchmark"
    }
  ],
  demo: {
    inputs: [
      {
        table: [
          ["Car Name", "Horsepower", "Weight"],
          ["ford torino", "140", "3,449"],
          ["amc hornet", "97", "2,774"],
          ["toyota corolla", "65", "1,773"]
        ],
        type: "tabular"
      }
    ],
    outputs: [
      {
        table: [["MPG (miles per gallon)"], ["17"], ["18"], ["31"]],
        type: "tabular"
      }
    ]
  },
  metrics: [
    {
      description: "",
      id: "mse"
    },
    {
      description: "Coefficient of determination (or R-squared) is a measure of how well the model fits the data. Higher R-squared is considered a better fit.",
      id: "r-squared"
    }
  ],
  models: [
    {
      description: "Fish weight prediction based on length measurements and species.",
      id: "scikit-learn/Fish-Weight"
    }
  ],
  spaces: [
    {
      description: "An application that can predict weight of a fish based on set of attributes.",
      id: "scikit-learn/fish-weight-prediction"
    }
  ],
  summary: "Tabular regression is the task of predicting a numerical value given a set of attributes.",
  widgetModels: ["scikit-learn/Fish-Weight"],
  youtubeId: ""
};
var data_default23 = taskData23;

// node_modules/@huggingface/tasks/dist/esm/tasks/text-to-image/data.js
var taskData24 = {
  datasets: [
    {
      description: "RedCaps is a large-scale dataset of 12M image-text pairs collected from Reddit.",
      id: "red_caps"
    },
    {
      description: "Conceptual Captions is a dataset consisting of ~3.3M images annotated with captions.",
      id: "conceptual_captions"
    },
    {
      description: "12M image-caption pairs.",
      id: "Spawning/PD12M"
    }
  ],
  demo: {
    inputs: [
      {
        label: "Input",
        content: "A city above clouds, pastel colors, Victorian style",
        type: "text"
      }
    ],
    outputs: [
      {
        filename: "image.jpeg",
        type: "img"
      }
    ]
  },
  metrics: [
    {
      description: "The Inception Score (IS) measure assesses diversity and meaningfulness. It uses a generated image sample to predict its label. A higher score signifies more diverse and meaningful images.",
      id: "IS"
    },
    {
      description: "The Fréchet Inception Distance (FID) calculates the distance between distributions between synthetic and real samples. A lower FID score indicates better similarity between the distributions of real and generated images.",
      id: "FID"
    },
    {
      description: "R-precision assesses how the generated image aligns with the provided text description. It uses the generated images as queries to retrieve relevant text descriptions. The top 'r' relevant descriptions are selected and used to calculate R-precision as r/R, where 'R' is the number of ground truth descriptions associated with the generated images. A higher R-precision value indicates a better model.",
      id: "R-Precision"
    }
  ],
  models: [
    {
      description: "One of the most powerful image generation models that can generate realistic outputs.",
      id: "black-forest-labs/FLUX.1-dev"
    },
    {
      description: "A powerful yet fast image generation model.",
      id: "latent-consistency/lcm-lora-sdxl"
    },
    {
      description: "Text-to-image model for photorealistic generation.",
      id: "Kwai-Kolors/Kolors"
    },
    {
      description: "A powerful text-to-image model.",
      id: "stabilityai/stable-diffusion-3-medium-diffusers"
    }
  ],
  spaces: [
    {
      description: "A powerful text-to-image application.",
      id: "stabilityai/stable-diffusion-3-medium"
    },
    {
      description: "A text-to-image application to generate comics.",
      id: "jbilcke-hf/ai-comic-factory"
    },
    {
      description: "An application to match multiple custom image generation models.",
      id: "multimodalart/flux-lora-lab"
    },
    {
      description: "A powerful yet very fast image generation application.",
      id: "latent-consistency/lcm-lora-for-sdxl"
    },
    {
      description: "A gallery to explore various text-to-image models.",
      id: "multimodalart/LoraTheExplorer"
    },
    {
      description: "An application for `text-to-image`, `image-to-image` and image inpainting.",
      id: "ArtGAN/Stable-Diffusion-ControlNet-WebUI"
    },
    {
      description: "An application to generate realistic images given photos of a person and a prompt.",
      id: "InstantX/InstantID"
    }
  ],
  summary: "Text-to-image is the task of generating images from input text. These pipelines can also be used to modify and edit images based on text prompts.",
  widgetModels: ["black-forest-labs/FLUX.1-dev"],
  youtubeId: ""
};
var data_default24 = taskData24;

// node_modules/@huggingface/tasks/dist/esm/tasks/text-to-speech/data.js
var taskData25 = {
  canonicalId: "text-to-audio",
  datasets: [
    {
      description: "10K hours of multi-speaker English dataset.",
      id: "parler-tts/mls_eng_10k"
    },
    {
      description: "Multi-speaker English dataset.",
      id: "mythicinfinity/libritts_r"
    },
    {
      description: "Multi-lingual dataset.",
      id: "facebook/multilingual_librispeech"
    }
  ],
  demo: {
    inputs: [
      {
        label: "Input",
        content: "I love audio models on the Hub!",
        type: "text"
      }
    ],
    outputs: [
      {
        filename: "audio.wav",
        type: "audio"
      }
    ]
  },
  metrics: [
    {
      description: "The Mel Cepstral Distortion (MCD) metric is used to calculate the quality of generated speech.",
      id: "mel cepstral distortion"
    }
  ],
  models: [
    {
      description: "A prompt based, powerful TTS model.",
      id: "parler-tts/parler-tts-large-v1"
    },
    {
      description: "A powerful TTS model that supports English and Chinese.",
      id: "SWivid/F5-TTS"
    },
    {
      description: "A massively multi-lingual TTS model.",
      id: "fishaudio/fish-speech-1.5"
    },
    {
      description: "A powerful TTS model.",
      id: "OuteAI/OuteTTS-0.1-350M"
    },
    {
      description: "Small yet powerful TTS model.",
      id: "hexgrad/Kokoro-82M"
    }
  ],
  spaces: [
    {
      description: "An application for generate high quality speech in different languages.",
      id: "hexgrad/Kokoro-TTS"
    },
    {
      description: "A multilingual text-to-speech application.",
      id: "fishaudio/fish-speech-1"
    },
    {
      description: "An application that generates speech in different styles in English and Chinese.",
      id: "mrfakename/E2-F5-TTS"
    },
    {
      description: "An application that synthesizes emotional speech for diverse speaker prompts.",
      id: "parler-tts/parler-tts-expresso"
    },
    {
      description: "An application that generates podcast episodes.",
      id: "ngxson/kokoro-podcast-generator"
    }
  ],
  summary: "Text-to-Speech (TTS) is the task of generating natural sounding speech given text input. TTS models can be extended to have a single model that generates speech for multiple speakers and multiple languages.",
  widgetModels: ["suno/bark"],
  youtubeId: "NW62DpzJ274"
};
var data_default25 = taskData25;

// node_modules/@huggingface/tasks/dist/esm/tasks/token-classification/data.js
var taskData26 = {
  datasets: [
    {
      description: "A widely used dataset useful to benchmark named entity recognition models.",
      id: "eriktks/conll2003"
    },
    {
      description: "A multilingual dataset of Wikipedia articles annotated for named entity recognition in over 150 different languages.",
      id: "unimelb-nlp/wikiann"
    }
  ],
  demo: {
    inputs: [
      {
        label: "Input",
        content: "My name is Omar and I live in Zürich.",
        type: "text"
      }
    ],
    outputs: [
      {
        text: "My name is Omar and I live in Zürich.",
        tokens: [
          {
            type: "PERSON",
            start: 11,
            end: 15
          },
          {
            type: "GPE",
            start: 30,
            end: 36
          }
        ],
        type: "text-with-tokens"
      }
    ]
  },
  metrics: [
    {
      description: "",
      id: "accuracy"
    },
    {
      description: "",
      id: "recall"
    },
    {
      description: "",
      id: "precision"
    },
    {
      description: "",
      id: "f1"
    }
  ],
  models: [
    {
      description: "A robust performance model to identify people, locations, organizations and names of miscellaneous entities.",
      id: "dslim/bert-base-NER"
    },
    {
      description: "A strong model to identify people, locations, organizations and names in multiple languages.",
      id: "FacebookAI/xlm-roberta-large-finetuned-conll03-english"
    },
    {
      description: "A token classification model specialized on medical entity recognition.",
      id: "blaze999/Medical-NER"
    },
    {
      description: "Flair models are typically the state of the art in named entity recognition tasks.",
      id: "flair/ner-english"
    }
  ],
  spaces: [
    {
      description: "An application that can recognizes entities, extracts noun chunks and recognizes various linguistic features of each token.",
      id: "spacy/gradio_pipeline_visualizer"
    }
  ],
  summary: "Token classification is a natural language understanding task in which a label is assigned to some tokens in a text. Some popular token classification subtasks are Named Entity Recognition (NER) and Part-of-Speech (PoS) tagging. NER models could be trained to identify specific entities in a text, such as dates, individuals and places; and PoS tagging would identify, for example, which words in a text are verbs, nouns, and punctuation marks.",
  widgetModels: ["FacebookAI/xlm-roberta-large-finetuned-conll03-english"],
  youtubeId: "wVHdVlPScxA"
};
var data_default26 = taskData26;

// node_modules/@huggingface/tasks/dist/esm/tasks/translation/data.js
var taskData27 = {
  canonicalId: "text2text-generation",
  datasets: [
    {
      description: "A dataset of copyright-free books translated into 16 different languages.",
      id: "Helsinki-NLP/opus_books"
    },
    {
      description: "An example of translation between programming languages. This dataset consists of functions in Java and C#.",
      id: "google/code_x_glue_cc_code_to_code_trans"
    }
  ],
  demo: {
    inputs: [
      {
        label: "Input",
        content: "My name is Omar and I live in Zürich.",
        type: "text"
      }
    ],
    outputs: [
      {
        label: "Output",
        content: "Mein Name ist Omar und ich wohne in Zürich.",
        type: "text"
      }
    ]
  },
  metrics: [
    {
      description: "BLEU score is calculated by counting the number of shared single or subsequent tokens between the generated sequence and the reference. Subsequent n tokens are called “n-grams”. Unigram refers to a single token while bi-gram refers to token pairs and n-grams refer to n subsequent tokens. The score ranges from 0 to 1, where 1 means the translation perfectly matched and 0 did not match at all",
      id: "bleu"
    },
    {
      description: "",
      id: "sacrebleu"
    }
  ],
  models: [
    {
      description: "Very powerful model that can translate many languages between each other, especially low-resource languages.",
      id: "facebook/nllb-200-1.3B"
    },
    {
      description: "A general-purpose Transformer that can be used to translate from English to German, French, or Romanian.",
      id: "google-t5/t5-base"
    }
  ],
  spaces: [
    {
      description: "An application that can translate between 100 languages.",
      id: "Iker/Translate-100-languages"
    },
    {
      description: "An application that can translate between many languages.",
      id: "Geonmo/nllb-translation-demo"
    }
  ],
  summary: "Translation is the task of converting text from one language to another.",
  widgetModels: ["facebook/mbart-large-50-many-to-many-mmt"],
  youtubeId: "1JvfrvZgi6c"
};
var data_default27 = taskData27;

// node_modules/@huggingface/tasks/dist/esm/tasks/text-classification/data.js
var taskData28 = {
  datasets: [
    {
      description: "A widely used dataset used to benchmark multiple variants of text classification.",
      id: "nyu-mll/glue"
    },
    {
      description: "A text classification dataset used to benchmark natural language inference models",
      id: "stanfordnlp/snli"
    }
  ],
  demo: {
    inputs: [
      {
        label: "Input",
        content: "I love Hugging Face!",
        type: "text"
      }
    ],
    outputs: [
      {
        type: "chart",
        data: [
          {
            label: "POSITIVE",
            score: 0.9
          },
          {
            label: "NEUTRAL",
            score: 0.1
          },
          {
            label: "NEGATIVE",
            score: 0
          }
        ]
      }
    ]
  },
  metrics: [
    {
      description: "",
      id: "accuracy"
    },
    {
      description: "",
      id: "recall"
    },
    {
      description: "",
      id: "precision"
    },
    {
      description: "The F1 metric is the harmonic mean of the precision and recall. It can be calculated as: F1 = 2 * (precision * recall) / (precision + recall)",
      id: "f1"
    }
  ],
  models: [
    {
      description: "A robust model trained for sentiment analysis.",
      id: "distilbert/distilbert-base-uncased-finetuned-sst-2-english"
    },
    {
      description: "A sentiment analysis model specialized in financial sentiment.",
      id: "ProsusAI/finbert"
    },
    {
      description: "A sentiment analysis model specialized in analyzing tweets.",
      id: "cardiffnlp/twitter-roberta-base-sentiment-latest"
    },
    {
      description: "A model that can classify languages.",
      id: "papluca/xlm-roberta-base-language-detection"
    },
    {
      description: "A model that can classify text generation attacks.",
      id: "meta-llama/Prompt-Guard-86M"
    }
  ],
  spaces: [
    {
      description: "An application that can classify financial sentiment.",
      id: "IoannisTr/Tech_Stocks_Trading_Assistant"
    },
    {
      description: "A dashboard that contains various text classification tasks.",
      id: "miesnerjacob/Multi-task-NLP"
    },
    {
      description: "An application that analyzes user reviews in healthcare.",
      id: "spacy/healthsea-demo"
    }
  ],
  summary: "Text Classification is the task of assigning a label or class to a given text. Some use cases are sentiment analysis, natural language inference, and assessing grammatical correctness.",
  widgetModels: ["distilbert/distilbert-base-uncased-finetuned-sst-2-english"],
  youtubeId: "leNG9fN9FQU"
};
var data_default28 = taskData28;

// node_modules/@huggingface/tasks/dist/esm/tasks/text-generation/data.js
var taskData29 = {
  datasets: [
    {
      description: "Multilingual dataset used to evaluate text generation models.",
      id: "CohereForAI/Global-MMLU"
    },
    {
      description: "High quality multilingual data used to train text-generation models.",
      id: "HuggingFaceFW/fineweb-2"
    },
    {
      description: "Truly open-source, curated and cleaned dialogue dataset.",
      id: "HuggingFaceH4/ultrachat_200k"
    },
    {
      description: "A reasoning dataset.",
      id: "open-r1/OpenThoughts-114k-math"
    },
    {
      description: "A multilingual instruction dataset with preference ratings on responses.",
      id: "allenai/tulu-3-sft-mixture"
    },
    {
      description: "A large synthetic dataset for alignment of text generation models.",
      id: "HuggingFaceTB/smoltalk"
    },
    {
      description: "A dataset made for training text generation models solving math questions.",
      id: "HuggingFaceTB/finemath"
    }
  ],
  demo: {
    inputs: [
      {
        label: "Input",
        content: "Once upon a time,",
        type: "text"
      }
    ],
    outputs: [
      {
        label: "Output",
        content: "Once upon a time, we knew that our ancestors were on the verge of extinction. The great explorers and poets of the Old World, from Alexander the Great to Chaucer, are dead and gone. A good many of our ancient explorers and poets have",
        type: "text"
      }
    ]
  },
  metrics: [
    {
      description: "Cross Entropy is a metric that calculates the difference between two probability distributions. Each probability distribution is the distribution of predicted words",
      id: "Cross Entropy"
    },
    {
      description: "The Perplexity metric is the exponential of the cross-entropy loss. It evaluates the probabilities assigned to the next word by the model. Lower perplexity indicates better performance",
      id: "Perplexity"
    }
  ],
  models: [
    { description: "A text-generation model trained to follow instructions.", id: "google/gemma-2-2b-it" },
    {
      description: "Smaller variant of one of the most powerful models.",
      id: "deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B"
    },
    {
      description: "Very powerful text generation model trained to follow instructions.",
      id: "meta-llama/Meta-Llama-3.1-8B-Instruct"
    },
    {
      description: "Powerful text generation model by Microsoft.",
      id: "microsoft/phi-4"
    },
    {
      description: "A very powerful model with reasoning capabilities.",
      id: "simplescaling/s1.1-32B"
    },
    {
      description: "Strong conversational model that supports very long instructions.",
      id: "Qwen/Qwen2.5-7B-Instruct-1M"
    },
    {
      description: "Text generation model used to write code.",
      id: "Qwen/Qwen2.5-Coder-32B-Instruct"
    },
    {
      description: "Powerful reasoning based open large language model.",
      id: "deepseek-ai/DeepSeek-R1"
    }
  ],
  spaces: [
    {
      description: "A leaderboard to compare different open-source text generation models based on various benchmarks.",
      id: "open-llm-leaderboard/open_llm_leaderboard"
    },
    {
      description: "A leaderboard for comparing chain-of-thought performance of models.",
      id: "logikon/open_cot_leaderboard"
    },
    {
      description: "An text generation based application based on a very powerful LLaMA2 model.",
      id: "ysharma/Explore_llamav2_with_TGI"
    },
    {
      description: "An text generation based application to converse with Zephyr model.",
      id: "HuggingFaceH4/zephyr-chat"
    },
    {
      description: "A leaderboard that ranks text generation models based on blind votes from people.",
      id: "lmsys/chatbot-arena-leaderboard"
    },
    {
      description: "An chatbot to converse with a very powerful text generation model.",
      id: "mlabonne/phixtral-chat"
    }
  ],
  summary: "Generating text is the task of generating new text given another text. These models can, for example, fill in incomplete text or paraphrase.",
  widgetModels: ["mistralai/Mistral-Nemo-Instruct-2407"],
  youtubeId: "e9gNEAlsOvU"
};
var data_default29 = taskData29;

// node_modules/@huggingface/tasks/dist/esm/tasks/text-ranking/data.js
var taskData30 = {
  datasets: [
    {
      description: "Bing queries with relevant passages from various web sources.",
      id: "microsoft/ms_marco"
    }
  ],
  demo: {
    inputs: [
      {
        label: "Source sentence",
        content: "Machine learning is so easy.",
        type: "text"
      },
      {
        label: "Sentences to compare to",
        content: "Deep learning is so straightforward.",
        type: "text"
      },
      {
        label: "",
        content: "This is so difficult, like rocket science.",
        type: "text"
      },
      {
        label: "",
        content: "I can't believe how much I struggled with this.",
        type: "text"
      }
    ],
    outputs: [
      {
        type: "chart",
        data: [
          {
            label: "Deep learning is so straightforward.",
            score: 2.2006407
          },
          {
            label: "This is so difficult, like rocket science.",
            score: -6.2634873
          },
          {
            label: "I can't believe how much I struggled with this.",
            score: -10.251488
          }
        ]
      }
    ]
  },
  metrics: [
    {
      description: "Discounted Cumulative Gain (DCG) measures the gain, or usefulness, of search results discounted by their position. The normalization is done by dividing the DCG by the ideal DCG, which is the DCG of the perfect ranking.",
      id: "Normalized Discounted Cumulative Gain"
    },
    {
      description: "Reciprocal Rank is a measure used to rank the relevancy of documents given a set of documents. Reciprocal Rank is the reciprocal of the rank of the document retrieved, meaning, if the rank is 3, the Reciprocal Rank is 0.33. If the rank is 1, the Reciprocal Rank is 1",
      id: "Mean Reciprocal Rank"
    },
    {
      description: "Mean Average Precision (mAP) is the overall average of the Average Precision (AP) values, where AP is the Area Under the PR Curve (AUC-PR)",
      id: "Mean Average Precision"
    }
  ],
  models: [
    {
      description: "An extremely efficient text ranking model trained on a web search dataset.",
      id: "cross-encoder/ms-marco-MiniLM-L6-v2"
    },
    {
      description: "A strong multilingual text reranker model.",
      id: "Alibaba-NLP/gte-multilingual-reranker-base"
    },
    {
      description: "An efficient text ranking model that punches above its weight.",
      id: "Alibaba-NLP/gte-reranker-modernbert-base"
    }
  ],
  spaces: [],
  summary: "Text Ranking is the task of ranking a set of texts based on their relevance to a query. Text ranking models are trained on large datasets of queries and relevant documents to learn how to rank documents based on their relevance to the query. This task is particularly useful for search engines and information retrieval systems.",
  widgetModels: ["cross-encoder/ms-marco-MiniLM-L6-v2"],
  youtubeId: ""
};
var data_default30 = taskData30;

// node_modules/@huggingface/tasks/dist/esm/tasks/text-to-video/data.js
var taskData31 = {
  datasets: [
    {
      description: "Microsoft Research Video to Text is a large-scale dataset for open domain video captioning",
      id: "iejMac/CLIP-MSR-VTT"
    },
    {
      description: "UCF101 Human Actions dataset consists of 13,320 video clips from YouTube, with 101 classes.",
      id: "quchenyuan/UCF101-ZIP"
    },
    {
      description: "A high-quality dataset for human action recognition in YouTube videos.",
      id: "nateraw/kinetics"
    },
    {
      description: "A dataset of video clips of humans performing pre-defined basic actions with everyday objects.",
      id: "HuggingFaceM4/something_something_v2"
    },
    {
      description: "This dataset consists of text-video pairs and contains noisy samples with irrelevant video descriptions",
      id: "HuggingFaceM4/webvid"
    },
    {
      description: "A dataset of short Flickr videos for the temporal localization of events with descriptions.",
      id: "iejMac/CLIP-DiDeMo"
    }
  ],
  demo: {
    inputs: [
      {
        label: "Input",
        content: "Darth Vader is surfing on the waves.",
        type: "text"
      }
    ],
    outputs: [
      {
        filename: "text-to-video-output.gif",
        type: "img"
      }
    ]
  },
  metrics: [
    {
      description: "Inception Score uses an image classification model that predicts class labels and evaluates how distinct and diverse the images are. A higher score indicates better video generation.",
      id: "is"
    },
    {
      description: "Frechet Inception Distance uses an image classification model to obtain image embeddings. The metric compares mean and standard deviation of the embeddings of real and generated images. A smaller score indicates better video generation.",
      id: "fid"
    },
    {
      description: "Frechet Video Distance uses a model that captures coherence for changes in frames and the quality of each frame. A smaller score indicates better video generation.",
      id: "fvd"
    },
    {
      description: "CLIPSIM measures similarity between video frames and text using an image-text similarity model. A higher score indicates better video generation.",
      id: "clipsim"
    }
  ],
  models: [
    {
      description: "A strong model for consistent video generation.",
      id: "tencent/HunyuanVideo"
    },
    {
      description: "A text-to-video model with high fidelity motion and strong prompt adherence.",
      id: "Lightricks/LTX-Video"
    },
    {
      description: "A text-to-video model focusing on physics-aware applications like robotics.",
      id: "nvidia/Cosmos-1.0-Diffusion-7B-Text2World"
    },
    {
      description: "A robust model for video generation.",
      id: "Wan-AI/Wan2.1-T2V-1.3B"
    }
  ],
  spaces: [
    {
      description: "An application that generates video from text.",
      id: "VideoCrafter/VideoCrafter"
    },
    {
      description: "Consistent video generation application.",
      id: "Wan-AI/Wan2.1"
    },
    {
      description: "A cutting edge video generation application.",
      id: "Pyramid-Flow/pyramid-flow"
    }
  ],
  summary: "Text-to-video models can be used in any application that requires generating consistent sequence of images from text. ",
  widgetModels: ["Wan-AI/Wan2.1-T2V-14B"],
  youtubeId: void 0
};
var data_default31 = taskData31;

// node_modules/@huggingface/tasks/dist/esm/tasks/unconditional-image-generation/data.js
var taskData32 = {
  datasets: [
    {
      description: "The CIFAR-100 dataset consists of 60000 32x32 colour images in 100 classes, with 600 images per class.",
      id: "cifar100"
    },
    {
      description: "Multiple images of celebrities, used for facial expression translation.",
      id: "CelebA"
    }
  ],
  demo: {
    inputs: [
      {
        label: "Seed",
        content: "42",
        type: "text"
      },
      {
        label: "Number of images to generate:",
        content: "4",
        type: "text"
      }
    ],
    outputs: [
      {
        filename: "unconditional-image-generation-output.jpeg",
        type: "img"
      }
    ]
  },
  metrics: [
    {
      description: "The inception score (IS) evaluates the quality of generated images. It measures the diversity of the generated images (the model predictions are evenly distributed across all possible labels) and their 'distinction' or 'sharpness' (the model confidently predicts a single label for each image).",
      id: "Inception score (IS)"
    },
    {
      description: "The Fréchet Inception Distance (FID) evaluates the quality of images created by a generative model by calculating the distance between feature vectors for real and generated images.",
      id: "Frećhet Inception Distance (FID)"
    }
  ],
  models: [
    {
      description: "High-quality image generation model trained on the CIFAR-10 dataset. It synthesizes images of the ten classes presented in the dataset using diffusion probabilistic models, a class of latent variable models inspired by considerations from nonequilibrium thermodynamics.",
      id: "google/ddpm-cifar10-32"
    },
    {
      description: "High-quality image generation model trained on the 256x256 CelebA-HQ dataset. It synthesizes images of faces using diffusion probabilistic models, a class of latent variable models inspired by considerations from nonequilibrium thermodynamics.",
      id: "google/ddpm-celebahq-256"
    }
  ],
  spaces: [
    {
      description: "An application that can generate realistic faces.",
      id: "CompVis/celeba-latent-diffusion"
    }
  ],
  summary: "Unconditional image generation is the task of generating images with no condition in any context (like a prompt text or another image). Once trained, the model will create images that resemble its training data distribution.",
  widgetModels: [""],
  // TODO: Add related video
  youtubeId: ""
};
var data_default32 = taskData32;

// node_modules/@huggingface/tasks/dist/esm/tasks/video-classification/data.js
var taskData33 = {
  datasets: [
    {
      // TODO write proper description
      description: "Benchmark dataset used for video classification with videos that belong to 400 classes.",
      id: "kinetics400"
    }
  ],
  demo: {
    inputs: [
      {
        filename: "video-classification-input.gif",
        type: "img"
      }
    ],
    outputs: [
      {
        type: "chart",
        data: [
          {
            label: "Playing Guitar",
            score: 0.514
          },
          {
            label: "Playing Tennis",
            score: 0.193
          },
          {
            label: "Cooking",
            score: 0.068
          }
        ]
      }
    ]
  },
  metrics: [
    {
      description: "",
      id: "accuracy"
    },
    {
      description: "",
      id: "recall"
    },
    {
      description: "",
      id: "precision"
    },
    {
      description: "",
      id: "f1"
    }
  ],
  models: [
    {
      // TO DO: write description
      description: "Strong Video Classification model trained on the Kinetics 400 dataset.",
      id: "google/vivit-b-16x2-kinetics400"
    },
    {
      // TO DO: write description
      description: "Strong Video Classification model trained on the Kinetics 400 dataset.",
      id: "microsoft/xclip-base-patch32"
    }
  ],
  spaces: [
    {
      description: "An application that classifies video at different timestamps.",
      id: "nateraw/lavila"
    },
    {
      description: "An application that classifies video.",
      id: "fcakyon/video-classification"
    }
  ],
  summary: "Video classification is the task of assigning a label or class to an entire video. Videos are expected to have only one class for each video. Video classification models take a video as input and return a prediction about which class the video belongs to.",
  widgetModels: [],
  youtubeId: ""
};
var data_default33 = taskData33;

// node_modules/@huggingface/tasks/dist/esm/tasks/visual-question-answering/data.js
var taskData34 = {
  datasets: [
    {
      description: "A widely used dataset containing questions (with answers) about images.",
      id: "Graphcore/vqa"
    },
    {
      description: "A dataset to benchmark visual reasoning based on text in images.",
      id: "facebook/textvqa"
    }
  ],
  demo: {
    inputs: [
      {
        filename: "elephant.jpeg",
        type: "img"
      },
      {
        label: "Question",
        content: "What is in this image?",
        type: "text"
      }
    ],
    outputs: [
      {
        type: "chart",
        data: [
          {
            label: "elephant",
            score: 0.97
          },
          {
            label: "elephants",
            score: 0.06
          },
          {
            label: "animal",
            score: 3e-3
          }
        ]
      }
    ]
  },
  isPlaceholder: false,
  metrics: [
    {
      description: "",
      id: "accuracy"
    },
    {
      description: "Measures how much a predicted answer differs from the ground truth based on the difference in their semantic meaning.",
      id: "wu-palmer similarity"
    }
  ],
  models: [
    {
      description: "A visual question answering model trained to convert charts and plots to text.",
      id: "google/deplot"
    },
    {
      description: "A visual question answering model trained for mathematical reasoning and chart derendering from images.",
      id: "google/matcha-base"
    },
    {
      description: "A strong visual question answering that answers questions from book covers.",
      id: "google/pix2struct-ocrvqa-large"
    }
  ],
  spaces: [
    {
      description: "An application that compares visual question answering models across different tasks.",
      id: "merve/pix2struct"
    },
    {
      description: "An application that can answer questions based on images.",
      id: "nielsr/vilt-vqa"
    },
    {
      description: "An application that can caption images and answer questions about a given image. ",
      id: "Salesforce/BLIP"
    },
    {
      description: "An application that can caption images and answer questions about a given image. ",
      id: "vumichien/Img2Prompt"
    }
  ],
  summary: "Visual Question Answering is the task of answering open-ended questions based on an image. They output natural language responses to natural language questions.",
  widgetModels: ["dandelin/vilt-b32-finetuned-vqa"],
  youtubeId: ""
};
var data_default34 = taskData34;

// node_modules/@huggingface/tasks/dist/esm/tasks/zero-shot-classification/data.js
var taskData35 = {
  datasets: [
    {
      description: "A widely used dataset used to benchmark multiple variants of text classification.",
      id: "nyu-mll/glue"
    },
    {
      description: "The Multi-Genre Natural Language Inference (MultiNLI) corpus is a crowd-sourced collection of 433k sentence pairs annotated with textual entailment information.",
      id: "nyu-mll/multi_nli"
    },
    {
      description: "FEVER is a publicly available dataset for fact extraction and verification against textual sources.",
      id: "fever/fever"
    }
  ],
  demo: {
    inputs: [
      {
        label: "Text Input",
        content: "Dune is the best movie ever.",
        type: "text"
      },
      {
        label: "Candidate Labels",
        content: "CINEMA, ART, MUSIC",
        type: "text"
      }
    ],
    outputs: [
      {
        type: "chart",
        data: [
          {
            label: "CINEMA",
            score: 0.9
          },
          {
            label: "ART",
            score: 0.1
          },
          {
            label: "MUSIC",
            score: 0
          }
        ]
      }
    ]
  },
  metrics: [],
  models: [
    {
      description: "Powerful zero-shot text classification model.",
      id: "facebook/bart-large-mnli"
    },
    {
      description: "Cutting-edge zero-shot multilingual text classification model.",
      id: "MoritzLaurer/ModernBERT-large-zeroshot-v2.0"
    },
    {
      description: "Zero-shot text classification model that can be used for topic and sentiment classification.",
      id: "knowledgator/gliclass-modern-base-v2.0-init"
    }
  ],
  spaces: [],
  summary: "Zero-shot text classification is a task in natural language processing where a model is trained on a set of labeled examples but is then able to classify new examples from previously unseen classes.",
  widgetModels: ["facebook/bart-large-mnli"]
};
var data_default35 = taskData35;

// node_modules/@huggingface/tasks/dist/esm/tasks/zero-shot-image-classification/data.js
var taskData36 = {
  datasets: [
    {
      // TODO write proper description
      description: "",
      id: ""
    }
  ],
  demo: {
    inputs: [
      {
        filename: "image-classification-input.jpeg",
        type: "img"
      },
      {
        label: "Classes",
        content: "cat, dog, bird",
        type: "text"
      }
    ],
    outputs: [
      {
        type: "chart",
        data: [
          {
            label: "Cat",
            score: 0.664
          },
          {
            label: "Dog",
            score: 0.329
          },
          {
            label: "Bird",
            score: 8e-3
          }
        ]
      }
    ]
  },
  metrics: [
    {
      description: "Computes the number of times the correct label appears in top K labels predicted",
      id: "top-K accuracy"
    }
  ],
  models: [
    {
      description: "Multilingual image classification model for 80 languages.",
      id: "visheratin/mexma-siglip"
    },
    {
      description: "Strong zero-shot image classification model.",
      id: "google/siglip2-base-patch16-224"
    },
    {
      description: "Robust zero-shot image classification model.",
      id: "intfloat/mmE5-mllama-11b-instruct"
    },
    {
      description: "Powerful zero-shot image classification model supporting 94 languages.",
      id: "jinaai/jina-clip-v2"
    },
    {
      description: "Strong image classification model for biomedical domain.",
      id: "microsoft/BiomedCLIP-PubMedBERT_256-vit_base_patch16_224"
    }
  ],
  spaces: [
    {
      description: "An application that leverages zero-shot image classification to find best captions to generate an image. ",
      id: "pharma/CLIP-Interrogator"
    },
    {
      description: "An application to compare different zero-shot image classification models. ",
      id: "merve/compare_clip_siglip"
    }
  ],
  summary: "Zero-shot image classification is the task of classifying previously unseen classes during training of a model.",
  widgetModels: ["google/siglip-so400m-patch14-224"],
  youtubeId: ""
};
var data_default36 = taskData36;

// node_modules/@huggingface/tasks/dist/esm/tasks/zero-shot-object-detection/data.js
var taskData37 = {
  datasets: [],
  demo: {
    inputs: [
      {
        filename: "zero-shot-object-detection-input.jpg",
        type: "img"
      },
      {
        label: "Classes",
        content: "cat, dog, bird",
        type: "text"
      }
    ],
    outputs: [
      {
        filename: "zero-shot-object-detection-output.jpg",
        type: "img"
      }
    ]
  },
  metrics: [
    {
      description: "The Average Precision (AP) metric is the Area Under the PR Curve (AUC-PR). It is calculated for each class separately",
      id: "Average Precision"
    },
    {
      description: "The Mean Average Precision (mAP) metric is the overall average of the AP values",
      id: "Mean Average Precision"
    },
    {
      description: "The APα metric is the Average Precision at the IoU threshold of a α value, for example, AP50 and AP75",
      id: "APα"
    }
  ],
  models: [
    {
      description: "Solid zero-shot object detection model.",
      id: "IDEA-Research/grounding-dino-base"
    },
    {
      description: "Cutting-edge zero-shot object detection model.",
      id: "google/owlv2-base-patch16-ensemble"
    }
  ],
  spaces: [
    {
      description: "A demo to try the state-of-the-art zero-shot object detection model, OWLv2.",
      id: "merve/owlv2"
    },
    {
      description: "A demo that combines a zero-shot object detection and mask generation model for zero-shot segmentation.",
      id: "merve/OWLSAM"
    }
  ],
  summary: "Zero-shot object detection is a computer vision task to detect objects and their classes in images, without any prior training or knowledge of the classes. Zero-shot object detection models receive an image as input, as well as a list of candidate classes, and output the bounding boxes and labels where the objects have been detected.",
  widgetModels: [],
  youtubeId: ""
};
var data_default37 = taskData37;

// node_modules/@huggingface/tasks/dist/esm/tasks/image-to-3d/data.js
var taskData38 = {
  datasets: [
    {
      description: "A large dataset of over 10 million 3D objects.",
      id: "allenai/objaverse-xl"
    },
    {
      description: "A dataset of isolated object images for evaluating image-to-3D models.",
      id: "dylanebert/iso3d"
    }
  ],
  demo: {
    inputs: [
      {
        filename: "image-to-3d-image-input.png",
        type: "img"
      }
    ],
    outputs: [
      {
        label: "Result",
        content: "image-to-3d-3d-output-filename.glb",
        type: "text"
      }
    ]
  },
  metrics: [],
  models: [
    {
      description: "Fast image-to-3D mesh model by Tencent.",
      id: "TencentARC/InstantMesh"
    },
    {
      description: "Fast image-to-3D mesh model by StabilityAI",
      id: "stabilityai/TripoSR"
    },
    {
      description: "A scaled up image-to-3D mesh model derived from TripoSR.",
      id: "hwjiang/Real3D"
    },
    {
      description: "Consistent image-to-3d generation model.",
      id: "stabilityai/stable-point-aware-3d"
    }
  ],
  spaces: [
    {
      description: "Leaderboard to evaluate image-to-3D models.",
      id: "dylanebert/3d-arena"
    },
    {
      description: "Image-to-3D demo with mesh outputs.",
      id: "TencentARC/InstantMesh"
    },
    {
      description: "Image-to-3D demo.",
      id: "stabilityai/stable-point-aware-3d"
    },
    {
      description: "Image-to-3D demo with mesh outputs.",
      id: "hwjiang/Real3D"
    },
    {
      description: "Image-to-3D demo with splat outputs.",
      id: "dylanebert/LGM-mini"
    }
  ],
  summary: "Image-to-3D models take in image input and produce 3D output.",
  widgetModels: [],
  youtubeId: ""
};
var data_default38 = taskData38;

// node_modules/@huggingface/tasks/dist/esm/tasks/text-to-3d/data.js
var taskData39 = {
  datasets: [
    {
      description: "A large dataset of over 10 million 3D objects.",
      id: "allenai/objaverse-xl"
    },
    {
      description: "Descriptive captions for 3D objects in Objaverse.",
      id: "tiange/Cap3D"
    }
  ],
  demo: {
    inputs: [
      {
        label: "Prompt",
        content: "a cat statue",
        type: "text"
      }
    ],
    outputs: [
      {
        label: "Result",
        content: "text-to-3d-3d-output-filename.glb",
        type: "text"
      }
    ]
  },
  metrics: [],
  models: [
    {
      description: "Text-to-3D mesh model by OpenAI",
      id: "openai/shap-e"
    },
    {
      description: "Generative 3D gaussian splatting model.",
      id: "ashawkey/LGM"
    }
  ],
  spaces: [
    {
      description: "Text-to-3D demo with mesh outputs.",
      id: "hysts/Shap-E"
    },
    {
      description: "Text/image-to-3D demo with splat outputs.",
      id: "ashawkey/LGM"
    }
  ],
  summary: "Text-to-3D models take in text input and produce 3D output.",
  widgetModels: [],
  youtubeId: ""
};
var data_default39 = taskData39;

// node_modules/@huggingface/tasks/dist/esm/tasks/keypoint-detection/data.js
var taskData40 = {
  datasets: [
    {
      description: "A dataset of hand keypoints of over 500k examples.",
      id: "Vincent-luo/hagrid-mediapipe-hands"
    }
  ],
  demo: {
    inputs: [
      {
        filename: "keypoint-detection-input.png",
        type: "img"
      }
    ],
    outputs: [
      {
        filename: "keypoint-detection-output.png",
        type: "img"
      }
    ]
  },
  metrics: [],
  models: [
    {
      description: "A robust keypoint detection model.",
      id: "magic-leap-community/superpoint"
    },
    {
      description: "A robust keypoint matching model.",
      id: "magic-leap-community/superglue_outdoor"
    },
    {
      description: "Strong keypoint detection model used to detect human pose.",
      id: "facebook/sapiens-pose-1b"
    },
    {
      description: "Powerful keypoint detection model used to detect human pose.",
      id: "usyd-community/vitpose-plus-base"
    }
  ],
  spaces: [
    {
      description: "An application that detects hand keypoints in real-time.",
      id: "datasciencedojo/Hand-Keypoint-Detection-Realtime"
    },
    {
      description: "An application to try a universal keypoint detection model.",
      id: "merve/SuperPoint"
    }
  ],
  summary: "Keypoint detection is the task of identifying meaningful distinctive points or features in an image.",
  widgetModels: [],
  youtubeId: ""
};
var data_default40 = taskData40;

// node_modules/@huggingface/tasks/dist/esm/tasks/video-text-to-text/data.js
var taskData41 = {
  datasets: [
    {
      description: "Multiple-choice questions and answers about videos.",
      id: "lmms-lab/Video-MME"
    },
    {
      description: "A dataset of instructions and question-answer pairs about videos.",
      id: "lmms-lab/VideoChatGPT"
    },
    {
      description: "Large video understanding dataset.",
      id: "HuggingFaceFV/finevideo"
    }
  ],
  demo: {
    inputs: [
      {
        filename: "video-text-to-text-input.gif",
        type: "img"
      },
      {
        label: "Text Prompt",
        content: "What is happening in this video?",
        type: "text"
      }
    ],
    outputs: [
      {
        label: "Answer",
        content: "The video shows a series of images showing a fountain with water jets and a variety of colorful flowers and butterflies in the background.",
        type: "text"
      }
    ]
  },
  metrics: [],
  models: [
    {
      description: "A robust video-text-to-text model.",
      id: "Vision-CAIR/LongVU_Qwen2_7B"
    },
    {
      description: "Strong video-text-to-text model with reasoning capabilities.",
      id: "GoodiesHere/Apollo-LMMs-Apollo-7B-t32"
    },
    {
      description: "Strong video-text-to-text model.",
      id: "HuggingFaceTB/SmolVLM2-2.2B-Instruct"
    }
  ],
  spaces: [
    {
      description: "An application to chat with a video-text-to-text model.",
      id: "llava-hf/video-llava"
    },
    {
      description: "A leaderboard for various video-text-to-text models.",
      id: "opencompass/openvlm_video_leaderboard"
    },
    {
      description: "An application to generate highlights from a video.",
      id: "HuggingFaceTB/SmolVLM2-HighlightGenerator"
    }
  ],
  summary: "Video-text-to-text models take in a video and a text prompt and output text. These models are also called video-language models.",
  widgetModels: [""],
  youtubeId: ""
};
var data_default41 = taskData41;

// node_modules/@huggingface/tasks/dist/esm/tasks/index.js
var TASKS_MODEL_LIBRARIES = {
  "audio-classification": ["speechbrain", "transformers", "transformers.js"],
  "audio-to-audio": ["asteroid", "fairseq", "speechbrain"],
  "automatic-speech-recognition": ["espnet", "nemo", "speechbrain", "transformers", "transformers.js"],
  "audio-text-to-text": [],
  "depth-estimation": ["transformers", "transformers.js"],
  "document-question-answering": ["transformers", "transformers.js"],
  "feature-extraction": ["sentence-transformers", "transformers", "transformers.js"],
  "fill-mask": ["transformers", "transformers.js"],
  "graph-ml": ["transformers"],
  "image-classification": ["keras", "timm", "transformers", "transformers.js"],
  "image-feature-extraction": ["timm", "transformers"],
  "image-segmentation": ["transformers", "transformers.js"],
  "image-text-to-text": ["transformers"],
  "image-to-image": ["diffusers", "transformers", "transformers.js"],
  "image-to-text": ["transformers", "transformers.js"],
  "image-to-video": ["diffusers"],
  "keypoint-detection": ["transformers"],
  "video-classification": ["transformers"],
  "mask-generation": ["transformers"],
  "multiple-choice": ["transformers"],
  "object-detection": ["transformers", "transformers.js", "ultralytics"],
  other: [],
  "question-answering": ["adapter-transformers", "allennlp", "transformers", "transformers.js"],
  robotics: [],
  "reinforcement-learning": ["transformers", "stable-baselines3", "ml-agents", "sample-factory"],
  "sentence-similarity": ["sentence-transformers", "spacy", "transformers.js"],
  summarization: ["transformers", "transformers.js"],
  "table-question-answering": ["transformers"],
  "table-to-text": ["transformers"],
  "tabular-classification": ["sklearn"],
  "tabular-regression": ["sklearn"],
  "tabular-to-text": ["transformers"],
  "text-classification": ["adapter-transformers", "setfit", "spacy", "transformers", "transformers.js"],
  "text-generation": ["transformers", "transformers.js"],
  "text-ranking": ["sentence-transformers", "transformers"],
  "text-retrieval": [],
  "text-to-image": ["diffusers"],
  "text-to-speech": ["espnet", "tensorflowtts", "transformers", "transformers.js"],
  "text-to-audio": ["transformers", "transformers.js"],
  "text-to-video": ["diffusers"],
  "text2text-generation": ["transformers", "transformers.js"],
  "time-series-forecasting": [],
  "token-classification": [
    "adapter-transformers",
    "flair",
    "spacy",
    "span-marker",
    "stanza",
    "transformers",
    "transformers.js"
  ],
  translation: ["transformers", "transformers.js"],
  "unconditional-image-generation": ["diffusers"],
  "video-text-to-text": ["transformers"],
  "visual-question-answering": ["transformers", "transformers.js"],
  "voice-activity-detection": [],
  "zero-shot-classification": ["transformers", "transformers.js"],
  "zero-shot-image-classification": ["transformers", "transformers.js"],
  "zero-shot-object-detection": ["transformers", "transformers.js"],
  "text-to-3d": ["diffusers"],
  "image-to-3d": ["diffusers"],
  "any-to-any": ["transformers"],
  "visual-document-retrieval": ["transformers"]
};
function getData(type, partialTaskData = data_default16) {
  return {
    ...partialTaskData,
    id: type,
    label: PIPELINE_DATA[type].name,
    libraries: TASKS_MODEL_LIBRARIES[type]
  };
}
var TASKS_DATA = {
  "any-to-any": getData("any-to-any", data_default16),
  "audio-classification": getData("audio-classification", data_default),
  "audio-to-audio": getData("audio-to-audio", data_default2),
  "audio-text-to-text": getData("audio-text-to-text", data_default16),
  "automatic-speech-recognition": getData("automatic-speech-recognition", data_default3),
  "depth-estimation": getData("depth-estimation", data_default15),
  "document-question-answering": getData("document-question-answering", data_default4),
  "visual-document-retrieval": getData("visual-document-retrieval", data_default16),
  "feature-extraction": getData("feature-extraction", data_default5),
  "fill-mask": getData("fill-mask", data_default6),
  "graph-ml": void 0,
  "image-classification": getData("image-classification", data_default7),
  "image-feature-extraction": getData("image-feature-extraction", data_default8),
  "image-segmentation": getData("image-segmentation", data_default12),
  "image-to-image": getData("image-to-image", data_default9),
  "image-text-to-text": getData("image-text-to-text", data_default11),
  "image-to-text": getData("image-to-text", data_default10),
  "image-to-video": void 0,
  "keypoint-detection": getData("keypoint-detection", data_default40),
  "mask-generation": getData("mask-generation", data_default13),
  "multiple-choice": void 0,
  "object-detection": getData("object-detection", data_default14),
  "video-classification": getData("video-classification", data_default33),
  other: void 0,
  "question-answering": getData("question-answering", data_default18),
  "reinforcement-learning": getData("reinforcement-learning", data_default17),
  robotics: void 0,
  "sentence-similarity": getData("sentence-similarity", data_default19),
  summarization: getData("summarization", data_default20),
  "table-question-answering": getData("table-question-answering", data_default21),
  "table-to-text": void 0,
  "tabular-classification": getData("tabular-classification", data_default22),
  "tabular-regression": getData("tabular-regression", data_default23),
  "tabular-to-text": void 0,
  "text-classification": getData("text-classification", data_default28),
  "text-generation": getData("text-generation", data_default29),
  "text-ranking": getData("text-ranking", data_default30),
  "text-retrieval": void 0,
  "text-to-image": getData("text-to-image", data_default24),
  "text-to-speech": getData("text-to-speech", data_default25),
  "text-to-audio": void 0,
  "text-to-video": getData("text-to-video", data_default31),
  "text2text-generation": void 0,
  "time-series-forecasting": void 0,
  "token-classification": getData("token-classification", data_default26),
  translation: getData("translation", data_default27),
  "unconditional-image-generation": getData("unconditional-image-generation", data_default32),
  "video-text-to-text": getData("video-text-to-text", data_default41),
  "visual-question-answering": getData("visual-question-answering", data_default34),
  "voice-activity-detection": void 0,
  "zero-shot-classification": getData("zero-shot-classification", data_default35),
  "zero-shot-image-classification": getData("zero-shot-image-classification", data_default36),
  "zero-shot-object-detection": getData("zero-shot-object-detection", data_default37),
  "text-to-3d": getData("text-to-3d", data_default39),
  "image-to-3d": getData("image-to-3d", data_default38)
};

// node_modules/@huggingface/tasks/dist/esm/snippets/inputs.js
var inputsZeroShotClassification = () => `"Hi, I recently bought a device from your company but it is not working as advertised and I would like to get reimbursed!"`;
var inputsTranslation = () => `"Меня зовут Вольфганг и я живу в Берлине"`;
var inputsSummarization = () => `"The tower is 324 metres (1,063 ft) tall, about the same height as an 81-storey building, and the tallest structure in Paris. Its base is square, measuring 125 metres (410 ft) on each side. During its construction, the Eiffel Tower surpassed the Washington Monument to become the tallest man-made structure in the world, a title it held for 41 years until the Chrysler Building in New York City was finished in 1930. It was the first structure to reach a height of 300 metres. Due to the addition of a broadcasting aerial at the top of the tower in 1957, it is now taller than the Chrysler Building by 5.2 metres (17 ft). Excluding transmitters, the Eiffel Tower is the second tallest free-standing structure in France after the Millau Viaduct."`;
var inputsTableQuestionAnswering = () => `{
    "query": "How many stars does the transformers repository have?",
    "table": {
        "Repository": ["Transformers", "Datasets", "Tokenizers"],
        "Stars": ["36542", "4512", "3934"],
        "Contributors": ["651", "77", "34"],
        "Programming language": [
            "Python",
            "Python",
            "Rust, Python and NodeJS"
        ]
    }
}`;
var inputsVisualQuestionAnswering = () => `{
        "image": "cat.png",
        "question": "What is in this image?"
    }`;
var inputsQuestionAnswering = () => `{
    "question": "What is my name?",
    "context": "My name is Clara and I live in Berkeley."
}`;
var inputsTextClassification = () => `"I like you. I love you"`;
var inputsTokenClassification = () => `"My name is Sarah Jessica Parker but you can call me Jessica"`;
var inputsTextGeneration = (model) => {
  if (model.tags.includes("conversational")) {
    return model.pipeline_tag === "text-generation" ? [{ role: "user", content: "What is the capital of France?" }] : [
      {
        role: "user",
        content: [
          {
            type: "text",
            text: "Describe this image in one sentence."
          },
          {
            type: "image_url",
            image_url: {
              url: "https://cdn.britannica.com/61/93061-050-99147DCE/Statue-of-Liberty-Island-New-York-Bay.jpg"
            }
          }
        ]
      }
    ];
  }
  return `"Can you please let us know more details about your "`;
};
var inputsText2TextGeneration = () => `"The answer to the universe is"`;
var inputsFillMask = (model) => `"The answer to the universe is ${model.mask_token}."`;
var inputsSentenceSimilarity = () => `{
    "source_sentence": "That is a happy person",
    "sentences": [
        "That is a happy dog",
        "That is a very happy person",
        "Today is a sunny day"
    ]
}`;
var inputsFeatureExtraction = () => `"Today is a sunny day and I will get some ice cream."`;
var inputsImageClassification = () => `"cats.jpg"`;
var inputsImageToText = () => `"cats.jpg"`;
var inputsImageToImage = () => `{
    "image": "cat.png",
    "prompt": "Turn the cat into a tiger."
}`;
var inputsImageSegmentation = () => `"cats.jpg"`;
var inputsObjectDetection = () => `"cats.jpg"`;
var inputsAudioToAudio = () => `"sample1.flac"`;
var inputsAudioClassification = () => `"sample1.flac"`;
var inputsTextToImage = () => `"Astronaut riding a horse"`;
var inputsTextToVideo = () => `"A young man walking on the street"`;
var inputsTextToSpeech = () => `"The answer to the universe is 42"`;
var inputsTextToAudio = () => `"liquid drum and bass, atmospheric synths, airy sounds"`;
var inputsAutomaticSpeechRecognition = () => `"sample1.flac"`;
var inputsTabularPrediction = () => `'{"Height":[11.52,12.48],"Length1":[23.2,24.0],"Length2":[25.4,26.3],"Species": ["Bream","Bream"]}'`;
var inputsZeroShotImageClassification = () => `"cats.jpg"`;
var modelInputSnippets = {
  "audio-to-audio": inputsAudioToAudio,
  "audio-classification": inputsAudioClassification,
  "automatic-speech-recognition": inputsAutomaticSpeechRecognition,
  "document-question-answering": inputsVisualQuestionAnswering,
  "feature-extraction": inputsFeatureExtraction,
  "fill-mask": inputsFillMask,
  "image-classification": inputsImageClassification,
  "image-to-text": inputsImageToText,
  "image-to-image": inputsImageToImage,
  "image-segmentation": inputsImageSegmentation,
  "object-detection": inputsObjectDetection,
  "question-answering": inputsQuestionAnswering,
  "sentence-similarity": inputsSentenceSimilarity,
  summarization: inputsSummarization,
  "table-question-answering": inputsTableQuestionAnswering,
  "tabular-regression": inputsTabularPrediction,
  "tabular-classification": inputsTabularPrediction,
  "text-classification": inputsTextClassification,
  "text-generation": inputsTextGeneration,
  "image-text-to-text": inputsTextGeneration,
  "text-to-image": inputsTextToImage,
  "text-to-video": inputsTextToVideo,
  "text-to-speech": inputsTextToSpeech,
  "text-to-audio": inputsTextToAudio,
  "text2text-generation": inputsText2TextGeneration,
  "token-classification": inputsTokenClassification,
  translation: inputsTranslation,
  "zero-shot-classification": inputsZeroShotClassification,
  "zero-shot-image-classification": inputsZeroShotImageClassification
};
function getModelInputSnippet(model, noWrap = false, noQuotes = false) {
  if (model.pipeline_tag) {
    const inputs = modelInputSnippets[model.pipeline_tag];
    if (inputs) {
      let result = inputs(model);
      if (typeof result === "string") {
        if (noWrap) {
          result = result.replace(/(?:(?:\r?\n|\r)\t*)|\t+/g, " ");
        }
        if (noQuotes) {
          const REGEX_QUOTES = /^"(.+)"$/s;
          const match = result.match(REGEX_QUOTES);
          result = match ? match[1] : result;
        }
      }
      return result;
    }
  }
  return "No input example has been defined for this model task.";
}

// node_modules/@huggingface/tasks/dist/esm/snippets/common.js
function stringifyMessages(messages, opts) {
  let messagesStr = JSON.stringify(messages, null, "	");
  if (opts == null ? void 0 : opts.indent) {
    messagesStr = messagesStr.replaceAll("\n", `
${opts.indent}`);
  }
  if (!(opts == null ? void 0 : opts.attributeKeyQuotes)) {
    messagesStr = messagesStr.replace(/"([^"]+)":/g, "$1:");
  }
  if (opts == null ? void 0 : opts.customContentEscaper) {
    messagesStr = opts.customContentEscaper(messagesStr);
  }
  return messagesStr;
}

// node_modules/@huggingface/tasks/dist/esm/model-libraries-snippets.js
var TAG_CUSTOM_CODE = "custom_code";
function nameWithoutNamespace(modelId) {
  const splitted = modelId.split("/");
  return splitted.length === 1 ? splitted[0] : splitted[1];
}
var escapeStringForJson = (str) => JSON.stringify(str).slice(1, -1);
var adapters = (model) => {
  var _a, _b;
  return [
    `from adapters import AutoAdapterModel

model = AutoAdapterModel.from_pretrained("${(_b = (_a = model.config) == null ? void 0 : _a.adapter_transformers) == null ? void 0 : _b.model_name}")
model.load_adapter("${model.id}", set_active=True)`
  ];
};
var allennlpUnknown = (model) => [
  `import allennlp_models
from allennlp.predictors.predictor import Predictor

predictor = Predictor.from_path("hf://${model.id}")`
];
var allennlpQuestionAnswering = (model) => [
  `import allennlp_models
from allennlp.predictors.predictor import Predictor

predictor = Predictor.from_path("hf://${model.id}")
predictor_input = {"passage": "My name is Wolfgang and I live in Berlin", "question": "Where do I live?"}
predictions = predictor.predict_json(predictor_input)`
];
var allennlp = (model) => {
  if (model.tags.includes("question-answering")) {
    return allennlpQuestionAnswering(model);
  }
  return allennlpUnknown(model);
};
var araclip = (model) => [
  `from araclip import AraClip

model = AraClip.from_pretrained("${model.id}")`
];
var asteroid = (model) => [
  `from asteroid.models import BaseModel

model = BaseModel.from_pretrained("${model.id}")`
];
var audioseal = (model) => {
  const watermarkSnippet = `# Watermark Generator
from audioseal import AudioSeal

model = AudioSeal.load_generator("${model.id}")
# pass a tensor (tensor_wav) of shape (batch, channels, samples) and a sample rate
wav, sr = tensor_wav, 16000
	
watermark = model.get_watermark(wav, sr)
watermarked_audio = wav + watermark`;
  const detectorSnippet = `# Watermark Detector
from audioseal import AudioSeal

detector = AudioSeal.load_detector("${model.id}")
	
result, message = detector.detect_watermark(watermarked_audio, sr)`;
  return [watermarkSnippet, detectorSnippet];
};
function get_base_diffusers_model(model) {
  var _a, _b;
  return ((_b = (_a = model.cardData) == null ? void 0 : _a.base_model) == null ? void 0 : _b.toString()) ?? "fill-in-base-model";
}
function get_prompt_from_diffusers_model(model) {
  var _a, _b, _c;
  const prompt = ((_b = (_a = model.widgetData) == null ? void 0 : _a[0]) == null ? void 0 : _b.text) ?? ((_c = model.cardData) == null ? void 0 : _c.instance_prompt);
  if (prompt) {
    return escapeStringForJson(prompt);
  }
}
var ben2 = (model) => [
  `import requests
from PIL import Image
from ben2 import AutoModel

url = "https://huggingface.co/datasets/mishig/sample_images/resolve/main/teapot.jpg"
image = Image.open(requests.get(url, stream=True).raw)

model = AutoModel.from_pretrained("${model.id}")
model.to("cuda").eval()
foreground = model.inference(image)
`
];
var bertopic = (model) => [
  `from bertopic import BERTopic

model = BERTopic.load("${model.id}")`
];
var bm25s = (model) => [
  `from bm25s.hf import BM25HF

retriever = BM25HF.load_from_hub("${model.id}")`
];
var cxr_foundation = () => [
  `# pip install git+https://github.com/Google-Health/cxr-foundation.git#subdirectory=python

# Load image as grayscale (Stillwaterising, CC0, via Wikimedia Commons)
import requests
from PIL import Image
from io import BytesIO
image_url = "https://upload.wikimedia.org/wikipedia/commons/c/c8/Chest_Xray_PA_3-8-2010.png"
img = Image.open(requests.get(image_url, headers={'User-Agent': 'Demo'}, stream=True).raw).convert('L')

# Run inference
from clientside.clients import make_hugging_face_client
cxr_client = make_hugging_face_client('cxr_model')
print(cxr_client.get_image_embeddings_from_images([img]))`
];
var depth_anything_v2 = (model) => {
  let encoder;
  let features;
  let out_channels;
  encoder = "<ENCODER>";
  features = "<NUMBER_OF_FEATURES>";
  out_channels = "<OUT_CHANNELS>";
  if (model.id === "depth-anything/Depth-Anything-V2-Small") {
    encoder = "vits";
    features = "64";
    out_channels = "[48, 96, 192, 384]";
  } else if (model.id === "depth-anything/Depth-Anything-V2-Base") {
    encoder = "vitb";
    features = "128";
    out_channels = "[96, 192, 384, 768]";
  } else if (model.id === "depth-anything/Depth-Anything-V2-Large") {
    encoder = "vitl";
    features = "256";
    out_channels = "[256, 512, 1024, 1024";
  }
  return [
    `
# Install from https://github.com/DepthAnything/Depth-Anything-V2

# Load the model and infer depth from an image
import cv2
import torch

from depth_anything_v2.dpt import DepthAnythingV2

# instantiate the model
model = DepthAnythingV2(encoder="${encoder}", features=${features}, out_channels=${out_channels})

# load the weights
filepath = hf_hub_download(repo_id="${model.id}", filename="depth_anything_v2_${encoder}.pth", repo_type="model")
state_dict = torch.load(filepath, map_location="cpu")
model.load_state_dict(state_dict).eval()

raw_img = cv2.imread("your/image/path")
depth = model.infer_image(raw_img) # HxW raw depth map in numpy
    `
  ];
};
var depth_pro = (model) => {
  const installSnippet = `# Download checkpoint
pip install huggingface-hub
huggingface-cli download --local-dir checkpoints ${model.id}`;
  const inferenceSnippet = `import depth_pro

# Load model and preprocessing transform
model, transform = depth_pro.create_model_and_transforms()
model.eval()

# Load and preprocess an image.
image, _, f_px = depth_pro.load_rgb("example.png")
image = transform(image)

# Run inference.
prediction = model.infer(image, f_px=f_px)

# Results: 1. Depth in meters
depth = prediction["depth"]
# Results: 2. Focal length in pixels
focallength_px = prediction["focallength_px"]`;
  return [installSnippet, inferenceSnippet];
};
var derm_foundation = () => [
  `from huggingface_hub import from_pretrained_keras
import tensorflow as tf, requests

# Load and format input
IMAGE_URL = "https://storage.googleapis.com/dx-scin-public-data/dataset/images/3445096909671059178.png"
input_tensor = tf.train.Example(
    features=tf.train.Features(
        feature={
            "image/encoded": tf.train.Feature(
                bytes_list=tf.train.BytesList(value=[requests.get(IMAGE_URL, stream=True).content])
            )
        }
    )
).SerializeToString()

# Load model and run inference
loaded_model = from_pretrained_keras("google/derm-foundation")
infer = loaded_model.signatures["serving_default"]
print(infer(inputs=tf.constant([input_tensor])))`
];
var diffusersDefaultPrompt = "Astronaut in a jungle, cold color palette, muted colors, detailed, 8k";
var diffusers_default = (model) => [
  `from diffusers import DiffusionPipeline

pipe = DiffusionPipeline.from_pretrained("${model.id}")

prompt = "${get_prompt_from_diffusers_model(model) ?? diffusersDefaultPrompt}"
image = pipe(prompt).images[0]`
];
var diffusers_controlnet = (model) => [
  `from diffusers import ControlNetModel, StableDiffusionControlNetPipeline

controlnet = ControlNetModel.from_pretrained("${model.id}")
pipe = StableDiffusionControlNetPipeline.from_pretrained(
	"${get_base_diffusers_model(model)}", controlnet=controlnet
)`
];
var diffusers_lora = (model) => [
  `from diffusers import DiffusionPipeline

pipe = DiffusionPipeline.from_pretrained("${get_base_diffusers_model(model)}")
pipe.load_lora_weights("${model.id}")

prompt = "${get_prompt_from_diffusers_model(model) ?? diffusersDefaultPrompt}"
image = pipe(prompt).images[0]`
];
var diffusers_textual_inversion = (model) => [
  `from diffusers import DiffusionPipeline

pipe = DiffusionPipeline.from_pretrained("${get_base_diffusers_model(model)}")
pipe.load_textual_inversion("${model.id}")`
];
var diffusers = (model) => {
  if (model.tags.includes("controlnet")) {
    return diffusers_controlnet(model);
  } else if (model.tags.includes("lora")) {
    return diffusers_lora(model);
  } else if (model.tags.includes("textual_inversion")) {
    return diffusers_textual_inversion(model);
  } else {
    return diffusers_default(model);
  }
};
var diffusionkit = (model) => {
  const sd3Snippet = `# Pipeline for Stable Diffusion 3
from diffusionkit.mlx import DiffusionPipeline

pipeline = DiffusionPipeline(
	shift=3.0,
	use_t5=False,
	model_version=${model.id},
	low_memory_mode=True,
	a16=True,
	w16=True,
)`;
  const fluxSnippet = `# Pipeline for Flux
from diffusionkit.mlx import FluxPipeline

pipeline = FluxPipeline(
  shift=1.0,
  model_version=${model.id},
  low_memory_mode=True,
  a16=True,
  w16=True,
)`;
  const generateSnippet = `# Image Generation
HEIGHT = 512
WIDTH = 512
NUM_STEPS = ${model.tags.includes("flux") ? 4 : 50}
CFG_WEIGHT = ${model.tags.includes("flux") ? 0 : 5}

image, _ = pipeline.generate_image(
  "a photo of a cat",
  cfg_weight=CFG_WEIGHT,
  num_steps=NUM_STEPS,
  latent_size=(HEIGHT // 8, WIDTH // 8),
)`;
  const pipelineSnippet = model.tags.includes("flux") ? fluxSnippet : sd3Snippet;
  return [pipelineSnippet, generateSnippet];
};
var cartesia_pytorch = (model) => [
  `# pip install --no-binary :all: cartesia-pytorch
from cartesia_pytorch import ReneLMHeadModel
from transformers import AutoTokenizer

model = ReneLMHeadModel.from_pretrained("${model.id}")
tokenizer = AutoTokenizer.from_pretrained("allenai/OLMo-1B-hf")

in_message = ["Rene Descartes was"]
inputs = tokenizer(in_message, return_tensors="pt")

outputs = model.generate(inputs.input_ids, max_length=50, top_k=100, top_p=0.99)
out_message = tokenizer.batch_decode(outputs, skip_special_tokens=True)[0]

print(out_message)
)`
];
var cartesia_mlx = (model) => [
  `import mlx.core as mx
import cartesia_mlx as cmx

model = cmx.from_pretrained("${model.id}")
model.set_dtype(mx.float32)   

prompt = "Rene Descartes was"

for text in model.generate(
    prompt,
    max_tokens=500,
    eval_every_n=5,
    verbose=True,
    top_p=0.99,
    temperature=0.85,
):
    print(text, end="", flush=True)
`
];
var edsnlp = (model) => {
  const packageName = nameWithoutNamespace(model.id).replaceAll("-", "_");
  return [
    `# Load it from the Hub directly
import edsnlp
nlp = edsnlp.load("${model.id}")
`,
    `# Or install it as a package
!pip install git+https://huggingface.co/${model.id}

# and import it as a module
import ${packageName}

nlp = ${packageName}.load()  # or edsnlp.load("${packageName}")
`
  ];
};
var espnetTTS = (model) => [
  `from espnet2.bin.tts_inference import Text2Speech

model = Text2Speech.from_pretrained("${model.id}")

speech, *_ = model("text to generate speech from")`
];
var espnetASR = (model) => [
  `from espnet2.bin.asr_inference import Speech2Text

model = Speech2Text.from_pretrained(
  "${model.id}"
)

speech, rate = soundfile.read("speech.wav")
text, *_ = model(speech)[0]`
];
var espnetUnknown = () => [`unknown model type (must be text-to-speech or automatic-speech-recognition)`];
var espnet = (model) => {
  if (model.tags.includes("text-to-speech")) {
    return espnetTTS(model);
  } else if (model.tags.includes("automatic-speech-recognition")) {
    return espnetASR(model);
  }
  return espnetUnknown();
};
var fairseq = (model) => [
  `from fairseq.checkpoint_utils import load_model_ensemble_and_task_from_hf_hub

models, cfg, task = load_model_ensemble_and_task_from_hf_hub(
    "${model.id}"
)`
];
var flair = (model) => [
  `from flair.models import SequenceTagger

tagger = SequenceTagger.load("${model.id}")`
];
var gliner = (model) => [
  `from gliner import GLiNER

model = GLiNER.from_pretrained("${model.id}")`
];
var htrflow = (model) => [
  `# CLI usage
# see docs: https://ai-riksarkivet.github.io/htrflow/latest/getting_started/quick_start.html
htrflow pipeline <path/to/pipeline.yaml> <path/to/image>`,
  `# Python usage
from htrflow.pipeline.pipeline import Pipeline
from htrflow.pipeline.steps import Task
from htrflow.models.framework.model import ModelClass

pipeline = Pipeline(
    [
        Task(
            ModelClass, {"model": "${model.id}"}, {}
        ),
    ])`
];
var keras = (model) => [
  `# Available backend options are: "jax", "torch", "tensorflow".
import os
os.environ["KERAS_BACKEND"] = "jax"
	
import keras

model = keras.saving.load_model("hf://${model.id}")
`
];
var _keras_hub_causal_lm = (modelId) => `
import keras_hub

# Load CausalLM model (optional: use half precision for inference)
causal_lm = keras_hub.models.CausalLM.from_preset("hf://${modelId}", dtype="bfloat16")
causal_lm.compile(sampler="greedy")  # (optional) specify a sampler

# Generate text
causal_lm.generate("Keras: deep learning for", max_length=64)
`;
var _keras_hub_text_to_image = (modelId) => `
import keras_hub

# Load TextToImage model (optional: use half precision for inference)
text_to_image = keras_hub.models.TextToImage.from_preset("hf://${modelId}", dtype="bfloat16")

# Generate images with a TextToImage model.
text_to_image.generate("Astronaut in a jungle")
`;
var _keras_hub_text_classifier = (modelId) => `
import keras_hub

# Load TextClassifier model
text_classifier = keras_hub.models.TextClassifier.from_preset(
    "hf://${modelId}",
    num_classes=2,
)
# Fine-tune
text_classifier.fit(x=["Thilling adventure!", "Total snoozefest."], y=[1, 0])
# Classify text
text_classifier.predict(["Not my cup of tea."])
`;
var _keras_hub_image_classifier = (modelId) => `
import keras_hub
import keras

# Load ImageClassifier model
image_classifier = keras_hub.models.ImageClassifier.from_preset(
    "hf://${modelId}",
    num_classes=2,
)
# Fine-tune
image_classifier.fit(
    x=keras.random.randint((32, 64, 64, 3), 0, 256),
    y=keras.random.randint((32, 1), 0, 2),
)
# Classify image
image_classifier.predict(keras.random.randint((1, 64, 64, 3), 0, 256))
`;
var _keras_hub_tasks_with_example = {
  CausalLM: _keras_hub_causal_lm,
  TextToImage: _keras_hub_text_to_image,
  TextClassifier: _keras_hub_text_classifier,
  ImageClassifier: _keras_hub_image_classifier
};
var _keras_hub_task_without_example = (task, modelId) => `
import keras_hub

# Create a ${task} model
task = keras_hub.models.${task}.from_preset("hf://${modelId}")
`;
var _keras_hub_generic_backbone = (modelId) => `
import keras_hub

# Create a Backbone model unspecialized for any task
backbone = keras_hub.models.Backbone.from_preset("hf://${modelId}")
`;
var keras_hub = (model) => {
  var _a, _b;
  const modelId = model.id;
  const tasks2 = ((_b = (_a = model.config) == null ? void 0 : _a.keras_hub) == null ? void 0 : _b.tasks) ?? [];
  const snippets2 = [];
  for (const [task, snippet] of Object.entries(_keras_hub_tasks_with_example)) {
    if (tasks2.includes(task)) {
      snippets2.push(snippet(modelId));
    }
  }
  for (const task of tasks2) {
    if (!Object.keys(_keras_hub_tasks_with_example).includes(task)) {
      snippets2.push(_keras_hub_task_without_example(task, modelId));
    }
  }
  snippets2.push(_keras_hub_generic_backbone(modelId));
  return snippets2;
};
var lightning_ir = (model) => {
  if (model.tags.includes("bi-encoder")) {
    return [
      `#install from https://github.com/webis-de/lightning-ir

from lightning_ir import BiEncoderModule
model = BiEncoderModule("${model.id}")

model.score("query", ["doc1", "doc2", "doc3"])`
    ];
  } else if (model.tags.includes("cross-encoder")) {
    return [
      `#install from https://github.com/webis-de/lightning-ir

from lightning_ir import CrossEncoderModule
model = CrossEncoderModule("${model.id}")

model.score("query", ["doc1", "doc2", "doc3"])`
    ];
  }
  return [
    `#install from https://github.com/webis-de/lightning-ir

from lightning_ir import BiEncoderModule, CrossEncoderModule

# depending on the model type, use either BiEncoderModule or CrossEncoderModule
model = BiEncoderModule("${model.id}") 
# model = CrossEncoderModule("${model.id}")

model.score("query", ["doc1", "doc2", "doc3"])`
  ];
};
var llama_cpp_python = (model) => {
  const snippets2 = [
    `from llama_cpp import Llama

llm = Llama.from_pretrained(
	repo_id="${model.id}",
	filename="{{GGUF_FILE}}",
)
`
  ];
  if (model.tags.includes("conversational")) {
    const messages = getModelInputSnippet(model);
    snippets2.push(`llm.create_chat_completion(
	messages = ${stringifyMessages(messages, { attributeKeyQuotes: true, indent: "	" })}
)`);
  } else {
    snippets2.push(`output = llm(
	"Once upon a time,",
	max_tokens=512,
	echo=True
)
print(output)`);
  }
  return snippets2;
};
var tf_keras = (model) => [
  `# Note: 'keras<3.x' or 'tf_keras' must be installed (legacy)
# See https://github.com/keras-team/tf-keras for more details.
from huggingface_hub import from_pretrained_keras

model = from_pretrained_keras("${model.id}")
`
];
var mamba_ssm = (model) => [
  `from mamba_ssm import MambaLMHeadModel

model = MambaLMHeadModel.from_pretrained("${model.id}")`
];
var mars5_tts = (model) => [
  `# Install from https://github.com/Camb-ai/MARS5-TTS

from inference import Mars5TTS
mars5 = Mars5TTS.from_pretrained("${model.id}")`
];
var matanyone = (model) => [
  `# Install from https://github.com/pq-yang/MatAnyone.git

from matanyone.model.matanyone import MatAnyone
model = MatAnyone.from_pretrained("${model.id}")`
];
var mesh_anything = () => [
  `# Install from https://github.com/buaacyw/MeshAnything.git

from MeshAnything.models.meshanything import MeshAnything

# refer to https://github.com/buaacyw/MeshAnything/blob/main/main.py#L91 on how to define args
# and https://github.com/buaacyw/MeshAnything/blob/main/app.py regarding usage
model = MeshAnything(args)`
];
var open_clip = (model) => [
  `import open_clip

model, preprocess_train, preprocess_val = open_clip.create_model_and_transforms('hf-hub:${model.id}')
tokenizer = open_clip.get_tokenizer('hf-hub:${model.id}')`
];
var paddlenlp = (model) => {
  var _a, _b;
  if ((_b = (_a = model.config) == null ? void 0 : _a.architectures) == null ? void 0 : _b[0]) {
    const architecture = model.config.architectures[0];
    return [
      [
        `from paddlenlp.transformers import AutoTokenizer, ${architecture}`,
        "",
        `tokenizer = AutoTokenizer.from_pretrained("${model.id}", from_hf_hub=True)`,
        `model = ${architecture}.from_pretrained("${model.id}", from_hf_hub=True)`
      ].join("\n")
    ];
  } else {
    return [
      [
        `# ⚠️ Type of model unknown`,
        `from paddlenlp.transformers import AutoTokenizer, AutoModel`,
        "",
        `tokenizer = AutoTokenizer.from_pretrained("${model.id}", from_hf_hub=True)`,
        `model = AutoModel.from_pretrained("${model.id}", from_hf_hub=True)`
      ].join("\n")
    ];
  }
};
var pyannote_audio_pipeline = (model) => [
  `from pyannote.audio import Pipeline
  
pipeline = Pipeline.from_pretrained("${model.id}")

# inference on the whole file
pipeline("file.wav")

# inference on an excerpt
from pyannote.core import Segment
excerpt = Segment(start=2.0, end=5.0)

from pyannote.audio import Audio
waveform, sample_rate = Audio().crop("file.wav", excerpt)
pipeline({"waveform": waveform, "sample_rate": sample_rate})`
];
var pyannote_audio_model = (model) => [
  `from pyannote.audio import Model, Inference

model = Model.from_pretrained("${model.id}")
inference = Inference(model)

# inference on the whole file
inference("file.wav")

# inference on an excerpt
from pyannote.core import Segment
excerpt = Segment(start=2.0, end=5.0)
inference.crop("file.wav", excerpt)`
];
var pyannote_audio = (model) => {
  if (model.tags.includes("pyannote-audio-pipeline")) {
    return pyannote_audio_pipeline(model);
  }
  return pyannote_audio_model(model);
};
var relik = (model) => [
  `from relik import Relik
 
relik = Relik.from_pretrained("${model.id}")`
];
var tensorflowttsTextToMel = (model) => [
  `from tensorflow_tts.inference import AutoProcessor, TFAutoModel

processor = AutoProcessor.from_pretrained("${model.id}")
model = TFAutoModel.from_pretrained("${model.id}")
`
];
var tensorflowttsMelToWav = (model) => [
  `from tensorflow_tts.inference import TFAutoModel

model = TFAutoModel.from_pretrained("${model.id}")
audios = model.inference(mels)
`
];
var tensorflowttsUnknown = (model) => [
  `from tensorflow_tts.inference import TFAutoModel

model = TFAutoModel.from_pretrained("${model.id}")
`
];
var tensorflowtts = (model) => {
  if (model.tags.includes("text-to-mel")) {
    return tensorflowttsTextToMel(model);
  } else if (model.tags.includes("mel-to-wav")) {
    return tensorflowttsMelToWav(model);
  }
  return tensorflowttsUnknown(model);
};
var timm = (model) => [
  `import timm

model = timm.create_model("hf_hub:${model.id}", pretrained=True)`
];
var saelens = () => [
  `# pip install sae-lens
from sae_lens import SAE

sae, cfg_dict, sparsity = SAE.from_pretrained(
    release = "RELEASE_ID", # e.g., "gpt2-small-res-jb". See other options in https://github.com/jbloomAus/SAELens/blob/main/sae_lens/pretrained_saes.yaml
    sae_id = "SAE_ID", # e.g., "blocks.8.hook_resid_pre". Won't always be a hook point
)`
];
var seed_story = () => [
  `# seed_story_cfg_path refers to 'https://github.com/TencentARC/SEED-Story/blob/master/configs/clm_models/agent_7b_sft.yaml'
# llm_cfg_path refers to 'https://github.com/TencentARC/SEED-Story/blob/master/configs/clm_models/llama2chat7b_lora.yaml'
from omegaconf import OmegaConf
import hydra

# load Llama2
llm_cfg = OmegaConf.load(llm_cfg_path)
llm = hydra.utils.instantiate(llm_cfg, torch_dtype="fp16")

# initialize seed_story
seed_story_cfg = OmegaConf.load(seed_story_cfg_path)
seed_story = hydra.utils.instantiate(seed_story_cfg, llm=llm) `
];
var skopsPickle = (model, modelFile) => {
  return [
    `import joblib
from skops.hub_utils import download
download("${model.id}", "path_to_folder")
model = joblib.load(
	"${modelFile}"
)
# only load pickle files from sources you trust
# read more about it here https://skops.readthedocs.io/en/stable/persistence.html`
  ];
};
var skopsFormat = (model, modelFile) => {
  return [
    `from skops.hub_utils import download
from skops.io import load
download("${model.id}", "path_to_folder")
# make sure model file is in skops format
# if model is a pickle file, make sure it's from a source you trust
model = load("path_to_folder/${modelFile}")`
  ];
};
var skopsJobLib = (model) => {
  return [
    `from huggingface_hub import hf_hub_download
import joblib
model = joblib.load(
	hf_hub_download("${model.id}", "sklearn_model.joblib")
)
# only load pickle files from sources you trust
# read more about it here https://skops.readthedocs.io/en/stable/persistence.html`
  ];
};
var sklearn = (model) => {
  var _a, _b, _c, _d, _e;
  if (model.tags.includes("skops")) {
    const skopsmodelFile = (_c = (_b = (_a = model.config) == null ? void 0 : _a.sklearn) == null ? void 0 : _b.model) == null ? void 0 : _c.file;
    const skopssaveFormat = (_e = (_d = model.config) == null ? void 0 : _d.sklearn) == null ? void 0 : _e.model_format;
    if (!skopsmodelFile) {
      return [`# ⚠️ Model filename not specified in config.json`];
    }
    if (skopssaveFormat === "pickle") {
      return skopsPickle(model, skopsmodelFile);
    } else {
      return skopsFormat(model, skopsmodelFile);
    }
  } else {
    return skopsJobLib(model);
  }
};
var stable_audio_tools = (model) => [
  `import torch
import torchaudio
from einops import rearrange
from stable_audio_tools import get_pretrained_model
from stable_audio_tools.inference.generation import generate_diffusion_cond

device = "cuda" if torch.cuda.is_available() else "cpu"

# Download model
model, model_config = get_pretrained_model("${model.id}")
sample_rate = model_config["sample_rate"]
sample_size = model_config["sample_size"]

model = model.to(device)

# Set up text and timing conditioning
conditioning = [{
	"prompt": "128 BPM tech house drum loop",
}]

# Generate stereo audio
output = generate_diffusion_cond(
	model,
	conditioning=conditioning,
	sample_size=sample_size,
	device=device
)

# Rearrange audio batch to a single sequence
output = rearrange(output, "b d n -> d (b n)")

# Peak normalize, clip, convert to int16, and save to file
output = output.to(torch.float32).div(torch.max(torch.abs(output))).clamp(-1, 1).mul(32767).to(torch.int16).cpu()
torchaudio.save("output.wav", output, sample_rate)`
];
var fastai = (model) => [
  `from huggingface_hub import from_pretrained_fastai

learn = from_pretrained_fastai("${model.id}")`
];
var sam2 = (model) => {
  const image_predictor = `# Use SAM2 with images
import torch
from sam2.sam2_image_predictor import SAM2ImagePredictor

predictor = SAM2ImagePredictor.from_pretrained(${model.id})

with torch.inference_mode(), torch.autocast("cuda", dtype=torch.bfloat16):
    predictor.set_image(<your_image>)
    masks, _, _ = predictor.predict(<input_prompts>)`;
  const video_predictor = `# Use SAM2 with videos
import torch
from sam2.sam2_video_predictor import SAM2VideoPredictor
	
predictor = SAM2VideoPredictor.from_pretrained(${model.id})

with torch.inference_mode(), torch.autocast("cuda", dtype=torch.bfloat16):
    state = predictor.init_state(<your_video>)

    # add new prompts and instantly get the output on the same frame
    frame_idx, object_ids, masks = predictor.add_new_points(state, <your_prompts>):

    # propagate the prompts to get masklets throughout the video
    for frame_idx, object_ids, masks in predictor.propagate_in_video(state):
        ...`;
  return [image_predictor, video_predictor];
};
var sampleFactory = (model) => [
  `python -m sample_factory.huggingface.load_from_hub -r ${model.id} -d ./train_dir`
];
function get_widget_examples_from_st_model(model) {
  var _a;
  const widgetExample = (_a = model.widgetData) == null ? void 0 : _a[0];
  if (widgetExample) {
    return [widgetExample.source_sentence, ...widgetExample.sentences];
  }
}
var sentenceTransformers = (model) => {
  const remote_code_snippet = model.tags.includes(TAG_CUSTOM_CODE) ? ", trust_remote_code=True" : "";
  const exampleSentences = get_widget_examples_from_st_model(model) ?? [
    "The weather is lovely today.",
    "It's so sunny outside!",
    "He drove to the stadium."
  ];
  return [
    `from sentence_transformers import SentenceTransformer

model = SentenceTransformer("${model.id}"${remote_code_snippet})

sentences = ${JSON.stringify(exampleSentences, null, 4)}
embeddings = model.encode(sentences)

similarities = model.similarity(embeddings, embeddings)
print(similarities.shape)
# [${exampleSentences.length}, ${exampleSentences.length}]`
  ];
};
var setfit = (model) => [
  `from setfit import SetFitModel

model = SetFitModel.from_pretrained("${model.id}")`
];
var spacy = (model) => [
  `!pip install https://huggingface.co/${model.id}/resolve/main/${nameWithoutNamespace(model.id)}-any-py3-none-any.whl

# Using spacy.load().
import spacy
nlp = spacy.load("${nameWithoutNamespace(model.id)}")

# Importing as module.
import ${nameWithoutNamespace(model.id)}
nlp = ${nameWithoutNamespace(model.id)}.load()`
];
var span_marker = (model) => [
  `from span_marker import SpanMarkerModel

model = SpanMarkerModel.from_pretrained("${model.id}")`
];
var stanza = (model) => [
  `import stanza

stanza.download("${nameWithoutNamespace(model.id).replace("stanza-", "")}")
nlp = stanza.Pipeline("${nameWithoutNamespace(model.id).replace("stanza-", "")}")`
];
var speechBrainMethod = (speechbrainInterface) => {
  switch (speechbrainInterface) {
    case "EncoderClassifier":
      return "classify_file";
    case "EncoderDecoderASR":
    case "EncoderASR":
      return "transcribe_file";
    case "SpectralMaskEnhancement":
      return "enhance_file";
    case "SepformerSeparation":
      return "separate_file";
    default:
      return void 0;
  }
};
var speechbrain = (model) => {
  var _a, _b;
  const speechbrainInterface = (_b = (_a = model.config) == null ? void 0 : _a.speechbrain) == null ? void 0 : _b.speechbrain_interface;
  if (speechbrainInterface === void 0) {
    return [`# interface not specified in config.json`];
  }
  const speechbrainMethod = speechBrainMethod(speechbrainInterface);
  if (speechbrainMethod === void 0) {
    return [`# interface in config.json invalid`];
  }
  return [
    `from speechbrain.pretrained import ${speechbrainInterface}
model = ${speechbrainInterface}.from_hparams(
  "${model.id}"
)
model.${speechbrainMethod}("file.wav")`
  ];
};
var terratorch = (model) => [
  `from terratorch.registry import BACKBONE_REGISTRY

model = BACKBONE_REGISTRY.build("${model.id}")`
];
var transformers = (model) => {
  var _a, _b, _c, _d, _e;
  const info = model.transformersInfo;
  if (!info) {
    return [`# ⚠️ Type of model unknown`];
  }
  const remote_code_snippet = model.tags.includes(TAG_CUSTOM_CODE) ? ", trust_remote_code=True" : "";
  let autoSnippet;
  if (info.processor) {
    const varName = info.processor === "AutoTokenizer" ? "tokenizer" : info.processor === "AutoFeatureExtractor" ? "extractor" : "processor";
    autoSnippet = [
      "# Load model directly",
      `from transformers import ${info.processor}, ${info.auto_model}`,
      "",
      `${varName} = ${info.processor}.from_pretrained("${model.id}"` + remote_code_snippet + ")",
      `model = ${info.auto_model}.from_pretrained("${model.id}"` + remote_code_snippet + ")"
    ].join("\n");
  } else {
    autoSnippet = [
      "# Load model directly",
      `from transformers import ${info.auto_model}`,
      `model = ${info.auto_model}.from_pretrained("${model.id}"` + remote_code_snippet + ")"
    ].join("\n");
  }
  if (model.pipeline_tag && ((_a = LIBRARY_TASK_MAPPING.transformers) == null ? void 0 : _a.includes(model.pipeline_tag))) {
    const pipelineSnippet = ["# Use a pipeline as a high-level helper", "from transformers import pipeline", ""];
    if (model.tags.includes("conversational") && ((_c = (_b = model.config) == null ? void 0 : _b.tokenizer_config) == null ? void 0 : _c.chat_template)) {
      pipelineSnippet.push("messages = [", '    {"role": "user", "content": "Who are you?"},', "]");
    }
    pipelineSnippet.push(`pipe = pipeline("${model.pipeline_tag}", model="${model.id}"` + remote_code_snippet + ")");
    if (model.tags.includes("conversational") && ((_e = (_d = model.config) == null ? void 0 : _d.tokenizer_config) == null ? void 0 : _e.chat_template)) {
      pipelineSnippet.push("pipe(messages)");
    }
    return [pipelineSnippet.join("\n"), autoSnippet];
  }
  return [autoSnippet];
};
var transformersJS = (model) => {
  if (!model.pipeline_tag) {
    return [`// ⚠️ Unknown pipeline tag`];
  }
  const libName = "@huggingface/transformers";
  return [
    `// npm i ${libName}
import { pipeline } from '${libName}';

// Allocate pipeline
const pipe = await pipeline('${model.pipeline_tag}', '${model.id}');`
  ];
};
var peftTask = (peftTaskType) => {
  switch (peftTaskType) {
    case "CAUSAL_LM":
      return "CausalLM";
    case "SEQ_2_SEQ_LM":
      return "Seq2SeqLM";
    case "TOKEN_CLS":
      return "TokenClassification";
    case "SEQ_CLS":
      return "SequenceClassification";
    default:
      return void 0;
  }
};
var peft = (model) => {
  var _a;
  const { base_model_name_or_path: peftBaseModel, task_type: peftTaskType } = ((_a = model.config) == null ? void 0 : _a.peft) ?? {};
  const pefttask = peftTask(peftTaskType);
  if (!pefttask) {
    return [`Task type is invalid.`];
  }
  if (!peftBaseModel) {
    return [`Base model is not found.`];
  }
  return [
    `from peft import PeftModel
from transformers import AutoModelFor${pefttask}

base_model = AutoModelFor${pefttask}.from_pretrained("${peftBaseModel}")
model = PeftModel.from_pretrained(base_model, "${model.id}")`
  ];
};
var fasttext = (model) => [
  `from huggingface_hub import hf_hub_download
import fasttext

model = fasttext.load_model(hf_hub_download("${model.id}", "model.bin"))`
];
var stableBaselines3 = (model) => [
  `from huggingface_sb3 import load_from_hub
checkpoint = load_from_hub(
	repo_id="${model.id}",
	filename="{MODEL FILENAME}.zip",
)`
];
var nemoDomainResolver = (domain, model) => {
  switch (domain) {
    case "ASR":
      return [
        `import nemo.collections.asr as nemo_asr
asr_model = nemo_asr.models.ASRModel.from_pretrained("${model.id}")

transcriptions = asr_model.transcribe(["file.wav"])`
      ];
    default:
      return void 0;
  }
};
var mlAgents = (model) => [
  `mlagents-load-from-hf --repo-id="${model.id}" --local-dir="./download: string[]s"`
];
var sentis = () => [
  `string modelName = "[Your model name here].sentis";
Model model = ModelLoader.Load(Application.streamingAssetsPath + "/" + modelName);
IWorker engine = WorkerFactory.CreateWorker(BackendType.GPUCompute, model);
// Please see provided C# file for more details
`
];
var sana = (model) => [
  `
# Load the model and infer image from text
import torch
from app.sana_pipeline import SanaPipeline
from torchvision.utils import save_image

sana = SanaPipeline("configs/sana_config/1024ms/Sana_1600M_img1024.yaml")
sana.from_pretrained("hf://${model.id}")

image = sana(
    prompt='a cyberpunk cat with a neon sign that says "Sana"',
    height=1024,
    width=1024,
    guidance_scale=5.0,
    pag_guidance_scale=2.0,
    num_inference_steps=18,
) `
];
var vfimamba = (model) => [
  `from Trainer_finetune import Model

model = Model.from_pretrained("${model.id}")`
];
var voicecraft = (model) => [
  `from voicecraft import VoiceCraft

model = VoiceCraft.from_pretrained("${model.id}")`
];
var chattts = () => [
  `import ChatTTS
import torchaudio

chat = ChatTTS.Chat()
chat.load_models(compile=False) # Set to True for better performance

texts = ["PUT YOUR TEXT HERE",]

wavs = chat.infer(texts, )

torchaudio.save("output1.wav", torch.from_numpy(wavs[0]), 24000)`
];
var ultralytics = (model) => {
  const versionTag = model.tags.find((tag) => tag.match(/^yolov\d+$/));
  const className = versionTag ? `YOLOv${versionTag.slice(4)}` : "YOLOvXX";
  const prefix = versionTag ? "" : `# Couldn't find a valid YOLO version tag.
# Replace XX with the correct version.
`;
  return [
    prefix + `from ultralytics import ${className}

model = ${className}.from_pretrained("${model.id}")
source = 'http://images.cocodataset.org/val2017/000000039769.jpg'
model.predict(source=source, save=True)`
  ];
};
var birefnet = (model) => [
  `# Option 1: use with transformers

from transformers import AutoModelForImageSegmentation
birefnet = AutoModelForImageSegmentation.from_pretrained("${model.id}", trust_remote_code=True)
`,
  `# Option 2: use with BiRefNet

# Install from https://github.com/ZhengPeng7/BiRefNet

from models.birefnet import BiRefNet
model = BiRefNet.from_pretrained("${model.id}")`
];
var swarmformer = (model) => [
  `from swarmformer import SwarmFormerModel

model = SwarmFormerModel.from_pretrained("${model.id}")
`
];
var mlx = (model) => [
  `pip install huggingface_hub hf_transfer

export HF_HUB_ENABLE_HF_TRANSFER=1
huggingface-cli download --local-dir ${nameWithoutNamespace(model.id)} ${model.id}`
];
var mlxim = (model) => [
  `from mlxim.model import create_model

model = create_model(${model.id})`
];
var model2vec = (model) => [
  `from model2vec import StaticModel

model = StaticModel.from_pretrained("${model.id}")`
];
var nemo = (model) => {
  let command = void 0;
  if (model.tags.includes("automatic-speech-recognition")) {
    command = nemoDomainResolver("ASR", model);
  }
  return command ?? [`# tag did not correspond to a valid NeMo domain.`];
};
var pxia = (model) => [
  `from pxia import AutoModel

model = AutoModel.from_pretrained("${model.id}")`
];
var pythae = (model) => [
  `from pythae.models import AutoModel

model = AutoModel.load_from_hf_hub("${model.id}")`
];
var musicgen = (model) => [
  `from audiocraft.models import MusicGen

model = MusicGen.get_pretrained("${model.id}")

descriptions = ['happy rock', 'energetic EDM', 'sad jazz']
wav = model.generate(descriptions)  # generates 3 samples.`
];
var magnet = (model) => [
  `from audiocraft.models import MAGNeT
	
model = MAGNeT.get_pretrained("${model.id}")

descriptions = ['disco beat', 'energetic EDM', 'funky groove']
wav = model.generate(descriptions)  # generates 3 samples.`
];
var audiogen = (model) => [
  `from audiocraft.models import AudioGen
	
model = AudioGen.get_pretrained("${model.id}")
model.set_generation_params(duration=5)  # generate 5 seconds.
descriptions = ['dog barking', 'sirene of an emergency vehicle', 'footsteps in a corridor']
wav = model.generate(descriptions)  # generates 3 samples.`
];
var anemoi = (model) => [
  `from anemoi.inference.runners.default import DefaultRunner
from anemoi.inference.config import Configuration
# Create Configuration
config = Configuration(checkpoint = {"huggingface":{"repo_id":"${model.id}"}})
# Load Runner
runner = DefaultRunner(config)`
];
var audiocraft = (model) => {
  if (model.tags.includes("musicgen")) {
    return musicgen(model);
  } else if (model.tags.includes("audiogen")) {
    return audiogen(model);
  } else if (model.tags.includes("magnet")) {
    return magnet(model);
  } else {
    return [`# Type of model unknown.`];
  }
};
var whisperkit = () => [
  `# Install CLI with Homebrew on macOS device
brew install whisperkit-cli

# View all available inference options
whisperkit-cli transcribe --help
	
# Download and run inference using whisper base model
whisperkit-cli transcribe --audio-path /path/to/audio.mp3

# Or use your preferred model variant
whisperkit-cli transcribe --model "large-v3" --model-prefix "distil" --audio-path /path/to/audio.mp3 --verbose`
];
var threedtopia_xl = (model) => [
  `from threedtopia_xl.models import threedtopia_xl

model = threedtopia_xl.from_pretrained("${model.id}")
model.generate(cond="path/to/image.png")`
];

// node_modules/@huggingface/tasks/dist/esm/model-libraries.js
var MODEL_LIBRARIES_UI_ELEMENTS = {
  "adapter-transformers": {
    prettyLabel: "Adapters",
    repoName: "adapters",
    repoUrl: "https://github.com/Adapter-Hub/adapters",
    docsUrl: "https://huggingface.co/docs/hub/adapters",
    snippets: adapters,
    filter: true,
    countDownloads: `path:"adapter_config.json"`
  },
  allennlp: {
    prettyLabel: "AllenNLP",
    repoName: "AllenNLP",
    repoUrl: "https://github.com/allenai/allennlp",
    docsUrl: "https://huggingface.co/docs/hub/allennlp",
    snippets: allennlp,
    filter: true
  },
  anemoi: {
    prettyLabel: "AnemoI",
    repoName: "AnemoI",
    repoUrl: "https://github.com/ecmwf/anemoi-inference",
    docsUrl: "https://anemoi-docs.readthedocs.io/en/latest/",
    filter: false,
    countDownloads: `path_extension:"ckpt"`,
    snippets: anemoi
  },
  araclip: {
    prettyLabel: "AraClip",
    repoName: "AraClip",
    repoUrl: "https://huggingface.co/Arabic-Clip/araclip",
    filter: false,
    snippets: araclip
  },
  asteroid: {
    prettyLabel: "Asteroid",
    repoName: "Asteroid",
    repoUrl: "https://github.com/asteroid-team/asteroid",
    docsUrl: "https://huggingface.co/docs/hub/asteroid",
    snippets: asteroid,
    filter: true,
    countDownloads: `path:"pytorch_model.bin"`
  },
  audiocraft: {
    prettyLabel: "Audiocraft",
    repoName: "audiocraft",
    repoUrl: "https://github.com/facebookresearch/audiocraft",
    snippets: audiocraft,
    filter: false,
    countDownloads: `path:"state_dict.bin"`
  },
  audioseal: {
    prettyLabel: "AudioSeal",
    repoName: "audioseal",
    repoUrl: "https://github.com/facebookresearch/audioseal",
    filter: false,
    countDownloads: `path_extension:"pth"`,
    snippets: audioseal
  },
  ben2: {
    prettyLabel: "BEN2",
    repoName: "BEN2",
    repoUrl: "https://github.com/PramaLLC/BEN2",
    snippets: ben2,
    filter: false
  },
  bertopic: {
    prettyLabel: "BERTopic",
    repoName: "BERTopic",
    repoUrl: "https://github.com/MaartenGr/BERTopic",
    snippets: bertopic,
    filter: true
  },
  big_vision: {
    prettyLabel: "Big Vision",
    repoName: "big_vision",
    repoUrl: "https://github.com/google-research/big_vision",
    filter: false,
    countDownloads: `path_extension:"npz"`
  },
  birder: {
    prettyLabel: "Birder",
    repoName: "Birder",
    repoUrl: "https://gitlab.com/birder/birder",
    filter: false,
    countDownloads: `path_extension:"pt"`
  },
  birefnet: {
    prettyLabel: "BiRefNet",
    repoName: "BiRefNet",
    repoUrl: "https://github.com/ZhengPeng7/BiRefNet",
    snippets: birefnet,
    filter: false
  },
  bm25s: {
    prettyLabel: "BM25S",
    repoName: "bm25s",
    repoUrl: "https://github.com/xhluca/bm25s",
    snippets: bm25s,
    filter: false,
    countDownloads: `path:"params.index.json"`
  },
  champ: {
    prettyLabel: "Champ",
    repoName: "Champ",
    repoUrl: "https://github.com/fudan-generative-vision/champ",
    countDownloads: `path:"champ/motion_module.pth"`
  },
  chat_tts: {
    prettyLabel: "ChatTTS",
    repoName: "ChatTTS",
    repoUrl: "https://github.com/2noise/ChatTTS.git",
    snippets: chattts,
    filter: false,
    countDownloads: `path:"asset/GPT.pt"`
  },
  colpali: {
    prettyLabel: "ColPali",
    repoName: "ColPali",
    repoUrl: "https://github.com/ManuelFay/colpali",
    filter: false,
    countDownloads: `path:"adapter_config.json"`
  },
  comet: {
    prettyLabel: "COMET",
    repoName: "COMET",
    repoUrl: "https://github.com/Unbabel/COMET/",
    countDownloads: `path:"hparams.yaml"`
  },
  cosmos: {
    prettyLabel: "Cosmos",
    repoName: "Cosmos",
    repoUrl: "https://github.com/NVIDIA/Cosmos",
    countDownloads: `path:"config.json" OR path_extension:"pt"`
  },
  "cxr-foundation": {
    prettyLabel: "CXR Foundation",
    repoName: "cxr-foundation",
    repoUrl: "https://github.com/google-health/cxr-foundation",
    snippets: cxr_foundation,
    filter: false,
    countDownloads: `path:"precomputed_embeddings/embeddings.npz" OR path:"pax-elixr-b-text/saved_model.pb"`
  },
  deepforest: {
    prettyLabel: "DeepForest",
    repoName: "deepforest",
    docsUrl: "https://deepforest.readthedocs.io/en/latest/",
    repoUrl: "https://github.com/weecology/DeepForest"
  },
  "depth-anything-v2": {
    prettyLabel: "DepthAnythingV2",
    repoName: "Depth Anything V2",
    repoUrl: "https://github.com/DepthAnything/Depth-Anything-V2",
    snippets: depth_anything_v2,
    filter: false,
    countDownloads: `path_extension:"pth"`
  },
  "depth-pro": {
    prettyLabel: "Depth Pro",
    repoName: "Depth Pro",
    repoUrl: "https://github.com/apple/ml-depth-pro",
    countDownloads: `path_extension:"pt"`,
    snippets: depth_pro,
    filter: false
  },
  "derm-foundation": {
    prettyLabel: "Derm Foundation",
    repoName: "derm-foundation",
    repoUrl: "https://github.com/google-health/derm-foundation",
    snippets: derm_foundation,
    filter: false,
    countDownloads: `path:"scin_dataset_precomputed_embeddings.npz" OR path:"saved_model.pb"`
  },
  diffree: {
    prettyLabel: "Diffree",
    repoName: "Diffree",
    repoUrl: "https://github.com/OpenGVLab/Diffree",
    filter: false,
    countDownloads: `path:"diffree-step=000010999.ckpt"`
  },
  diffusers: {
    prettyLabel: "Diffusers",
    repoName: "🤗/diffusers",
    repoUrl: "https://github.com/huggingface/diffusers",
    docsUrl: "https://huggingface.co/docs/hub/diffusers",
    snippets: diffusers,
    filter: true
    /// diffusers has its own more complex "countDownloads" query
  },
  diffusionkit: {
    prettyLabel: "DiffusionKit",
    repoName: "DiffusionKit",
    repoUrl: "https://github.com/argmaxinc/DiffusionKit",
    snippets: diffusionkit
  },
  doctr: {
    prettyLabel: "docTR",
    repoName: "doctr",
    repoUrl: "https://github.com/mindee/doctr"
  },
  cartesia_pytorch: {
    prettyLabel: "Cartesia Pytorch",
    repoName: "Cartesia Pytorch",
    repoUrl: "https://github.com/cartesia-ai/cartesia_pytorch",
    snippets: cartesia_pytorch
  },
  cartesia_mlx: {
    prettyLabel: "Cartesia MLX",
    repoName: "Cartesia MLX",
    repoUrl: "https://github.com/cartesia-ai/cartesia_mlx",
    snippets: cartesia_mlx
  },
  clipscope: {
    prettyLabel: "clipscope",
    repoName: "clipscope",
    repoUrl: "https://github.com/Lewington-pitsos/clipscope",
    filter: false,
    countDownloads: `path_extension:"pt"`
  },
  cosyvoice: {
    prettyLabel: "CosyVoice",
    repoName: "CosyVoice",
    repoUrl: "https://github.com/FunAudioLLM/CosyVoice",
    filter: false,
    countDownloads: `path_extension:"onnx" OR path_extension:"pt"`
  },
  cotracker: {
    prettyLabel: "CoTracker",
    repoName: "CoTracker",
    repoUrl: "https://github.com/facebookresearch/co-tracker",
    filter: false,
    countDownloads: `path_extension:"pth"`
  },
  edsnlp: {
    prettyLabel: "EDS-NLP",
    repoName: "edsnlp",
    repoUrl: "https://github.com/aphp/edsnlp",
    docsUrl: "https://aphp.github.io/edsnlp/latest/",
    filter: false,
    snippets: edsnlp,
    countDownloads: `path_filename:"config" AND path_extension:"cfg"`
  },
  elm: {
    prettyLabel: "ELM",
    repoName: "elm",
    repoUrl: "https://github.com/slicex-ai/elm",
    filter: false,
    countDownloads: `path_filename:"slicex_elm_config" AND path_extension:"json"`
  },
  espnet: {
    prettyLabel: "ESPnet",
    repoName: "ESPnet",
    repoUrl: "https://github.com/espnet/espnet",
    docsUrl: "https://huggingface.co/docs/hub/espnet",
    snippets: espnet,
    filter: true
  },
  fairseq: {
    prettyLabel: "Fairseq",
    repoName: "fairseq",
    repoUrl: "https://github.com/pytorch/fairseq",
    snippets: fairseq,
    filter: true
  },
  fastai: {
    prettyLabel: "fastai",
    repoName: "fastai",
    repoUrl: "https://github.com/fastai/fastai",
    docsUrl: "https://huggingface.co/docs/hub/fastai",
    snippets: fastai,
    filter: true
  },
  fasttext: {
    prettyLabel: "fastText",
    repoName: "fastText",
    repoUrl: "https://fasttext.cc/",
    snippets: fasttext,
    filter: true,
    countDownloads: `path_extension:"bin"`
  },
  flair: {
    prettyLabel: "Flair",
    repoName: "Flair",
    repoUrl: "https://github.com/flairNLP/flair",
    docsUrl: "https://huggingface.co/docs/hub/flair",
    snippets: flair,
    filter: true,
    countDownloads: `path:"pytorch_model.bin"`
  },
  "gemma.cpp": {
    prettyLabel: "gemma.cpp",
    repoName: "gemma.cpp",
    repoUrl: "https://github.com/google/gemma.cpp",
    filter: false,
    countDownloads: `path_extension:"sbs"`
  },
  gliner: {
    prettyLabel: "GLiNER",
    repoName: "GLiNER",
    repoUrl: "https://github.com/urchade/GLiNER",
    snippets: gliner,
    filter: false,
    countDownloads: `path:"gliner_config.json"`
  },
  "glyph-byt5": {
    prettyLabel: "Glyph-ByT5",
    repoName: "Glyph-ByT5",
    repoUrl: "https://github.com/AIGText/Glyph-ByT5",
    filter: false,
    countDownloads: `path:"checkpoints/byt5_model.pt"`
  },
  grok: {
    prettyLabel: "Grok",
    repoName: "Grok",
    repoUrl: "https://github.com/xai-org/grok-1",
    filter: false,
    countDownloads: `path:"ckpt/tensor00000_000" OR path:"ckpt-0/tensor00000_000"`
  },
  hallo: {
    prettyLabel: "Hallo",
    repoName: "Hallo",
    repoUrl: "https://github.com/fudan-generative-vision/hallo",
    countDownloads: `path:"hallo/net.pth"`
  },
  hezar: {
    prettyLabel: "Hezar",
    repoName: "Hezar",
    repoUrl: "https://github.com/hezarai/hezar",
    docsUrl: "https://hezarai.github.io/hezar",
    countDownloads: `path:"model_config.yaml" OR path:"embedding/embedding_config.yaml"`
  },
  htrflow: {
    prettyLabel: "HTRflow",
    repoName: "HTRflow",
    repoUrl: "https://github.com/AI-Riksarkivet/htrflow",
    docsUrl: "https://ai-riksarkivet.github.io/htrflow",
    snippets: htrflow
  },
  "hunyuan-dit": {
    prettyLabel: "HunyuanDiT",
    repoName: "HunyuanDiT",
    repoUrl: "https://github.com/Tencent/HunyuanDiT",
    countDownloads: `path:"pytorch_model_ema.pt" OR path:"pytorch_model_distill.pt"`
  },
  "hunyuan3d-2": {
    prettyLabel: "Hunyuan3D-2",
    repoName: "Hunyuan3D-2",
    repoUrl: "https://github.com/Tencent/Hunyuan3D-2",
    countDownloads: `path_filename:"model_index" OR path_filename:"config"`
  },
  imstoucan: {
    prettyLabel: "IMS Toucan",
    repoName: "IMS-Toucan",
    repoUrl: "https://github.com/DigitalPhonetics/IMS-Toucan",
    countDownloads: `path:"embedding_gan.pt" OR path:"Vocoder.pt" OR path:"ToucanTTS.pt"`
  },
  keras: {
    prettyLabel: "Keras",
    repoName: "Keras",
    repoUrl: "https://github.com/keras-team/keras",
    docsUrl: "https://huggingface.co/docs/hub/keras",
    snippets: keras,
    filter: true,
    countDownloads: `path:"config.json" OR path_extension:"keras"`
  },
  "tf-keras": {
    // Legacy "Keras 2" library (tensorflow-only)
    prettyLabel: "TF-Keras",
    repoName: "TF-Keras",
    repoUrl: "https://github.com/keras-team/tf-keras",
    docsUrl: "https://huggingface.co/docs/hub/tf-keras",
    snippets: tf_keras,
    countDownloads: `path:"saved_model.pb"`
  },
  "keras-hub": {
    prettyLabel: "KerasHub",
    repoName: "KerasHub",
    repoUrl: "https://github.com/keras-team/keras-hub",
    docsUrl: "https://keras.io/keras_hub/",
    snippets: keras_hub,
    filter: true
  },
  k2: {
    prettyLabel: "K2",
    repoName: "k2",
    repoUrl: "https://github.com/k2-fsa/k2"
  },
  "lightning-ir": {
    prettyLabel: "Lightning IR",
    repoName: "Lightning IR",
    repoUrl: "https://github.com/webis-de/lightning-ir",
    snippets: lightning_ir
  },
  liveportrait: {
    prettyLabel: "LivePortrait",
    repoName: "LivePortrait",
    repoUrl: "https://github.com/KwaiVGI/LivePortrait",
    filter: false,
    countDownloads: `path:"liveportrait/landmark.onnx"`
  },
  "llama-cpp-python": {
    prettyLabel: "llama-cpp-python",
    repoName: "llama-cpp-python",
    repoUrl: "https://github.com/abetlen/llama-cpp-python",
    snippets: llama_cpp_python
  },
  "mini-omni2": {
    prettyLabel: "Mini-Omni2",
    repoName: "Mini-Omni2",
    repoUrl: "https://github.com/gpt-omni/mini-omni2",
    countDownloads: `path:"model_config.yaml"`
  },
  mindspore: {
    prettyLabel: "MindSpore",
    repoName: "mindspore",
    repoUrl: "https://github.com/mindspore-ai/mindspore"
  },
  "mamba-ssm": {
    prettyLabel: "MambaSSM",
    repoName: "MambaSSM",
    repoUrl: "https://github.com/state-spaces/mamba",
    filter: false,
    snippets: mamba_ssm
  },
  "mars5-tts": {
    prettyLabel: "MARS5-TTS",
    repoName: "MARS5-TTS",
    repoUrl: "https://github.com/Camb-ai/MARS5-TTS",
    filter: false,
    countDownloads: `path:"mars5_ar.safetensors"`,
    snippets: mars5_tts
  },
  matanyone: {
    prettyLabel: "MatAnyone",
    repoName: "MatAnyone",
    repoUrl: "https://github.com/pq-yang/MatAnyone",
    snippets: matanyone,
    filter: false
  },
  "mesh-anything": {
    prettyLabel: "MeshAnything",
    repoName: "MeshAnything",
    repoUrl: "https://github.com/buaacyw/MeshAnything",
    filter: false,
    countDownloads: `path:"MeshAnything_350m.pth"`,
    snippets: mesh_anything
  },
  merlin: {
    prettyLabel: "Merlin",
    repoName: "Merlin",
    repoUrl: "https://github.com/StanfordMIMI/Merlin",
    filter: false,
    countDownloads: `path_extension:"pt"`
  },
  medvae: {
    prettyLabel: "MedVAE",
    repoName: "MedVAE",
    repoUrl: "https://github.com/StanfordMIMI/MedVAE",
    filter: false,
    countDownloads: `path_extension:"ckpt"`
  },
  mitie: {
    prettyLabel: "MITIE",
    repoName: "MITIE",
    repoUrl: "https://github.com/mit-nlp/MITIE",
    countDownloads: `path_filename:"total_word_feature_extractor"`
  },
  "ml-agents": {
    prettyLabel: "ml-agents",
    repoName: "ml-agents",
    repoUrl: "https://github.com/Unity-Technologies/ml-agents",
    docsUrl: "https://huggingface.co/docs/hub/ml-agents",
    snippets: mlAgents,
    filter: true,
    countDownloads: `path_extension:"onnx"`
  },
  mlx: {
    prettyLabel: "MLX",
    repoName: "MLX",
    repoUrl: "https://github.com/ml-explore/mlx-examples/tree/main",
    snippets: mlx,
    filter: true
  },
  "mlx-image": {
    prettyLabel: "mlx-image",
    repoName: "mlx-image",
    repoUrl: "https://github.com/riccardomusmeci/mlx-image",
    docsUrl: "https://huggingface.co/docs/hub/mlx-image",
    snippets: mlxim,
    filter: false,
    countDownloads: `path:"model.safetensors"`
  },
  "mlc-llm": {
    prettyLabel: "MLC-LLM",
    repoName: "MLC-LLM",
    repoUrl: "https://github.com/mlc-ai/mlc-llm",
    docsUrl: "https://llm.mlc.ai/docs/",
    filter: false,
    countDownloads: `path:"mlc-chat-config.json"`
  },
  model2vec: {
    prettyLabel: "Model2Vec",
    repoName: "model2vec",
    repoUrl: "https://github.com/MinishLab/model2vec",
    snippets: model2vec,
    filter: false
  },
  moshi: {
    prettyLabel: "Moshi",
    repoName: "Moshi",
    repoUrl: "https://github.com/kyutai-labs/moshi",
    filter: false,
    countDownloads: `path:"tokenizer-e351c8d8-checkpoint125.safetensors"`
  },
  nemo: {
    prettyLabel: "NeMo",
    repoName: "NeMo",
    repoUrl: "https://github.com/NVIDIA/NeMo",
    snippets: nemo,
    filter: true,
    countDownloads: `path_extension:"nemo" OR path:"model_config.yaml"`
  },
  "open-oasis": {
    prettyLabel: "open-oasis",
    repoName: "open-oasis",
    repoUrl: "https://github.com/etched-ai/open-oasis",
    countDownloads: `path:"oasis500m.safetensors"`
  },
  open_clip: {
    prettyLabel: "OpenCLIP",
    repoName: "OpenCLIP",
    repoUrl: "https://github.com/mlfoundations/open_clip",
    snippets: open_clip,
    filter: true,
    countDownloads: `path:"open_clip_model.safetensors"
			OR path:"model.safetensors"
			OR path:"open_clip_pytorch_model.bin"
			OR path:"pytorch_model.bin"`
  },
  "open-sora": {
    prettyLabel: "Open-Sora",
    repoName: "Open-Sora",
    repoUrl: "https://github.com/hpcaitech/Open-Sora",
    filter: false,
    countDownloads: `path:"Open_Sora_v2.safetensors"`
  },
  paddlenlp: {
    prettyLabel: "paddlenlp",
    repoName: "PaddleNLP",
    repoUrl: "https://github.com/PaddlePaddle/PaddleNLP",
    docsUrl: "https://huggingface.co/docs/hub/paddlenlp",
    snippets: paddlenlp,
    filter: true,
    countDownloads: `path:"model_config.json"`
  },
  peft: {
    prettyLabel: "PEFT",
    repoName: "PEFT",
    repoUrl: "https://github.com/huggingface/peft",
    snippets: peft,
    filter: true,
    countDownloads: `path:"adapter_config.json"`
  },
  pxia: {
    prettyLabel: "pxia",
    repoName: "pxia",
    repoUrl: "https://github.com/not-lain/pxia",
    snippets: pxia,
    filter: false
  },
  "pyannote-audio": {
    prettyLabel: "pyannote.audio",
    repoName: "pyannote-audio",
    repoUrl: "https://github.com/pyannote/pyannote-audio",
    snippets: pyannote_audio,
    filter: true
  },
  "py-feat": {
    prettyLabel: "Py-Feat",
    repoName: "Py-Feat",
    repoUrl: "https://github.com/cosanlab/py-feat",
    docsUrl: "https://py-feat.org/",
    filter: false
  },
  pythae: {
    prettyLabel: "pythae",
    repoName: "pythae",
    repoUrl: "https://github.com/clementchadebec/benchmark_VAE",
    snippets: pythae,
    filter: false
  },
  recurrentgemma: {
    prettyLabel: "RecurrentGemma",
    repoName: "recurrentgemma",
    repoUrl: "https://github.com/google-deepmind/recurrentgemma",
    filter: false,
    countDownloads: `path:"tokenizer.model"`
  },
  relik: {
    prettyLabel: "Relik",
    repoName: "Relik",
    repoUrl: "https://github.com/SapienzaNLP/relik",
    snippets: relik,
    filter: false
  },
  refiners: {
    prettyLabel: "Refiners",
    repoName: "Refiners",
    repoUrl: "https://github.com/finegrain-ai/refiners",
    docsUrl: "https://refine.rs/",
    filter: false,
    countDownloads: `path:"model.safetensors"`
  },
  reverb: {
    prettyLabel: "Reverb",
    repoName: "Reverb",
    repoUrl: "https://github.com/revdotcom/reverb",
    filter: false
  },
  saelens: {
    prettyLabel: "SAELens",
    repoName: "SAELens",
    repoUrl: "https://github.com/jbloomAus/SAELens",
    snippets: saelens,
    filter: false
  },
  sam2: {
    prettyLabel: "sam2",
    repoName: "sam2",
    repoUrl: "https://github.com/facebookresearch/segment-anything-2",
    filter: false,
    snippets: sam2,
    countDownloads: `path_extension:"pt"`
  },
  "sample-factory": {
    prettyLabel: "sample-factory",
    repoName: "sample-factory",
    repoUrl: "https://github.com/alex-petrenko/sample-factory",
    docsUrl: "https://huggingface.co/docs/hub/sample-factory",
    snippets: sampleFactory,
    filter: true,
    countDownloads: `path:"cfg.json"`
  },
  sapiens: {
    prettyLabel: "sapiens",
    repoName: "sapiens",
    repoUrl: "https://github.com/facebookresearch/sapiens",
    filter: false,
    countDownloads: `path_extension:"pt2" OR path_extension:"pth" OR path_extension:"onnx"`
  },
  "sentence-transformers": {
    prettyLabel: "sentence-transformers",
    repoName: "sentence-transformers",
    repoUrl: "https://github.com/UKPLab/sentence-transformers",
    docsUrl: "https://huggingface.co/docs/hub/sentence-transformers",
    snippets: sentenceTransformers,
    filter: true
  },
  setfit: {
    prettyLabel: "setfit",
    repoName: "setfit",
    repoUrl: "https://github.com/huggingface/setfit",
    docsUrl: "https://huggingface.co/docs/hub/setfit",
    snippets: setfit,
    filter: true
  },
  sklearn: {
    prettyLabel: "Scikit-learn",
    repoName: "Scikit-learn",
    repoUrl: "https://github.com/scikit-learn/scikit-learn",
    snippets: sklearn,
    filter: true,
    countDownloads: `path:"sklearn_model.joblib"`
  },
  spacy: {
    prettyLabel: "spaCy",
    repoName: "spaCy",
    repoUrl: "https://github.com/explosion/spaCy",
    docsUrl: "https://huggingface.co/docs/hub/spacy",
    snippets: spacy,
    filter: true,
    countDownloads: `path_extension:"whl"`
  },
  "span-marker": {
    prettyLabel: "SpanMarker",
    repoName: "SpanMarkerNER",
    repoUrl: "https://github.com/tomaarsen/SpanMarkerNER",
    docsUrl: "https://huggingface.co/docs/hub/span_marker",
    snippets: span_marker,
    filter: true
  },
  speechbrain: {
    prettyLabel: "speechbrain",
    repoName: "speechbrain",
    repoUrl: "https://github.com/speechbrain/speechbrain",
    docsUrl: "https://huggingface.co/docs/hub/speechbrain",
    snippets: speechbrain,
    filter: true,
    countDownloads: `path:"hyperparams.yaml"`
  },
  "ssr-speech": {
    prettyLabel: "SSR-Speech",
    repoName: "SSR-Speech",
    repoUrl: "https://github.com/WangHelin1997/SSR-Speech",
    filter: false,
    countDownloads: `path_extension:".pth"`
  },
  "stable-audio-tools": {
    prettyLabel: "Stable Audio Tools",
    repoName: "stable-audio-tools",
    repoUrl: "https://github.com/Stability-AI/stable-audio-tools.git",
    filter: false,
    countDownloads: `path:"model.safetensors"`,
    snippets: stable_audio_tools
  },
  "diffusion-single-file": {
    prettyLabel: "Diffusion Single File",
    repoName: "diffusion-single-file",
    repoUrl: "https://github.com/comfyanonymous/ComfyUI",
    filter: false,
    countDownloads: `path_extension:"safetensors"`
  },
  "seed-story": {
    prettyLabel: "SEED-Story",
    repoName: "SEED-Story",
    repoUrl: "https://github.com/TencentARC/SEED-Story",
    filter: false,
    countDownloads: `path:"cvlm_llama2_tokenizer/tokenizer.model"`,
    snippets: seed_story
  },
  soloaudio: {
    prettyLabel: "SoloAudio",
    repoName: "SoloAudio",
    repoUrl: "https://github.com/WangHelin1997/SoloAudio",
    filter: false,
    countDownloads: `path:"soloaudio_v2.pt"`
  },
  "stable-baselines3": {
    prettyLabel: "stable-baselines3",
    repoName: "stable-baselines3",
    repoUrl: "https://github.com/huggingface/huggingface_sb3",
    docsUrl: "https://huggingface.co/docs/hub/stable-baselines3",
    snippets: stableBaselines3,
    filter: true,
    countDownloads: `path_extension:"zip"`
  },
  stanza: {
    prettyLabel: "Stanza",
    repoName: "stanza",
    repoUrl: "https://github.com/stanfordnlp/stanza",
    docsUrl: "https://huggingface.co/docs/hub/stanza",
    snippets: stanza,
    filter: true,
    countDownloads: `path:"models/default.zip"`
  },
  swarmformer: {
    prettyLabel: "SwarmFormer",
    repoName: "SwarmFormer",
    repoUrl: "https://github.com/takara-ai/SwarmFormer",
    snippets: swarmformer,
    filter: false
  },
  "f5-tts": {
    prettyLabel: "F5-TTS",
    repoName: "F5-TTS",
    repoUrl: "https://github.com/SWivid/F5-TTS",
    filter: false,
    countDownloads: `path_extension:"safetensors" OR path_extension:"pt"`
  },
  genmo: {
    prettyLabel: "Genmo",
    repoName: "Genmo",
    repoUrl: "https://github.com/genmoai/models",
    filter: false,
    countDownloads: `path:"vae_stats.json"`
  },
  tensorflowtts: {
    prettyLabel: "TensorFlowTTS",
    repoName: "TensorFlowTTS",
    repoUrl: "https://github.com/TensorSpeech/TensorFlowTTS",
    snippets: tensorflowtts
  },
  tabpfn: {
    prettyLabel: "TabPFN",
    repoName: "TabPFN",
    repoUrl: "https://github.com/PriorLabs/TabPFN"
  },
  terratorch: {
    prettyLabel: "TerraTorch",
    repoName: "TerraTorch",
    repoUrl: "https://github.com/IBM/terratorch",
    docsUrl: "https://ibm.github.io/terratorch/",
    filter: false,
    countDownloads: `path_extension:"pt"`,
    snippets: terratorch
  },
  "tic-clip": {
    prettyLabel: "TiC-CLIP",
    repoName: "TiC-CLIP",
    repoUrl: "https://github.com/apple/ml-tic-clip",
    filter: false,
    countDownloads: `path_extension:"pt" AND path_prefix:"checkpoints/"`
  },
  timesfm: {
    prettyLabel: "TimesFM",
    repoName: "timesfm",
    repoUrl: "https://github.com/google-research/timesfm",
    filter: false,
    countDownloads: `path:"checkpoints/checkpoint_1100000/state/checkpoint"`
  },
  timm: {
    prettyLabel: "timm",
    repoName: "pytorch-image-models",
    repoUrl: "https://github.com/rwightman/pytorch-image-models",
    docsUrl: "https://huggingface.co/docs/hub/timm",
    snippets: timm,
    filter: true,
    countDownloads: `path:"pytorch_model.bin" OR path:"model.safetensors"`
  },
  transformers: {
    prettyLabel: "Transformers",
    repoName: "🤗/transformers",
    repoUrl: "https://github.com/huggingface/transformers",
    docsUrl: "https://huggingface.co/docs/hub/transformers",
    snippets: transformers,
    filter: true
  },
  "transformers.js": {
    prettyLabel: "Transformers.js",
    repoName: "transformers.js",
    repoUrl: "https://github.com/huggingface/transformers.js",
    docsUrl: "https://huggingface.co/docs/hub/transformers-js",
    snippets: transformersJS,
    filter: true
  },
  trellis: {
    prettyLabel: "Trellis",
    repoName: "Trellis",
    repoUrl: "https://github.com/microsoft/TRELLIS",
    countDownloads: `path_extension:"safetensors"`
  },
  ultralytics: {
    prettyLabel: "ultralytics",
    repoName: "ultralytics",
    repoUrl: "https://github.com/ultralytics/ultralytics",
    docsUrl: "https://github.com/ultralytics/ultralytics",
    filter: false,
    countDownloads: `path_extension:"pt"`,
    snippets: ultralytics
  },
  "uni-3dar": {
    prettyLabel: "Uni-3DAR",
    repoName: "Uni-3DAR",
    repoUrl: "https://github.com/dptech-corp/Uni-3DAR",
    docsUrl: "https://github.com/dptech-corp/Uni-3DAR",
    countDownloads: `path_extension:"pt"`
  },
  "unity-sentis": {
    prettyLabel: "unity-sentis",
    repoName: "unity-sentis",
    repoUrl: "https://github.com/Unity-Technologies/sentis-samples",
    snippets: sentis,
    filter: true,
    countDownloads: `path_extension:"sentis"`
  },
  sana: {
    prettyLabel: "Sana",
    repoName: "Sana",
    repoUrl: "https://github.com/NVlabs/Sana",
    countDownloads: `path_extension:"pth"`,
    snippets: sana
  },
  "vfi-mamba": {
    prettyLabel: "VFIMamba",
    repoName: "VFIMamba",
    repoUrl: "https://github.com/MCG-NJU/VFIMamba",
    countDownloads: `path_extension:"pkl"`,
    snippets: vfimamba
  },
  voicecraft: {
    prettyLabel: "VoiceCraft",
    repoName: "VoiceCraft",
    repoUrl: "https://github.com/jasonppy/VoiceCraft",
    docsUrl: "https://github.com/jasonppy/VoiceCraft",
    snippets: voicecraft
  },
  wham: {
    prettyLabel: "WHAM",
    repoName: "wham",
    repoUrl: "https://huggingface.co/microsoft/wham",
    docsUrl: "https://huggingface.co/microsoft/wham/blob/main/README.md",
    countDownloads: `path_extension:"ckpt"`
  },
  whisperkit: {
    prettyLabel: "WhisperKit",
    repoName: "WhisperKit",
    repoUrl: "https://github.com/argmaxinc/WhisperKit",
    docsUrl: "https://github.com/argmaxinc/WhisperKit?tab=readme-ov-file#homebrew",
    snippets: whisperkit,
    countDownloads: `path_filename:"model" AND path_extension:"mil" AND _exists_:"path_prefix"`
  },
  yolov10: {
    // YOLOv10 is a fork of ultraLytics. Code snippets and download count are the same but the repo is different.
    prettyLabel: "YOLOv10",
    repoName: "YOLOv10",
    repoUrl: "https://github.com/THU-MIG/yolov10",
    docsUrl: "https://github.com/THU-MIG/yolov10",
    countDownloads: `path_extension:"pt" OR path_extension:"safetensors"`,
    snippets: ultralytics
  },
  "3dtopia-xl": {
    prettyLabel: "3DTopia-XL",
    repoName: "3DTopia-XL",
    repoUrl: "https://github.com/3DTopia/3DTopia-XL",
    filter: false,
    countDownloads: `path:"model_vae_fp16.pt"`,
    snippets: threedtopia_xl
  }
};
var ALL_MODEL_LIBRARY_KEYS = Object.keys(MODEL_LIBRARIES_UI_ELEMENTS);
var ALL_DISPLAY_MODEL_LIBRARY_KEYS = Object.entries(MODEL_LIBRARIES_UI_ELEMENTS).filter(([_, v]) => v.filter).map(([k]) => k);

// node_modules/@huggingface/tasks/dist/esm/gguf.js
var GGMLQuantizationType;
(function(GGMLQuantizationType2) {
  GGMLQuantizationType2[GGMLQuantizationType2["F32"] = 0] = "F32";
  GGMLQuantizationType2[GGMLQuantizationType2["F16"] = 1] = "F16";
  GGMLQuantizationType2[GGMLQuantizationType2["Q4_0"] = 2] = "Q4_0";
  GGMLQuantizationType2[GGMLQuantizationType2["Q4_1"] = 3] = "Q4_1";
  GGMLQuantizationType2[GGMLQuantizationType2["Q5_0"] = 6] = "Q5_0";
  GGMLQuantizationType2[GGMLQuantizationType2["Q5_1"] = 7] = "Q5_1";
  GGMLQuantizationType2[GGMLQuantizationType2["Q8_0"] = 8] = "Q8_0";
  GGMLQuantizationType2[GGMLQuantizationType2["Q8_1"] = 9] = "Q8_1";
  GGMLQuantizationType2[GGMLQuantizationType2["Q2_K"] = 10] = "Q2_K";
  GGMLQuantizationType2[GGMLQuantizationType2["Q3_K"] = 11] = "Q3_K";
  GGMLQuantizationType2[GGMLQuantizationType2["Q4_K"] = 12] = "Q4_K";
  GGMLQuantizationType2[GGMLQuantizationType2["Q5_K"] = 13] = "Q5_K";
  GGMLQuantizationType2[GGMLQuantizationType2["Q6_K"] = 14] = "Q6_K";
  GGMLQuantizationType2[GGMLQuantizationType2["Q8_K"] = 15] = "Q8_K";
  GGMLQuantizationType2[GGMLQuantizationType2["IQ2_XXS"] = 16] = "IQ2_XXS";
  GGMLQuantizationType2[GGMLQuantizationType2["IQ2_XS"] = 17] = "IQ2_XS";
  GGMLQuantizationType2[GGMLQuantizationType2["IQ3_XXS"] = 18] = "IQ3_XXS";
  GGMLQuantizationType2[GGMLQuantizationType2["IQ1_S"] = 19] = "IQ1_S";
  GGMLQuantizationType2[GGMLQuantizationType2["IQ4_NL"] = 20] = "IQ4_NL";
  GGMLQuantizationType2[GGMLQuantizationType2["IQ3_S"] = 21] = "IQ3_S";
  GGMLQuantizationType2[GGMLQuantizationType2["IQ2_S"] = 22] = "IQ2_S";
  GGMLQuantizationType2[GGMLQuantizationType2["IQ4_XS"] = 23] = "IQ4_XS";
  GGMLQuantizationType2[GGMLQuantizationType2["I8"] = 24] = "I8";
  GGMLQuantizationType2[GGMLQuantizationType2["I16"] = 25] = "I16";
  GGMLQuantizationType2[GGMLQuantizationType2["I32"] = 26] = "I32";
  GGMLQuantizationType2[GGMLQuantizationType2["I64"] = 27] = "I64";
  GGMLQuantizationType2[GGMLQuantizationType2["F64"] = 28] = "F64";
  GGMLQuantizationType2[GGMLQuantizationType2["IQ1_M"] = 29] = "IQ1_M";
  GGMLQuantizationType2[GGMLQuantizationType2["BF16"] = 30] = "BF16";
})(GGMLQuantizationType || (GGMLQuantizationType = {}));
var ggufQuants = Object.values(GGMLQuantizationType).filter((v) => typeof v === "string");
var GGUF_QUANT_RE = new RegExp(`(?<quant>${ggufQuants.join("|")})(_(?<sizeVariation>[A-Z]+))?`);
var GGUF_QUANT_RE_GLOBAL = new RegExp(GGUF_QUANT_RE, "g");

// node_modules/@huggingface/tasks/dist/esm/snippets/types.js
var inferenceSnippetLanguages = ["python", "js", "sh"];

// node_modules/@huggingface/tasks/dist/esm/hardware.js
var TFLOPS_THRESHOLD_WHITE_HOUSE_MODEL_TRAINING_TOTAL = 10 ** 14;
var TFLOPS_THRESHOLD_WHITE_HOUSE_MODEL_TRAINING_TOTAL_BIOLOGY = 10 ** 11;
var TFLOPS_THRESHOLD_WHITE_HOUSE_CLUSTER = 10 ** 8;
var TFLOPS_THRESHOLD_EU_AI_ACT_MODEL_TRAINING_TOTAL = 10 ** 13;

// node_modules/@huggingface/jinja/dist/index.js
var TOKEN_TYPES = Object.freeze({
  Text: "Text",
  // The text between Jinja statements or expressions
  NumericLiteral: "NumericLiteral",
  // e.g., 123
  BooleanLiteral: "BooleanLiteral",
  // true or false
  NullLiteral: "NullLiteral",
  // none
  StringLiteral: "StringLiteral",
  // 'string'
  Identifier: "Identifier",
  // Variables, functions, etc.
  Equals: "Equals",
  // =
  OpenParen: "OpenParen",
  // (
  CloseParen: "CloseParen",
  // )
  OpenStatement: "OpenStatement",
  // {%
  CloseStatement: "CloseStatement",
  // %}
  OpenExpression: "OpenExpression",
  // {{
  CloseExpression: "CloseExpression",
  // }}
  OpenSquareBracket: "OpenSquareBracket",
  // [
  CloseSquareBracket: "CloseSquareBracket",
  // ]
  OpenCurlyBracket: "OpenCurlyBracket",
  // {
  CloseCurlyBracket: "CloseCurlyBracket",
  // }
  Comma: "Comma",
  // ,
  Dot: "Dot",
  // .
  Colon: "Colon",
  // :
  Pipe: "Pipe",
  // |
  CallOperator: "CallOperator",
  // ()
  AdditiveBinaryOperator: "AdditiveBinaryOperator",
  // + -
  MultiplicativeBinaryOperator: "MultiplicativeBinaryOperator",
  // * / %
  ComparisonBinaryOperator: "ComparisonBinaryOperator",
  // < > <= >= == !=
  UnaryOperator: "UnaryOperator",
  // ! - +
  // Keywords
  Set: "Set",
  If: "If",
  For: "For",
  In: "In",
  Is: "Is",
  NotIn: "NotIn",
  Else: "Else",
  EndIf: "EndIf",
  ElseIf: "ElseIf",
  EndFor: "EndFor",
  And: "And",
  Or: "Or",
  Not: "UnaryOperator",
  Macro: "Macro",
  EndMacro: "EndMacro"
});
var KEYWORDS = Object.freeze({
  set: TOKEN_TYPES.Set,
  for: TOKEN_TYPES.For,
  in: TOKEN_TYPES.In,
  is: TOKEN_TYPES.Is,
  if: TOKEN_TYPES.If,
  else: TOKEN_TYPES.Else,
  endif: TOKEN_TYPES.EndIf,
  elif: TOKEN_TYPES.ElseIf,
  endfor: TOKEN_TYPES.EndFor,
  and: TOKEN_TYPES.And,
  or: TOKEN_TYPES.Or,
  not: TOKEN_TYPES.Not,
  "not in": TOKEN_TYPES.NotIn,
  macro: TOKEN_TYPES.Macro,
  endmacro: TOKEN_TYPES.EndMacro,
  // Literals
  true: TOKEN_TYPES.BooleanLiteral,
  false: TOKEN_TYPES.BooleanLiteral,
  none: TOKEN_TYPES.NullLiteral,
  // NOTE: According to the Jinja docs: The special constants true, false, and none are indeed lowercase.
  // Because that caused confusion in the past, (True used to expand to an undefined variable that was considered false),
  // all three can now also be written in title case (True, False, and None). However, for consistency, (all Jinja identifiers are lowercase)
  // you should use the lowercase versions.
  True: TOKEN_TYPES.BooleanLiteral,
  False: TOKEN_TYPES.BooleanLiteral,
  None: TOKEN_TYPES.NullLiteral
});
var Token = class {
  /**
   * Constructs a new Token.
   * @param {string} value The raw value as seen inside the source code.
   * @param {TokenType} type The type of token.
   */
  constructor(value, type) {
    this.value = value;
    this.type = type;
  }
};
function isWord(char) {
  return /\w/.test(char);
}
function isInteger(char) {
  return /[0-9]/.test(char);
}
var ORDERED_MAPPING_TABLE = [
  // Control sequences
  ["{%", TOKEN_TYPES.OpenStatement],
  ["%}", TOKEN_TYPES.CloseStatement],
  ["{{", TOKEN_TYPES.OpenExpression],
  ["}}", TOKEN_TYPES.CloseExpression],
  // Single character tokens
  ["(", TOKEN_TYPES.OpenParen],
  [")", TOKEN_TYPES.CloseParen],
  ["{", TOKEN_TYPES.OpenCurlyBracket],
  ["}", TOKEN_TYPES.CloseCurlyBracket],
  ["[", TOKEN_TYPES.OpenSquareBracket],
  ["]", TOKEN_TYPES.CloseSquareBracket],
  [",", TOKEN_TYPES.Comma],
  [".", TOKEN_TYPES.Dot],
  [":", TOKEN_TYPES.Colon],
  ["|", TOKEN_TYPES.Pipe],
  // Comparison operators
  ["<=", TOKEN_TYPES.ComparisonBinaryOperator],
  [">=", TOKEN_TYPES.ComparisonBinaryOperator],
  ["==", TOKEN_TYPES.ComparisonBinaryOperator],
  ["!=", TOKEN_TYPES.ComparisonBinaryOperator],
  ["<", TOKEN_TYPES.ComparisonBinaryOperator],
  [">", TOKEN_TYPES.ComparisonBinaryOperator],
  // Arithmetic operators
  ["+", TOKEN_TYPES.AdditiveBinaryOperator],
  ["-", TOKEN_TYPES.AdditiveBinaryOperator],
  ["*", TOKEN_TYPES.MultiplicativeBinaryOperator],
  ["/", TOKEN_TYPES.MultiplicativeBinaryOperator],
  ["%", TOKEN_TYPES.MultiplicativeBinaryOperator],
  // Assignment operator
  ["=", TOKEN_TYPES.Equals]
];
var ESCAPE_CHARACTERS = /* @__PURE__ */ new Map([
  ["n", "\n"],
  // New line
  ["t", "	"],
  // Horizontal tab
  ["r", "\r"],
  // Carriage return
  ["b", "\b"],
  // Backspace
  ["f", "\f"],
  // Form feed
  ["v", "\v"],
  // Vertical tab
  ["'", "'"],
  // Single quote
  ['"', '"'],
  // Double quote
  ["\\", "\\"]
  // Backslash
]);
function preprocess(template, options = {}) {
  if (template.endsWith("\n")) {
    template = template.slice(0, -1);
  }
  template = template.replace(/{#.*?#}/gs, "{##}");
  if (options.lstrip_blocks) {
    template = template.replace(/^[ \t]*({[#%])/gm, "$1");
  }
  if (options.trim_blocks) {
    template = template.replace(/([#%]})\n/g, "$1");
  }
  return template.replace(/{##}/g, "").replace(/-%}\s*/g, "%}").replace(/\s*{%-/g, "{%").replace(/-}}\s*/g, "}}").replace(/\s*{{-/g, "{{");
}
function tokenize(source, options = {}) {
  var _a, _b, _c;
  const tokens = [];
  const src = preprocess(source, options);
  let cursorPosition = 0;
  const consumeWhile = (predicate) => {
    let str = "";
    while (predicate(src[cursorPosition])) {
      if (src[cursorPosition] === "\\") {
        ++cursorPosition;
        if (cursorPosition >= src.length)
          throw new SyntaxError("Unexpected end of input");
        const escaped = src[cursorPosition++];
        const unescaped = ESCAPE_CHARACTERS.get(escaped);
        if (unescaped === void 0) {
          throw new SyntaxError(`Unexpected escaped character: ${escaped}`);
        }
        str += unescaped;
        continue;
      }
      str += src[cursorPosition++];
      if (cursorPosition >= src.length)
        throw new SyntaxError("Unexpected end of input");
    }
    return str;
  };
  main:
    while (cursorPosition < src.length) {
      const lastTokenType = (_a = tokens.at(-1)) == null ? void 0 : _a.type;
      if (lastTokenType === void 0 || lastTokenType === TOKEN_TYPES.CloseStatement || lastTokenType === TOKEN_TYPES.CloseExpression) {
        let text = "";
        while (cursorPosition < src.length && // Keep going until we hit the next Jinja statement or expression
        !(src[cursorPosition] === "{" && (src[cursorPosition + 1] === "%" || src[cursorPosition + 1] === "{"))) {
          text += src[cursorPosition++];
        }
        if (text.length > 0) {
          tokens.push(new Token(text, TOKEN_TYPES.Text));
          continue;
        }
      }
      consumeWhile((char2) => /\s/.test(char2));
      const char = src[cursorPosition];
      if (char === "-" || char === "+") {
        const lastTokenType2 = (_b = tokens.at(-1)) == null ? void 0 : _b.type;
        if (lastTokenType2 === TOKEN_TYPES.Text || lastTokenType2 === void 0) {
          throw new SyntaxError(`Unexpected character: ${char}`);
        }
        switch (lastTokenType2) {
          case TOKEN_TYPES.Identifier:
          case TOKEN_TYPES.NumericLiteral:
          case TOKEN_TYPES.BooleanLiteral:
          case TOKEN_TYPES.NullLiteral:
          case TOKEN_TYPES.StringLiteral:
          case TOKEN_TYPES.CloseParen:
          case TOKEN_TYPES.CloseSquareBracket:
            break;
          default: {
            ++cursorPosition;
            const num = consumeWhile(isInteger);
            tokens.push(
              new Token(`${char}${num}`, num.length > 0 ? TOKEN_TYPES.NumericLiteral : TOKEN_TYPES.UnaryOperator)
            );
            continue;
          }
        }
      }
      for (const [char2, token] of ORDERED_MAPPING_TABLE) {
        const slice2 = src.slice(cursorPosition, cursorPosition + char2.length);
        if (slice2 === char2) {
          tokens.push(new Token(char2, token));
          cursorPosition += char2.length;
          continue main;
        }
      }
      if (char === "'" || char === '"') {
        ++cursorPosition;
        const str = consumeWhile((c) => c !== char);
        tokens.push(new Token(str, TOKEN_TYPES.StringLiteral));
        ++cursorPosition;
        continue;
      }
      if (isInteger(char)) {
        const num = consumeWhile(isInteger);
        tokens.push(new Token(num, TOKEN_TYPES.NumericLiteral));
        continue;
      }
      if (isWord(char)) {
        const word = consumeWhile(isWord);
        const type = Object.hasOwn(KEYWORDS, word) ? KEYWORDS[word] : TOKEN_TYPES.Identifier;
        if (type === TOKEN_TYPES.In && ((_c = tokens.at(-1)) == null ? void 0 : _c.type) === TOKEN_TYPES.Not) {
          tokens.pop();
          tokens.push(new Token("not in", TOKEN_TYPES.NotIn));
        } else {
          tokens.push(new Token(word, type));
        }
        continue;
      }
      throw new SyntaxError(`Unexpected character: ${char}`);
    }
  return tokens;
}
var Statement = class {
  constructor() {
    __publicField(this, "type", "Statement");
  }
};
var Program = class extends Statement {
  constructor(body) {
    super();
    __publicField(this, "type", "Program");
    this.body = body;
  }
};
var If = class extends Statement {
  constructor(test, body, alternate) {
    super();
    __publicField(this, "type", "If");
    this.test = test;
    this.body = body;
    this.alternate = alternate;
  }
};
var For = class extends Statement {
  constructor(loopvar, iterable, body, defaultBlock) {
    super();
    __publicField(this, "type", "For");
    this.loopvar = loopvar;
    this.iterable = iterable;
    this.body = body;
    this.defaultBlock = defaultBlock;
  }
};
var SetStatement = class extends Statement {
  constructor(assignee, value) {
    super();
    __publicField(this, "type", "Set");
    this.assignee = assignee;
    this.value = value;
  }
};
var Macro = class extends Statement {
  constructor(name2, args, body) {
    super();
    __publicField(this, "type", "Macro");
    this.name = name2;
    this.args = args;
    this.body = body;
  }
};
var Expression = class extends Statement {
  constructor() {
    super(...arguments);
    __publicField(this, "type", "Expression");
  }
};
var MemberExpression = class extends Expression {
  constructor(object, property, computed) {
    super();
    __publicField(this, "type", "MemberExpression");
    this.object = object;
    this.property = property;
    this.computed = computed;
  }
};
var CallExpression = class extends Expression {
  constructor(callee, args) {
    super();
    __publicField(this, "type", "CallExpression");
    this.callee = callee;
    this.args = args;
  }
};
var Identifier = class extends Expression {
  /**
   * @param {string} value The name of the identifier
   */
  constructor(value) {
    super();
    __publicField(this, "type", "Identifier");
    this.value = value;
  }
};
var Literal = class extends Expression {
  constructor(value) {
    super();
    __publicField(this, "type", "Literal");
    this.value = value;
  }
};
var NumericLiteral = class extends Literal {
  constructor() {
    super(...arguments);
    __publicField(this, "type", "NumericLiteral");
  }
};
var StringLiteral = class extends Literal {
  constructor() {
    super(...arguments);
    __publicField(this, "type", "StringLiteral");
  }
};
var BooleanLiteral = class extends Literal {
  constructor() {
    super(...arguments);
    __publicField(this, "type", "BooleanLiteral");
  }
};
var NullLiteral = class extends Literal {
  constructor() {
    super(...arguments);
    __publicField(this, "type", "NullLiteral");
  }
};
var ArrayLiteral = class extends Literal {
  constructor() {
    super(...arguments);
    __publicField(this, "type", "ArrayLiteral");
  }
};
var TupleLiteral = class extends Literal {
  constructor() {
    super(...arguments);
    __publicField(this, "type", "TupleLiteral");
  }
};
var ObjectLiteral = class extends Literal {
  constructor() {
    super(...arguments);
    __publicField(this, "type", "ObjectLiteral");
  }
};
var BinaryExpression = class extends Expression {
  constructor(operator, left, right) {
    super();
    __publicField(this, "type", "BinaryExpression");
    this.operator = operator;
    this.left = left;
    this.right = right;
  }
};
var FilterExpression = class extends Expression {
  constructor(operand, filter) {
    super();
    __publicField(this, "type", "FilterExpression");
    this.operand = operand;
    this.filter = filter;
  }
};
var SelectExpression = class extends Expression {
  constructor(iterable, test) {
    super();
    __publicField(this, "type", "SelectExpression");
    this.iterable = iterable;
    this.test = test;
  }
};
var TestExpression = class extends Expression {
  constructor(operand, negate, test) {
    super();
    __publicField(this, "type", "TestExpression");
    this.operand = operand;
    this.negate = negate;
    this.test = test;
  }
};
var UnaryExpression = class extends Expression {
  constructor(operator, argument) {
    super();
    __publicField(this, "type", "UnaryExpression");
    this.operator = operator;
    this.argument = argument;
  }
};
var SliceExpression = class extends Expression {
  constructor(start = void 0, stop = void 0, step = void 0) {
    super();
    __publicField(this, "type", "SliceExpression");
    this.start = start;
    this.stop = stop;
    this.step = step;
  }
};
var KeywordArgumentExpression = class extends Expression {
  constructor(key, value) {
    super();
    __publicField(this, "type", "KeywordArgumentExpression");
    this.key = key;
    this.value = value;
  }
};
function parse(tokens) {
  const program = new Program([]);
  let current = 0;
  function expect(type, error) {
    const prev = tokens[current++];
    if (!prev || prev.type !== type) {
      throw new Error(`Parser Error: ${error}. ${prev.type} !== ${type}.`);
    }
    return prev;
  }
  function parseAny() {
    switch (tokens[current].type) {
      case TOKEN_TYPES.Text:
        return parseText();
      case TOKEN_TYPES.OpenStatement:
        return parseJinjaStatement();
      case TOKEN_TYPES.OpenExpression:
        return parseJinjaExpression();
      default:
        throw new SyntaxError(`Unexpected token type: ${tokens[current].type}`);
    }
  }
  function not(...types) {
    return current + types.length <= tokens.length && types.some((type, i) => type !== tokens[current + i].type);
  }
  function is(...types) {
    return current + types.length <= tokens.length && types.every((type, i) => type === tokens[current + i].type);
  }
  function parseText() {
    return new StringLiteral(expect(TOKEN_TYPES.Text, "Expected text token").value);
  }
  function parseJinjaStatement() {
    expect(TOKEN_TYPES.OpenStatement, "Expected opening statement token");
    let result;
    switch (tokens[current].type) {
      case TOKEN_TYPES.Set:
        ++current;
        result = parseSetStatement();
        expect(TOKEN_TYPES.CloseStatement, "Expected closing statement token");
        break;
      case TOKEN_TYPES.If:
        ++current;
        result = parseIfStatement();
        expect(TOKEN_TYPES.OpenStatement, "Expected {% token");
        expect(TOKEN_TYPES.EndIf, "Expected endif token");
        expect(TOKEN_TYPES.CloseStatement, "Expected %} token");
        break;
      case TOKEN_TYPES.Macro:
        ++current;
        result = parseMacroStatement();
        expect(TOKEN_TYPES.OpenStatement, "Expected {% token");
        expect(TOKEN_TYPES.EndMacro, "Expected endmacro token");
        expect(TOKEN_TYPES.CloseStatement, "Expected %} token");
        break;
      case TOKEN_TYPES.For:
        ++current;
        result = parseForStatement();
        expect(TOKEN_TYPES.OpenStatement, "Expected {% token");
        expect(TOKEN_TYPES.EndFor, "Expected endfor token");
        expect(TOKEN_TYPES.CloseStatement, "Expected %} token");
        break;
      default:
        throw new SyntaxError(`Unknown statement type: ${tokens[current].type}`);
    }
    return result;
  }
  function parseJinjaExpression() {
    expect(TOKEN_TYPES.OpenExpression, "Expected opening expression token");
    const result = parseExpression();
    expect(TOKEN_TYPES.CloseExpression, "Expected closing expression token");
    return result;
  }
  function parseSetStatement() {
    const left = parseExpression();
    if (is(TOKEN_TYPES.Equals)) {
      ++current;
      const value = parseSetStatement();
      return new SetStatement(left, value);
    }
    return left;
  }
  function parseIfStatement() {
    var _a, _b, _c, _d, _e, _f, _g, _h;
    const test = parseExpression();
    expect(TOKEN_TYPES.CloseStatement, "Expected closing statement token");
    const body = [];
    const alternate = [];
    while (!(((_a = tokens[current]) == null ? void 0 : _a.type) === TOKEN_TYPES.OpenStatement && (((_b = tokens[current + 1]) == null ? void 0 : _b.type) === TOKEN_TYPES.ElseIf || ((_c = tokens[current + 1]) == null ? void 0 : _c.type) === TOKEN_TYPES.Else || ((_d = tokens[current + 1]) == null ? void 0 : _d.type) === TOKEN_TYPES.EndIf))) {
      body.push(parseAny());
    }
    if (((_e = tokens[current]) == null ? void 0 : _e.type) === TOKEN_TYPES.OpenStatement && ((_f = tokens[current + 1]) == null ? void 0 : _f.type) !== TOKEN_TYPES.EndIf) {
      ++current;
      if (is(TOKEN_TYPES.ElseIf)) {
        expect(TOKEN_TYPES.ElseIf, "Expected elseif token");
        alternate.push(parseIfStatement());
      } else {
        expect(TOKEN_TYPES.Else, "Expected else token");
        expect(TOKEN_TYPES.CloseStatement, "Expected closing statement token");
        while (!(((_g = tokens[current]) == null ? void 0 : _g.type) === TOKEN_TYPES.OpenStatement && ((_h = tokens[current + 1]) == null ? void 0 : _h.type) === TOKEN_TYPES.EndIf)) {
          alternate.push(parseAny());
        }
      }
    }
    return new If(test, body, alternate);
  }
  function parseMacroStatement() {
    const name2 = parsePrimaryExpression();
    if (name2.type !== "Identifier") {
      throw new SyntaxError(`Expected identifier following macro statement`);
    }
    const args = parseArgs();
    expect(TOKEN_TYPES.CloseStatement, "Expected closing statement token");
    const body = [];
    while (not(TOKEN_TYPES.OpenStatement, TOKEN_TYPES.EndMacro)) {
      body.push(parseAny());
    }
    return new Macro(name2, args, body);
  }
  function parseExpressionSequence(primary = false) {
    const fn = primary ? parsePrimaryExpression : parseExpression;
    const expressions = [fn()];
    const isTuple = is(TOKEN_TYPES.Comma);
    while (isTuple) {
      ++current;
      expressions.push(fn());
      if (!is(TOKEN_TYPES.Comma)) {
        break;
      }
    }
    return isTuple ? new TupleLiteral(expressions) : expressions[0];
  }
  function parseForStatement() {
    const loopVariable = parseExpressionSequence(true);
    if (!(loopVariable instanceof Identifier || loopVariable instanceof TupleLiteral)) {
      throw new SyntaxError(`Expected identifier/tuple for the loop variable, got ${loopVariable.type} instead`);
    }
    expect(TOKEN_TYPES.In, "Expected `in` keyword following loop variable");
    const iterable = parseExpression();
    expect(TOKEN_TYPES.CloseStatement, "Expected closing statement token");
    const body = [];
    while (not(TOKEN_TYPES.OpenStatement, TOKEN_TYPES.EndFor) && not(TOKEN_TYPES.OpenStatement, TOKEN_TYPES.Else)) {
      body.push(parseAny());
    }
    const alternative = [];
    if (is(TOKEN_TYPES.OpenStatement, TOKEN_TYPES.Else)) {
      ++current;
      ++current;
      expect(TOKEN_TYPES.CloseStatement, "Expected closing statement token");
      while (not(TOKEN_TYPES.OpenStatement, TOKEN_TYPES.EndFor)) {
        alternative.push(parseAny());
      }
    }
    return new For(loopVariable, iterable, body, alternative);
  }
  function parseExpression() {
    return parseIfExpression();
  }
  function parseIfExpression() {
    const a = parseLogicalOrExpression();
    if (is(TOKEN_TYPES.If)) {
      ++current;
      const predicate = parseLogicalOrExpression();
      if (is(TOKEN_TYPES.Else)) {
        ++current;
        const b = parseLogicalOrExpression();
        return new If(predicate, [a], [b]);
      } else {
        return new SelectExpression(a, predicate);
      }
    }
    return a;
  }
  function parseLogicalOrExpression() {
    let left = parseLogicalAndExpression();
    while (is(TOKEN_TYPES.Or)) {
      const operator = tokens[current];
      ++current;
      const right = parseLogicalAndExpression();
      left = new BinaryExpression(operator, left, right);
    }
    return left;
  }
  function parseLogicalAndExpression() {
    let left = parseLogicalNegationExpression();
    while (is(TOKEN_TYPES.And)) {
      const operator = tokens[current];
      ++current;
      const right = parseLogicalNegationExpression();
      left = new BinaryExpression(operator, left, right);
    }
    return left;
  }
  function parseLogicalNegationExpression() {
    let right;
    while (is(TOKEN_TYPES.Not)) {
      const operator = tokens[current];
      ++current;
      const arg = parseLogicalNegationExpression();
      right = new UnaryExpression(operator, arg);
    }
    return right ?? parseComparisonExpression();
  }
  function parseComparisonExpression() {
    let left = parseAdditiveExpression();
    while (is(TOKEN_TYPES.ComparisonBinaryOperator) || is(TOKEN_TYPES.In) || is(TOKEN_TYPES.NotIn)) {
      const operator = tokens[current];
      ++current;
      const right = parseAdditiveExpression();
      left = new BinaryExpression(operator, left, right);
    }
    return left;
  }
  function parseAdditiveExpression() {
    let left = parseMultiplicativeExpression();
    while (is(TOKEN_TYPES.AdditiveBinaryOperator)) {
      const operator = tokens[current];
      ++current;
      const right = parseMultiplicativeExpression();
      left = new BinaryExpression(operator, left, right);
    }
    return left;
  }
  function parseCallMemberExpression() {
    const member = parseMemberExpression(parsePrimaryExpression());
    if (is(TOKEN_TYPES.OpenParen)) {
      return parseCallExpression(member);
    }
    return member;
  }
  function parseCallExpression(callee) {
    let expression = new CallExpression(callee, parseArgs());
    expression = parseMemberExpression(expression);
    if (is(TOKEN_TYPES.OpenParen)) {
      expression = parseCallExpression(expression);
    }
    return expression;
  }
  function parseArgs() {
    expect(TOKEN_TYPES.OpenParen, "Expected opening parenthesis for arguments list");
    const args = parseArgumentsList();
    expect(TOKEN_TYPES.CloseParen, "Expected closing parenthesis for arguments list");
    return args;
  }
  function parseArgumentsList() {
    const args = [];
    while (!is(TOKEN_TYPES.CloseParen)) {
      let argument = parseExpression();
      if (is(TOKEN_TYPES.Equals)) {
        ++current;
        if (!(argument instanceof Identifier)) {
          throw new SyntaxError(`Expected identifier for keyword argument`);
        }
        const value = parseExpression();
        argument = new KeywordArgumentExpression(argument, value);
      }
      args.push(argument);
      if (is(TOKEN_TYPES.Comma)) {
        ++current;
      }
    }
    return args;
  }
  function parseMemberExpressionArgumentsList() {
    const slices = [];
    let isSlice = false;
    while (!is(TOKEN_TYPES.CloseSquareBracket)) {
      if (is(TOKEN_TYPES.Colon)) {
        slices.push(void 0);
        ++current;
        isSlice = true;
      } else {
        slices.push(parseExpression());
        if (is(TOKEN_TYPES.Colon)) {
          ++current;
          isSlice = true;
        }
      }
    }
    if (slices.length === 0) {
      throw new SyntaxError(`Expected at least one argument for member/slice expression`);
    }
    if (isSlice) {
      if (slices.length > 3) {
        throw new SyntaxError(`Expected 0-3 arguments for slice expression`);
      }
      return new SliceExpression(...slices);
    }
    return slices[0];
  }
  function parseMemberExpression(object) {
    while (is(TOKEN_TYPES.Dot) || is(TOKEN_TYPES.OpenSquareBracket)) {
      const operator = tokens[current];
      ++current;
      let property;
      const computed = operator.type !== TOKEN_TYPES.Dot;
      if (computed) {
        property = parseMemberExpressionArgumentsList();
        expect(TOKEN_TYPES.CloseSquareBracket, "Expected closing square bracket");
      } else {
        property = parsePrimaryExpression();
        if (property.type !== "Identifier") {
          throw new SyntaxError(`Expected identifier following dot operator`);
        }
      }
      object = new MemberExpression(object, property, computed);
    }
    return object;
  }
  function parseMultiplicativeExpression() {
    let left = parseTestExpression();
    while (is(TOKEN_TYPES.MultiplicativeBinaryOperator)) {
      const operator = tokens[current];
      ++current;
      const right = parseTestExpression();
      left = new BinaryExpression(operator, left, right);
    }
    return left;
  }
  function parseTestExpression() {
    let operand = parseFilterExpression();
    while (is(TOKEN_TYPES.Is)) {
      ++current;
      const negate = is(TOKEN_TYPES.Not);
      if (negate) {
        ++current;
      }
      let filter = parsePrimaryExpression();
      if (filter instanceof BooleanLiteral) {
        filter = new Identifier(filter.value.toString());
      } else if (filter instanceof NullLiteral) {
        filter = new Identifier("none");
      }
      if (!(filter instanceof Identifier)) {
        throw new SyntaxError(`Expected identifier for the test`);
      }
      operand = new TestExpression(operand, negate, filter);
    }
    return operand;
  }
  function parseFilterExpression() {
    let operand = parseCallMemberExpression();
    while (is(TOKEN_TYPES.Pipe)) {
      ++current;
      let filter = parsePrimaryExpression();
      if (!(filter instanceof Identifier)) {
        throw new SyntaxError(`Expected identifier for the filter`);
      }
      if (is(TOKEN_TYPES.OpenParen)) {
        filter = parseCallExpression(filter);
      }
      operand = new FilterExpression(operand, filter);
    }
    return operand;
  }
  function parsePrimaryExpression() {
    const token = tokens[current];
    switch (token.type) {
      case TOKEN_TYPES.NumericLiteral:
        ++current;
        return new NumericLiteral(Number(token.value));
      case TOKEN_TYPES.StringLiteral:
        ++current;
        return new StringLiteral(token.value);
      case TOKEN_TYPES.BooleanLiteral:
        ++current;
        return new BooleanLiteral(token.value.toLowerCase() === "true");
      case TOKEN_TYPES.NullLiteral:
        ++current;
        return new NullLiteral(null);
      case TOKEN_TYPES.Identifier:
        ++current;
        return new Identifier(token.value);
      case TOKEN_TYPES.OpenParen: {
        ++current;
        const expression = parseExpressionSequence();
        if (tokens[current].type !== TOKEN_TYPES.CloseParen) {
          throw new SyntaxError(`Expected closing parenthesis, got ${tokens[current].type} instead`);
        }
        ++current;
        return expression;
      }
      case TOKEN_TYPES.OpenSquareBracket: {
        ++current;
        const values = [];
        while (!is(TOKEN_TYPES.CloseSquareBracket)) {
          values.push(parseExpression());
          if (is(TOKEN_TYPES.Comma)) {
            ++current;
          }
        }
        ++current;
        return new ArrayLiteral(values);
      }
      case TOKEN_TYPES.OpenCurlyBracket: {
        ++current;
        const values = /* @__PURE__ */ new Map();
        while (!is(TOKEN_TYPES.CloseCurlyBracket)) {
          const key = parseExpression();
          expect(TOKEN_TYPES.Colon, "Expected colon between key and value in object literal");
          const value = parseExpression();
          values.set(key, value);
          if (is(TOKEN_TYPES.Comma)) {
            ++current;
          }
        }
        ++current;
        return new ObjectLiteral(values);
      }
      default:
        throw new SyntaxError(`Unexpected token: ${token.type}`);
    }
  }
  while (current < tokens.length) {
    program.body.push(parseAny());
  }
  return program;
}
function range(start, stop, step = 1) {
  if (stop === void 0) {
    stop = start;
    start = 0;
  }
  const result = [];
  for (let i = start; i < stop; i += step) {
    result.push(i);
  }
  return result;
}
function slice(array, start, stop, step = 1) {
  const direction = Math.sign(step);
  if (direction >= 0) {
    start = (start ?? (start = 0)) < 0 ? Math.max(array.length + start, 0) : Math.min(start, array.length);
    stop = (stop ?? (stop = array.length)) < 0 ? Math.max(array.length + stop, 0) : Math.min(stop, array.length);
  } else {
    start = (start ?? (start = array.length - 1)) < 0 ? Math.max(array.length + start, -1) : Math.min(start, array.length - 1);
    stop = (stop ?? (stop = -1)) < -1 ? Math.max(array.length + stop, -1) : Math.min(stop, array.length - 1);
  }
  const result = [];
  for (let i = start; direction * i < direction * stop; i += step) {
    result.push(array[i]);
  }
  return result;
}
function titleCase(value) {
  return value.replace(/\b\w/g, (c) => c.toUpperCase());
}
var RuntimeValue = class {
  /**
   * Creates a new RuntimeValue.
   */
  constructor(value = void 0) {
    __publicField(this, "type", "RuntimeValue");
    __publicField(this, "value");
    /**
     * A collection of built-in functions for this type.
     */
    __publicField(this, "builtins", /* @__PURE__ */ new Map());
    this.value = value;
  }
  /**
   * Determines truthiness or falsiness of the runtime value.
   * This function should be overridden by subclasses if it has custom truthiness criteria.
   * @returns {BooleanValue} BooleanValue(true) if the value is truthy, BooleanValue(false) otherwise.
   */
  __bool__() {
    return new BooleanValue(!!this.value);
  }
};
var NumericValue = class extends RuntimeValue {
  constructor() {
    super(...arguments);
    __publicField(this, "type", "NumericValue");
  }
};
var StringValue = class extends RuntimeValue {
  constructor() {
    super(...arguments);
    __publicField(this, "type", "StringValue");
    __publicField(this, "builtins", /* @__PURE__ */ new Map([
      [
        "upper",
        new FunctionValue(() => {
          return new StringValue(this.value.toUpperCase());
        })
      ],
      [
        "lower",
        new FunctionValue(() => {
          return new StringValue(this.value.toLowerCase());
        })
      ],
      [
        "strip",
        new FunctionValue(() => {
          return new StringValue(this.value.trim());
        })
      ],
      [
        "title",
        new FunctionValue(() => {
          return new StringValue(titleCase(this.value));
        })
      ],
      ["length", new NumericValue(this.value.length)],
      [
        "rstrip",
        new FunctionValue(() => {
          return new StringValue(this.value.trimEnd());
        })
      ],
      [
        "lstrip",
        new FunctionValue(() => {
          return new StringValue(this.value.trimStart());
        })
      ],
      [
        "split",
        // follows Python's `str.split(sep=None, maxsplit=-1)` function behavior
        // https://docs.python.org/3.13/library/stdtypes.html#str.split
        new FunctionValue((args) => {
          const sep = args[0] ?? new NullValue();
          if (!(sep instanceof StringValue || sep instanceof NullValue)) {
            throw new Error("sep argument must be a string or null");
          }
          const maxsplit = args[1] ?? new NumericValue(-1);
          if (!(maxsplit instanceof NumericValue)) {
            throw new Error("maxsplit argument must be a number");
          }
          let result = [];
          if (sep instanceof NullValue) {
            const text = this.value.trimStart();
            for (const { 0: match, index } of text.matchAll(/\S+/g)) {
              if (maxsplit.value !== -1 && result.length >= maxsplit.value && index !== void 0) {
                result.push(match + text.slice(index + match.length));
                break;
              }
              result.push(match);
            }
          } else {
            if (sep.value === "") {
              throw new Error("empty separator");
            }
            result = this.value.split(sep.value);
            if (maxsplit.value !== -1 && result.length > maxsplit.value) {
              result.push(result.splice(maxsplit.value).join(sep.value));
            }
          }
          return new ArrayValue(result.map((part) => new StringValue(part)));
        })
      ]
    ]));
  }
};
var BooleanValue = class extends RuntimeValue {
  constructor() {
    super(...arguments);
    __publicField(this, "type", "BooleanValue");
  }
};
var ObjectValue = class extends RuntimeValue {
  constructor() {
    super(...arguments);
    __publicField(this, "type", "ObjectValue");
    __publicField(this, "builtins", /* @__PURE__ */ new Map([
      [
        "get",
        new FunctionValue(([key, defaultValue]) => {
          if (!(key instanceof StringValue)) {
            throw new Error(`Object key must be a string: got ${key.type}`);
          }
          return this.value.get(key.value) ?? defaultValue ?? new NullValue();
        })
      ],
      [
        "items",
        new FunctionValue(() => {
          return new ArrayValue(
            Array.from(this.value.entries()).map(([key, value]) => new ArrayValue([new StringValue(key), value]))
          );
        })
      ]
    ]));
  }
  /**
   * NOTE: necessary to override since all JavaScript arrays are considered truthy,
   * while only non-empty Python arrays are consider truthy.
   *
   * e.g.,
   *  - JavaScript:  {} && 5 -> 5
   *  - Python:      {} and 5 -> {}
   */
  __bool__() {
    return new BooleanValue(this.value.size > 0);
  }
};
var KeywordArgumentsValue = class extends ObjectValue {
  constructor() {
    super(...arguments);
    __publicField(this, "type", "KeywordArgumentsValue");
  }
};
var ArrayValue = class extends RuntimeValue {
  constructor() {
    super(...arguments);
    __publicField(this, "type", "ArrayValue");
    __publicField(this, "builtins", /* @__PURE__ */ new Map([["length", new NumericValue(this.value.length)]]));
  }
  /**
   * NOTE: necessary to override since all JavaScript arrays are considered truthy,
   * while only non-empty Python arrays are consider truthy.
   *
   * e.g.,
   *  - JavaScript:  [] && 5 -> 5
   *  - Python:      [] and 5 -> []
   */
  __bool__() {
    return new BooleanValue(this.value.length > 0);
  }
};
var TupleValue = class extends ArrayValue {
  constructor() {
    super(...arguments);
    __publicField(this, "type", "TupleValue");
  }
};
var FunctionValue = class extends RuntimeValue {
  constructor() {
    super(...arguments);
    __publicField(this, "type", "FunctionValue");
  }
};
var NullValue = class extends RuntimeValue {
  constructor() {
    super(...arguments);
    __publicField(this, "type", "NullValue");
  }
};
var UndefinedValue = class extends RuntimeValue {
  constructor() {
    super(...arguments);
    __publicField(this, "type", "UndefinedValue");
  }
};
var Environment = class {
  constructor(parent) {
    /**
     * The variables declared in this environment.
     */
    __publicField(this, "variables", /* @__PURE__ */ new Map([
      [
        "namespace",
        new FunctionValue((args) => {
          if (args.length === 0) {
            return new ObjectValue(/* @__PURE__ */ new Map());
          }
          if (args.length !== 1 || !(args[0] instanceof ObjectValue)) {
            throw new Error("`namespace` expects either zero arguments or a single object argument");
          }
          return args[0];
        })
      ]
    ]));
    /**
     * The tests available in this environment.
     */
    __publicField(this, "tests", /* @__PURE__ */ new Map([
      ["boolean", (operand) => operand.type === "BooleanValue"],
      ["callable", (operand) => operand instanceof FunctionValue],
      [
        "odd",
        (operand) => {
          if (operand.type !== "NumericValue") {
            throw new Error(`Cannot apply test "odd" to type: ${operand.type}`);
          }
          return operand.value % 2 !== 0;
        }
      ],
      [
        "even",
        (operand) => {
          if (operand.type !== "NumericValue") {
            throw new Error(`Cannot apply test "even" to type: ${operand.type}`);
          }
          return operand.value % 2 === 0;
        }
      ],
      ["false", (operand) => operand.type === "BooleanValue" && !operand.value],
      ["true", (operand) => operand.type === "BooleanValue" && operand.value],
      ["none", (operand) => operand.type === "NullValue"],
      ["string", (operand) => operand.type === "StringValue"],
      ["number", (operand) => operand.type === "NumericValue"],
      ["integer", (operand) => operand.type === "NumericValue" && Number.isInteger(operand.value)],
      ["iterable", (operand) => operand.type === "ArrayValue" || operand.type === "StringValue"],
      ["mapping", (operand) => operand.type === "ObjectValue"],
      [
        "lower",
        (operand) => {
          const str = operand.value;
          return operand.type === "StringValue" && str === str.toLowerCase();
        }
      ],
      [
        "upper",
        (operand) => {
          const str = operand.value;
          return operand.type === "StringValue" && str === str.toUpperCase();
        }
      ],
      ["none", (operand) => operand.type === "NullValue"],
      ["defined", (operand) => operand.type !== "UndefinedValue"],
      ["undefined", (operand) => operand.type === "UndefinedValue"],
      ["equalto", (a, b) => a.value === b.value],
      ["eq", (a, b) => a.value === b.value]
    ]));
    this.parent = parent;
  }
  /**
   * Set the value of a variable in the current environment.
   */
  set(name2, value) {
    return this.declareVariable(name2, convertToRuntimeValues(value));
  }
  declareVariable(name2, value) {
    if (this.variables.has(name2)) {
      throw new SyntaxError(`Variable already declared: ${name2}`);
    }
    this.variables.set(name2, value);
    return value;
  }
  // private assignVariable(name: string, value: AnyRuntimeValue): AnyRuntimeValue {
  // 	const env = this.resolve(name);
  // 	env.variables.set(name, value);
  // 	return value;
  // }
  /**
   * Set variable in the current scope.
   * See https://jinja.palletsprojects.com/en/3.0.x/templates/#assignments for more information.
   */
  setVariable(name2, value) {
    this.variables.set(name2, value);
    return value;
  }
  /**
   * Resolve the environment in which the variable is declared.
   * @param {string} name The name of the variable.
   * @returns {Environment} The environment in which the variable is declared.
   */
  resolve(name2) {
    if (this.variables.has(name2)) {
      return this;
    }
    if (this.parent) {
      return this.parent.resolve(name2);
    }
    throw new Error(`Unknown variable: ${name2}`);
  }
  lookupVariable(name2) {
    try {
      return this.resolve(name2).variables.get(name2) ?? new UndefinedValue();
    } catch {
      return new UndefinedValue();
    }
  }
};
var Interpreter = class {
  constructor(env) {
    __publicField(this, "global");
    this.global = env ?? new Environment();
  }
  /**
   * Run the program.
   */
  run(program) {
    return this.evaluate(program, this.global);
  }
  /**
   * Evaluates expressions following the binary operation type.
   */
  evaluateBinaryExpression(node, environment) {
    const left = this.evaluate(node.left, environment);
    switch (node.operator.value) {
      case "and":
        return left.__bool__().value ? this.evaluate(node.right, environment) : left;
      case "or":
        return left.__bool__().value ? left : this.evaluate(node.right, environment);
    }
    const right = this.evaluate(node.right, environment);
    switch (node.operator.value) {
      case "==":
        return new BooleanValue(left.value == right.value);
      case "!=":
        return new BooleanValue(left.value != right.value);
    }
    if (left instanceof UndefinedValue || right instanceof UndefinedValue) {
      throw new Error("Cannot perform operation on undefined values");
    } else if (left instanceof NullValue || right instanceof NullValue) {
      throw new Error("Cannot perform operation on null values");
    } else if (left instanceof NumericValue && right instanceof NumericValue) {
      switch (node.operator.value) {
        case "+":
          return new NumericValue(left.value + right.value);
        case "-":
          return new NumericValue(left.value - right.value);
        case "*":
          return new NumericValue(left.value * right.value);
        case "/":
          return new NumericValue(left.value / right.value);
        case "%":
          return new NumericValue(left.value % right.value);
        case "<":
          return new BooleanValue(left.value < right.value);
        case ">":
          return new BooleanValue(left.value > right.value);
        case ">=":
          return new BooleanValue(left.value >= right.value);
        case "<=":
          return new BooleanValue(left.value <= right.value);
      }
    } else if (left instanceof ArrayValue && right instanceof ArrayValue) {
      switch (node.operator.value) {
        case "+":
          return new ArrayValue(left.value.concat(right.value));
      }
    } else if (right instanceof ArrayValue) {
      const member = right.value.find((x) => x.value === left.value) !== void 0;
      switch (node.operator.value) {
        case "in":
          return new BooleanValue(member);
        case "not in":
          return new BooleanValue(!member);
      }
    }
    if (left instanceof StringValue || right instanceof StringValue) {
      switch (node.operator.value) {
        case "+":
          return new StringValue(left.value.toString() + right.value.toString());
      }
    }
    if (left instanceof StringValue && right instanceof StringValue) {
      switch (node.operator.value) {
        case "in":
          return new BooleanValue(right.value.includes(left.value));
        case "not in":
          return new BooleanValue(!right.value.includes(left.value));
      }
    }
    if (left instanceof StringValue && right instanceof ObjectValue) {
      switch (node.operator.value) {
        case "in":
          return new BooleanValue(right.value.has(left.value));
        case "not in":
          return new BooleanValue(!right.value.has(left.value));
      }
    }
    throw new SyntaxError(`Unknown operator "${node.operator.value}" between ${left.type} and ${right.type}`);
  }
  evaluateArguments(args, environment) {
    const positionalArguments = [];
    const keywordArguments = /* @__PURE__ */ new Map();
    for (const argument of args) {
      if (argument.type === "KeywordArgumentExpression") {
        const kwarg = argument;
        keywordArguments.set(kwarg.key.value, this.evaluate(kwarg.value, environment));
      } else {
        if (keywordArguments.size > 0) {
          throw new Error("Positional arguments must come before keyword arguments");
        }
        positionalArguments.push(this.evaluate(argument, environment));
      }
    }
    return [positionalArguments, keywordArguments];
  }
  /**
   * Evaluates expressions following the filter operation type.
   */
  evaluateFilterExpression(node, environment) {
    const operand = this.evaluate(node.operand, environment);
    if (node.filter.type === "Identifier") {
      const filter = node.filter;
      if (filter.value === "tojson") {
        return new StringValue(toJSON(operand));
      }
      if (operand instanceof ArrayValue) {
        switch (filter.value) {
          case "list":
            return operand;
          case "first":
            return operand.value[0];
          case "last":
            return operand.value[operand.value.length - 1];
          case "length":
            return new NumericValue(operand.value.length);
          case "reverse":
            return new ArrayValue(operand.value.reverse());
          case "sort":
            return new ArrayValue(
              operand.value.sort((a, b) => {
                if (a.type !== b.type) {
                  throw new Error(`Cannot compare different types: ${a.type} and ${b.type}`);
                }
                switch (a.type) {
                  case "NumericValue":
                    return a.value - b.value;
                  case "StringValue":
                    return a.value.localeCompare(b.value);
                  default:
                    throw new Error(`Cannot compare type: ${a.type}`);
                }
              })
            );
          case "join":
            return new StringValue(operand.value.map((x) => x.value).join(""));
          default:
            throw new Error(`Unknown ArrayValue filter: ${filter.value}`);
        }
      } else if (operand instanceof StringValue) {
        switch (filter.value) {
          case "length":
            return new NumericValue(operand.value.length);
          case "upper":
            return new StringValue(operand.value.toUpperCase());
          case "lower":
            return new StringValue(operand.value.toLowerCase());
          case "title":
            return new StringValue(titleCase(operand.value));
          case "capitalize":
            return new StringValue(operand.value.charAt(0).toUpperCase() + operand.value.slice(1));
          case "trim":
            return new StringValue(operand.value.trim());
          case "indent":
            return new StringValue(
              operand.value.split("\n").map(
                (x, i) => (
                  // By default, don't indent the first line or empty lines
                  i === 0 || x.length === 0 ? x : "    " + x
                )
              ).join("\n")
            );
          case "join":
          case "string":
            return operand;
          default:
            throw new Error(`Unknown StringValue filter: ${filter.value}`);
        }
      } else if (operand instanceof NumericValue) {
        switch (filter.value) {
          case "abs":
            return new NumericValue(Math.abs(operand.value));
          default:
            throw new Error(`Unknown NumericValue filter: ${filter.value}`);
        }
      } else if (operand instanceof ObjectValue) {
        switch (filter.value) {
          case "items":
            return new ArrayValue(
              Array.from(operand.value.entries()).map(([key, value]) => new ArrayValue([new StringValue(key), value]))
            );
          case "length":
            return new NumericValue(operand.value.size);
          default:
            throw new Error(`Unknown ObjectValue filter: ${filter.value}`);
        }
      }
      throw new Error(`Cannot apply filter "${filter.value}" to type: ${operand.type}`);
    } else if (node.filter.type === "CallExpression") {
      const filter = node.filter;
      if (filter.callee.type !== "Identifier") {
        throw new Error(`Unknown filter: ${filter.callee.type}`);
      }
      const filterName = filter.callee.value;
      if (filterName === "tojson") {
        const [, kwargs] = this.evaluateArguments(filter.args, environment);
        const indent = kwargs.get("indent") ?? new NullValue();
        if (!(indent instanceof NumericValue || indent instanceof NullValue)) {
          throw new Error("If set, indent must be a number");
        }
        return new StringValue(toJSON(operand, indent.value));
      } else if (filterName === "join") {
        let value;
        if (operand instanceof StringValue) {
          value = Array.from(operand.value);
        } else if (operand instanceof ArrayValue) {
          value = operand.value.map((x) => x.value);
        } else {
          throw new Error(`Cannot apply filter "${filterName}" to type: ${operand.type}`);
        }
        const [args, kwargs] = this.evaluateArguments(filter.args, environment);
        const separator = args.at(0) ?? kwargs.get("separator") ?? new StringValue("");
        if (!(separator instanceof StringValue)) {
          throw new Error("separator must be a string");
        }
        return new StringValue(value.join(separator.value));
      }
      if (operand instanceof ArrayValue) {
        switch (filterName) {
          case "selectattr":
          case "rejectattr": {
            const select = filterName === "selectattr";
            if (operand.value.some((x) => !(x instanceof ObjectValue))) {
              throw new Error(`\`${filterName}\` can only be applied to array of objects`);
            }
            if (filter.args.some((x) => x.type !== "StringLiteral")) {
              throw new Error(`arguments of \`${filterName}\` must be strings`);
            }
            const [attr, testName, value] = filter.args.map((x) => this.evaluate(x, environment));
            let testFunction;
            if (testName) {
              const test = environment.tests.get(testName.value);
              if (!test) {
                throw new Error(`Unknown test: ${testName.value}`);
              }
              testFunction = test;
            } else {
              testFunction = (...x) => x[0].__bool__().value;
            }
            const filtered = operand.value.filter((item) => {
              const a = item.value.get(attr.value);
              const result = a ? testFunction(a, value) : false;
              return select ? result : !result;
            });
            return new ArrayValue(filtered);
          }
          case "map": {
            const [, kwargs] = this.evaluateArguments(filter.args, environment);
            if (kwargs.has("attribute")) {
              const attr = kwargs.get("attribute");
              if (!(attr instanceof StringValue)) {
                throw new Error("attribute must be a string");
              }
              const defaultValue = kwargs.get("default");
              const mapped = operand.value.map((item) => {
                if (!(item instanceof ObjectValue)) {
                  throw new Error("items in map must be an object");
                }
                return item.value.get(attr.value) ?? defaultValue ?? new UndefinedValue();
              });
              return new ArrayValue(mapped);
            } else {
              throw new Error("`map` expressions without `attribute` set are not currently supported.");
            }
          }
        }
        throw new Error(`Unknown ArrayValue filter: ${filterName}`);
      } else if (operand instanceof StringValue) {
        switch (filterName) {
          case "indent": {
            const [args, kwargs] = this.evaluateArguments(filter.args, environment);
            const width = args.at(0) ?? kwargs.get("width") ?? new NumericValue(4);
            if (!(width instanceof NumericValue)) {
              throw new Error("width must be a number");
            }
            const first = args.at(1) ?? kwargs.get("first") ?? new BooleanValue(false);
            const blank = args.at(2) ?? kwargs.get("blank") ?? new BooleanValue(false);
            const lines = operand.value.split("\n");
            const indent = " ".repeat(width.value);
            const indented = lines.map(
              (x, i) => !first.value && i === 0 || !blank.value && x.length === 0 ? x : indent + x
            );
            return new StringValue(indented.join("\n"));
          }
        }
        throw new Error(`Unknown StringValue filter: ${filterName}`);
      } else {
        throw new Error(`Cannot apply filter "${filterName}" to type: ${operand.type}`);
      }
    }
    throw new Error(`Unknown filter: ${node.filter.type}`);
  }
  /**
   * Evaluates expressions following the test operation type.
   */
  evaluateTestExpression(node, environment) {
    const operand = this.evaluate(node.operand, environment);
    const test = environment.tests.get(node.test.value);
    if (!test) {
      throw new Error(`Unknown test: ${node.test.value}`);
    }
    const result = test(operand);
    return new BooleanValue(node.negate ? !result : result);
  }
  /**
   * Evaluates expressions following the unary operation type.
   */
  evaluateUnaryExpression(node, environment) {
    const argument = this.evaluate(node.argument, environment);
    switch (node.operator.value) {
      case "not":
        return new BooleanValue(!argument.value);
      default:
        throw new SyntaxError(`Unknown operator: ${node.operator.value}`);
    }
  }
  evalProgram(program, environment) {
    return this.evaluateBlock(program.body, environment);
  }
  evaluateBlock(statements, environment) {
    let result = "";
    for (const statement of statements) {
      const lastEvaluated = this.evaluate(statement, environment);
      if (lastEvaluated.type !== "NullValue" && lastEvaluated.type !== "UndefinedValue") {
        result += lastEvaluated.value;
      }
    }
    return new StringValue(result);
  }
  evaluateIdentifier(node, environment) {
    return environment.lookupVariable(node.value);
  }
  evaluateCallExpression(expr, environment) {
    const [args, kwargs] = this.evaluateArguments(expr.args, environment);
    if (kwargs.size > 0) {
      args.push(new KeywordArgumentsValue(kwargs));
    }
    const fn = this.evaluate(expr.callee, environment);
    if (fn.type !== "FunctionValue") {
      throw new Error(`Cannot call something that is not a function: got ${fn.type}`);
    }
    return fn.value(args, environment);
  }
  evaluateSliceExpression(object, expr, environment) {
    if (!(object instanceof ArrayValue || object instanceof StringValue)) {
      throw new Error("Slice object must be an array or string");
    }
    const start = this.evaluate(expr.start, environment);
    const stop = this.evaluate(expr.stop, environment);
    const step = this.evaluate(expr.step, environment);
    if (!(start instanceof NumericValue || start instanceof UndefinedValue)) {
      throw new Error("Slice start must be numeric or undefined");
    }
    if (!(stop instanceof NumericValue || stop instanceof UndefinedValue)) {
      throw new Error("Slice stop must be numeric or undefined");
    }
    if (!(step instanceof NumericValue || step instanceof UndefinedValue)) {
      throw new Error("Slice step must be numeric or undefined");
    }
    if (object instanceof ArrayValue) {
      return new ArrayValue(slice(object.value, start.value, stop.value, step.value));
    } else {
      return new StringValue(slice(Array.from(object.value), start.value, stop.value, step.value).join(""));
    }
  }
  evaluateMemberExpression(expr, environment) {
    const object = this.evaluate(expr.object, environment);
    let property;
    if (expr.computed) {
      if (expr.property.type === "SliceExpression") {
        return this.evaluateSliceExpression(object, expr.property, environment);
      } else {
        property = this.evaluate(expr.property, environment);
      }
    } else {
      property = new StringValue(expr.property.value);
    }
    let value;
    if (object instanceof ObjectValue) {
      if (!(property instanceof StringValue)) {
        throw new Error(`Cannot access property with non-string: got ${property.type}`);
      }
      value = object.value.get(property.value) ?? object.builtins.get(property.value);
    } else if (object instanceof ArrayValue || object instanceof StringValue) {
      if (property instanceof NumericValue) {
        value = object.value.at(property.value);
        if (object instanceof StringValue) {
          value = new StringValue(object.value.at(property.value));
        }
      } else if (property instanceof StringValue) {
        value = object.builtins.get(property.value);
      } else {
        throw new Error(`Cannot access property with non-string/non-number: got ${property.type}`);
      }
    } else {
      if (!(property instanceof StringValue)) {
        throw new Error(`Cannot access property with non-string: got ${property.type}`);
      }
      value = object.builtins.get(property.value);
    }
    return value instanceof RuntimeValue ? value : new UndefinedValue();
  }
  evaluateSet(node, environment) {
    const rhs = this.evaluate(node.value, environment);
    if (node.assignee.type === "Identifier") {
      const variableName = node.assignee.value;
      environment.setVariable(variableName, rhs);
    } else if (node.assignee.type === "MemberExpression") {
      const member = node.assignee;
      const object = this.evaluate(member.object, environment);
      if (!(object instanceof ObjectValue)) {
        throw new Error("Cannot assign to member of non-object");
      }
      if (member.property.type !== "Identifier") {
        throw new Error("Cannot assign to member with non-identifier property");
      }
      object.value.set(member.property.value, rhs);
    } else {
      throw new Error(`Invalid LHS inside assignment expression: ${JSON.stringify(node.assignee)}`);
    }
    return new NullValue();
  }
  evaluateIf(node, environment) {
    const test = this.evaluate(node.test, environment);
    return this.evaluateBlock(test.__bool__().value ? node.body : node.alternate, environment);
  }
  evaluateFor(node, environment) {
    const scope = new Environment(environment);
    let test, iterable;
    if (node.iterable.type === "SelectExpression") {
      const select = node.iterable;
      iterable = this.evaluate(select.iterable, scope);
      test = select.test;
    } else {
      iterable = this.evaluate(node.iterable, scope);
    }
    if (!(iterable instanceof ArrayValue)) {
      throw new Error(`Expected iterable type in for loop: got ${iterable.type}`);
    }
    const items = [];
    const scopeUpdateFunctions = [];
    for (let i = 0; i < iterable.value.length; ++i) {
      const loopScope = new Environment(scope);
      const current = iterable.value[i];
      let scopeUpdateFunction;
      if (node.loopvar.type === "Identifier") {
        scopeUpdateFunction = (scope2) => scope2.setVariable(node.loopvar.value, current);
      } else if (node.loopvar.type === "TupleLiteral") {
        const loopvar = node.loopvar;
        if (current.type !== "ArrayValue") {
          throw new Error(`Cannot unpack non-iterable type: ${current.type}`);
        }
        const c = current;
        if (loopvar.value.length !== c.value.length) {
          throw new Error(`Too ${loopvar.value.length > c.value.length ? "few" : "many"} items to unpack`);
        }
        scopeUpdateFunction = (scope2) => {
          for (let j = 0; j < loopvar.value.length; ++j) {
            if (loopvar.value[j].type !== "Identifier") {
              throw new Error(`Cannot unpack non-identifier type: ${loopvar.value[j].type}`);
            }
            scope2.setVariable(loopvar.value[j].value, c.value[j]);
          }
        };
      } else {
        throw new Error(`Invalid loop variable(s): ${node.loopvar.type}`);
      }
      if (test) {
        scopeUpdateFunction(loopScope);
        const testValue = this.evaluate(test, loopScope);
        if (!testValue.__bool__().value) {
          continue;
        }
      }
      items.push(current);
      scopeUpdateFunctions.push(scopeUpdateFunction);
    }
    let result = "";
    let noIteration = true;
    for (let i = 0; i < items.length; ++i) {
      const loop = /* @__PURE__ */ new Map([
        ["index", new NumericValue(i + 1)],
        ["index0", new NumericValue(i)],
        ["revindex", new NumericValue(items.length - i)],
        ["revindex0", new NumericValue(items.length - i - 1)],
        ["first", new BooleanValue(i === 0)],
        ["last", new BooleanValue(i === items.length - 1)],
        ["length", new NumericValue(items.length)],
        ["previtem", i > 0 ? items[i - 1] : new UndefinedValue()],
        ["nextitem", i < items.length - 1 ? items[i + 1] : new UndefinedValue()]
      ]);
      scope.setVariable("loop", new ObjectValue(loop));
      scopeUpdateFunctions[i](scope);
      const evaluated = this.evaluateBlock(node.body, scope);
      result += evaluated.value;
      noIteration = false;
    }
    if (noIteration) {
      const defaultEvaluated = this.evaluateBlock(node.defaultBlock, scope);
      result += defaultEvaluated.value;
    }
    return new StringValue(result);
  }
  /**
   * See https://jinja.palletsprojects.com/en/3.1.x/templates/#macros for more information.
   */
  evaluateMacro(node, environment) {
    environment.setVariable(
      node.name.value,
      new FunctionValue((args, scope) => {
        var _a;
        const macroScope = new Environment(scope);
        args = args.slice();
        let kwargs;
        if (((_a = args.at(-1)) == null ? void 0 : _a.type) === "KeywordArgumentsValue") {
          kwargs = args.pop();
        }
        for (let i = 0; i < node.args.length; ++i) {
          const nodeArg = node.args[i];
          const passedArg = args[i];
          if (nodeArg.type === "Identifier") {
            const identifier = nodeArg;
            if (!passedArg) {
              throw new Error(`Missing positional argument: ${identifier.value}`);
            }
            macroScope.setVariable(identifier.value, passedArg);
          } else if (nodeArg.type === "KeywordArgumentExpression") {
            const kwarg = nodeArg;
            const value = passedArg ?? // Try positional arguments first
            (kwargs == null ? void 0 : kwargs.value.get(kwarg.key.value)) ?? // Look in user-passed kwargs
            this.evaluate(kwarg.value, macroScope);
            macroScope.setVariable(kwarg.key.value, value);
          } else {
            throw new Error(`Unknown argument type: ${nodeArg.type}`);
          }
        }
        return this.evaluateBlock(node.body, macroScope);
      })
    );
    return new NullValue();
  }
  evaluate(statement, environment) {
    if (statement === void 0)
      return new UndefinedValue();
    switch (statement.type) {
      case "Program":
        return this.evalProgram(statement, environment);
      case "Set":
        return this.evaluateSet(statement, environment);
      case "If":
        return this.evaluateIf(statement, environment);
      case "For":
        return this.evaluateFor(statement, environment);
      case "Macro":
        return this.evaluateMacro(statement, environment);
      case "NumericLiteral":
        return new NumericValue(Number(statement.value));
      case "StringLiteral":
        return new StringValue(statement.value);
      case "BooleanLiteral":
        return new BooleanValue(statement.value);
      case "NullLiteral":
        return new NullValue(statement.value);
      case "ArrayLiteral":
        return new ArrayValue(statement.value.map((x) => this.evaluate(x, environment)));
      case "TupleLiteral":
        return new TupleValue(statement.value.map((x) => this.evaluate(x, environment)));
      case "ObjectLiteral": {
        const mapping = /* @__PURE__ */ new Map();
        for (const [key, value] of statement.value) {
          const evaluatedKey = this.evaluate(key, environment);
          if (!(evaluatedKey instanceof StringValue)) {
            throw new Error(`Object keys must be strings: got ${evaluatedKey.type}`);
          }
          mapping.set(evaluatedKey.value, this.evaluate(value, environment));
        }
        return new ObjectValue(mapping);
      }
      case "Identifier":
        return this.evaluateIdentifier(statement, environment);
      case "CallExpression":
        return this.evaluateCallExpression(statement, environment);
      case "MemberExpression":
        return this.evaluateMemberExpression(statement, environment);
      case "UnaryExpression":
        return this.evaluateUnaryExpression(statement, environment);
      case "BinaryExpression":
        return this.evaluateBinaryExpression(statement, environment);
      case "FilterExpression":
        return this.evaluateFilterExpression(statement, environment);
      case "TestExpression":
        return this.evaluateTestExpression(statement, environment);
      default:
        throw new SyntaxError(`Unknown node type: ${statement.type}`);
    }
  }
};
function convertToRuntimeValues(input) {
  switch (typeof input) {
    case "number":
      return new NumericValue(input);
    case "string":
      return new StringValue(input);
    case "boolean":
      return new BooleanValue(input);
    case "undefined":
      return new UndefinedValue();
    case "object":
      if (input === null) {
        return new NullValue();
      } else if (Array.isArray(input)) {
        return new ArrayValue(input.map(convertToRuntimeValues));
      } else {
        return new ObjectValue(
          new Map(Object.entries(input).map(([key, value]) => [key, convertToRuntimeValues(value)]))
        );
      }
    case "function":
      return new FunctionValue((args, _scope) => {
        const result = input(...args.map((x) => x.value)) ?? null;
        return convertToRuntimeValues(result);
      });
    default:
      throw new Error(`Cannot convert to runtime value: ${input}`);
  }
}
function toJSON(input, indent, depth) {
  const currentDepth = depth ?? 0;
  switch (input.type) {
    case "NullValue":
    case "UndefinedValue":
      return "null";
    case "NumericValue":
    case "StringValue":
    case "BooleanValue":
      return JSON.stringify(input.value);
    case "ArrayValue":
    case "ObjectValue": {
      const indentValue = indent ? " ".repeat(indent) : "";
      const basePadding = "\n" + indentValue.repeat(currentDepth);
      const childrenPadding = basePadding + indentValue;
      if (input.type === "ArrayValue") {
        const core = input.value.map((x) => toJSON(x, indent, currentDepth + 1));
        return indent ? `[${childrenPadding}${core.join(`,${childrenPadding}`)}${basePadding}]` : `[${core.join(", ")}]`;
      } else {
        const core = Array.from(input.value.entries()).map(([key, value]) => {
          const v = `"${key}": ${toJSON(value, indent, currentDepth + 1)}`;
          return indent ? `${childrenPadding}${v}` : v;
        });
        return indent ? `{${core.join(",")}${basePadding}}` : `{${core.join(", ")}}`;
      }
    }
    default:
      throw new Error(`Cannot convert to JSON: ${input.type}`);
  }
}
var Template = class {
  /**
   * @param {string} template The template string
   */
  constructor(template) {
    __publicField(this, "parsed");
    const tokens = tokenize(template, {
      lstrip_blocks: true,
      trim_blocks: true
    });
    this.parsed = parse(tokens);
  }
  render(items) {
    const env = new Environment();
    env.set("false", false);
    env.set("true", true);
    env.set("raise_exception", (args) => {
      throw new Error(args);
    });
    env.set("range", range);
    if (items) {
      for (const [key, value] of Object.entries(items)) {
        env.set(key, value);
      }
    }
    const interpreter = new Interpreter(env);
    const result = interpreter.run(this.parsed);
    return result.value;
  }
};

// node_modules/@huggingface/inference/dist/index.js
var __defProp = Object.defineProperty;
var __export = (target, all) => {
  for (var name2 in all)
    __defProp(target, name2, { get: all[name2], enumerable: true });
};
var tasks_exports = {};
__export(tasks_exports, {
  audioClassification: () => audioClassification,
  audioToAudio: () => audioToAudio,
  automaticSpeechRecognition: () => automaticSpeechRecognition,
  chatCompletion: () => chatCompletion,
  chatCompletionStream: () => chatCompletionStream,
  documentQuestionAnswering: () => documentQuestionAnswering,
  featureExtraction: () => featureExtraction,
  fillMask: () => fillMask,
  imageClassification: () => imageClassification,
  imageSegmentation: () => imageSegmentation,
  imageToImage: () => imageToImage,
  imageToText: () => imageToText,
  objectDetection: () => objectDetection,
  questionAnswering: () => questionAnswering,
  request: () => request,
  sentenceSimilarity: () => sentenceSimilarity,
  streamingRequest: () => streamingRequest,
  summarization: () => summarization,
  tableQuestionAnswering: () => tableQuestionAnswering,
  tabularClassification: () => tabularClassification,
  tabularRegression: () => tabularRegression,
  textClassification: () => textClassification,
  textGeneration: () => textGeneration,
  textGenerationStream: () => textGenerationStream,
  textToImage: () => textToImage,
  textToSpeech: () => textToSpeech,
  textToVideo: () => textToVideo,
  tokenClassification: () => tokenClassification,
  translation: () => translation,
  visualQuestionAnswering: () => visualQuestionAnswering,
  zeroShotClassification: () => zeroShotClassification,
  zeroShotImageClassification: () => zeroShotImageClassification
});
var HF_HUB_URL = "https://huggingface.co";
var HF_ROUTER_URL = "https://router.huggingface.co";
var BLACK_FOREST_LABS_AI_API_BASE_URL = "https://api.us1.bfl.ai";
var makeBaseUrl = () => {
  return BLACK_FOREST_LABS_AI_API_BASE_URL;
};
var makeBody = (params) => {
  return params.args;
};
var makeHeaders = (params) => {
  if (params.authMethod === "provider-key") {
    return { "X-Key": `${params.accessToken}` };
  } else {
    return { Authorization: `Bearer ${params.accessToken}` };
  }
};
var makeUrl = (params) => {
  return `${params.baseUrl}/v1/${params.model}`;
};
var BLACK_FOREST_LABS_CONFIG = {
  makeBaseUrl,
  makeBody,
  makeHeaders,
  makeUrl
};
var CEREBRAS_API_BASE_URL = "https://api.cerebras.ai";
var makeBaseUrl2 = () => {
  return CEREBRAS_API_BASE_URL;
};
var makeBody2 = (params) => {
  return {
    ...params.args,
    model: params.model
  };
};
var makeHeaders2 = (params) => {
  return { Authorization: `Bearer ${params.accessToken}` };
};
var makeUrl2 = (params) => {
  return `${params.baseUrl}/v1/chat/completions`;
};
var CEREBRAS_CONFIG = {
  makeBaseUrl: makeBaseUrl2,
  makeBody: makeBody2,
  makeHeaders: makeHeaders2,
  makeUrl: makeUrl2
};
var COHERE_API_BASE_URL = "https://api.cohere.com";
var makeBaseUrl3 = () => {
  return COHERE_API_BASE_URL;
};
var makeBody3 = (params) => {
  return {
    ...params.args,
    model: params.model
  };
};
var makeHeaders3 = (params) => {
  return { Authorization: `Bearer ${params.accessToken}` };
};
var makeUrl3 = (params) => {
  return `${params.baseUrl}/compatibility/v1/chat/completions`;
};
var COHERE_CONFIG = {
  makeBaseUrl: makeBaseUrl3,
  makeBody: makeBody3,
  makeHeaders: makeHeaders3,
  makeUrl: makeUrl3
};
var InferenceOutputError = class extends TypeError {
  constructor(message) {
    super(
      `Invalid inference output: ${message}. Use the 'request' method with the same parameters to do a custom call with no type checking.`
    );
    this.name = "InferenceOutputError";
  }
};
function isUrl(modelOrUrl) {
  return /^http(s?):/.test(modelOrUrl) || modelOrUrl.startsWith("/");
}
function delay(ms) {
  return new Promise((resolve) => {
    setTimeout(() => resolve(), ms);
  });
}
var FAL_AI_API_BASE_URL = "https://fal.run";
var FAL_AI_API_BASE_URL_QUEUE = "https://queue.fal.run";
var makeBaseUrl4 = (task) => {
  return task === "text-to-video" ? FAL_AI_API_BASE_URL_QUEUE : FAL_AI_API_BASE_URL;
};
var makeBody4 = (params) => {
  return params.args;
};
var makeHeaders4 = (params) => {
  return {
    Authorization: params.authMethod === "provider-key" ? `Key ${params.accessToken}` : `Bearer ${params.accessToken}`
  };
};
var makeUrl4 = (params) => {
  const baseUrl = `${params.baseUrl}/${params.model}`;
  if (params.authMethod !== "provider-key" && params.task === "text-to-video") {
    return `${baseUrl}?_subdomain=queue`;
  }
  return baseUrl;
};
var FAL_AI_CONFIG = {
  makeBaseUrl: makeBaseUrl4,
  makeBody: makeBody4,
  makeHeaders: makeHeaders4,
  makeUrl: makeUrl4
};
async function pollFalResponse(res, url, headers) {
  const requestId = res.request_id;
  if (!requestId) {
    throw new InferenceOutputError("No request ID found in the response");
  }
  let status = res.status;
  const parsedUrl = new URL(url);
  const baseUrl = `${parsedUrl.protocol}//${parsedUrl.host}${parsedUrl.host === "router.huggingface.co" ? "/fal-ai" : ""}`;
  const modelId = new URL(res.response_url).pathname;
  const queryParams = parsedUrl.search;
  const statusUrl = `${baseUrl}${modelId}/status${queryParams}`;
  const resultUrl = `${baseUrl}${modelId}${queryParams}`;
  while (status !== "COMPLETED") {
    await delay(500);
    const statusResponse = await fetch(statusUrl, { headers });
    if (!statusResponse.ok) {
      throw new InferenceOutputError("Failed to fetch response status from fal-ai API");
    }
    try {
      status = (await statusResponse.json()).status;
    } catch (error) {
      throw new InferenceOutputError("Failed to parse status response from fal-ai API");
    }
  }
  const resultResponse = await fetch(resultUrl, { headers });
  let result;
  try {
    result = await resultResponse.json();
  } catch (error) {
    throw new InferenceOutputError("Failed to parse result response from fal-ai API");
  }
  if (typeof result === "object" && !!result && "video" in result && typeof result.video === "object" && !!result.video && "url" in result.video && typeof result.video.url === "string" && isUrl(result.video.url)) {
    const urlResponse = await fetch(result.video.url);
    return await urlResponse.blob();
  } else {
    throw new InferenceOutputError(
      "Expected { video: { url: string } } result format, got instead: " + JSON.stringify(result)
    );
  }
}
var FIREWORKS_AI_API_BASE_URL = "https://api.fireworks.ai";
var makeBaseUrl5 = () => {
  return FIREWORKS_AI_API_BASE_URL;
};
var makeBody5 = (params) => {
  return {
    ...params.args,
    ...params.chatCompletion ? { model: params.model } : void 0
  };
};
var makeHeaders5 = (params) => {
  return { Authorization: `Bearer ${params.accessToken}` };
};
var makeUrl5 = (params) => {
  if (params.chatCompletion) {
    return `${params.baseUrl}/inference/v1/chat/completions`;
  }
  return `${params.baseUrl}/inference`;
};
var FIREWORKS_AI_CONFIG = {
  makeBaseUrl: makeBaseUrl5,
  makeBody: makeBody5,
  makeHeaders: makeHeaders5,
  makeUrl: makeUrl5
};
var makeBaseUrl6 = () => {
  return `${HF_ROUTER_URL}/hf-inference`;
};
var makeBody6 = (params) => {
  return {
    ...params.args,
    ...params.chatCompletion ? { model: params.model } : void 0
  };
};
var makeHeaders6 = (params) => {
  return { Authorization: `Bearer ${params.accessToken}` };
};
var makeUrl6 = (params) => {
  if (params.task && ["feature-extraction", "sentence-similarity"].includes(params.task)) {
    return `${params.baseUrl}/pipeline/${params.task}/${params.model}`;
  }
  if (params.chatCompletion) {
    return `${params.baseUrl}/models/${params.model}/v1/chat/completions`;
  }
  return `${params.baseUrl}/models/${params.model}`;
};
var HF_INFERENCE_CONFIG = {
  makeBaseUrl: makeBaseUrl6,
  makeBody: makeBody6,
  makeHeaders: makeHeaders6,
  makeUrl: makeUrl6
};
var HYPERBOLIC_API_BASE_URL = "https://api.hyperbolic.xyz";
var makeBaseUrl7 = () => {
  return HYPERBOLIC_API_BASE_URL;
};
var makeBody7 = (params) => {
  return {
    ...params.args,
    ...params.task === "text-to-image" ? { model_name: params.model } : { model: params.model }
  };
};
var makeHeaders7 = (params) => {
  return { Authorization: `Bearer ${params.accessToken}` };
};
var makeUrl7 = (params) => {
  if (params.task === "text-to-image") {
    return `${params.baseUrl}/v1/images/generations`;
  }
  return `${params.baseUrl}/v1/chat/completions`;
};
var HYPERBOLIC_CONFIG = {
  makeBaseUrl: makeBaseUrl7,
  makeBody: makeBody7,
  makeHeaders: makeHeaders7,
  makeUrl: makeUrl7
};
var NEBIUS_API_BASE_URL = "https://api.studio.nebius.ai";
var makeBaseUrl8 = () => {
  return NEBIUS_API_BASE_URL;
};
var makeBody8 = (params) => {
  return {
    ...params.args,
    model: params.model
  };
};
var makeHeaders8 = (params) => {
  return { Authorization: `Bearer ${params.accessToken}` };
};
var makeUrl8 = (params) => {
  if (params.task === "text-to-image") {
    return `${params.baseUrl}/v1/images/generations`;
  }
  if (params.chatCompletion) {
    return `${params.baseUrl}/v1/chat/completions`;
  }
  if (params.task === "text-generation") {
    return `${params.baseUrl}/v1/completions`;
  }
  return params.baseUrl;
};
var NEBIUS_CONFIG = {
  makeBaseUrl: makeBaseUrl8,
  makeBody: makeBody8,
  makeHeaders: makeHeaders8,
  makeUrl: makeUrl8
};
var NOVITA_API_BASE_URL = "https://api.novita.ai";
var makeBaseUrl9 = () => {
  return NOVITA_API_BASE_URL;
};
var makeBody9 = (params) => {
  return {
    ...params.args,
    ...params.chatCompletion ? { model: params.model } : void 0
  };
};
var makeHeaders9 = (params) => {
  return { Authorization: `Bearer ${params.accessToken}` };
};
var makeUrl9 = (params) => {
  if (params.chatCompletion) {
    return `${params.baseUrl}/v3/openai/chat/completions`;
  } else if (params.task === "text-generation") {
    return `${params.baseUrl}/v3/openai/completions`;
  } else if (params.task === "text-to-video") {
    return `${params.baseUrl}/v3/hf/${params.model}`;
  }
  return params.baseUrl;
};
var NOVITA_CONFIG = {
  makeBaseUrl: makeBaseUrl9,
  makeBody: makeBody9,
  makeHeaders: makeHeaders9,
  makeUrl: makeUrl9
};
var REPLICATE_API_BASE_URL = "https://api.replicate.com";
var makeBaseUrl10 = () => {
  return REPLICATE_API_BASE_URL;
};
var makeBody10 = (params) => {
  return {
    input: params.args,
    version: params.model.includes(":") ? params.model.split(":")[1] : void 0
  };
};
var makeHeaders10 = (params) => {
  return { Authorization: `Bearer ${params.accessToken}`, Prefer: "wait" };
};
var makeUrl10 = (params) => {
  if (params.model.includes(":")) {
    return `${params.baseUrl}/v1/predictions`;
  }
  return `${params.baseUrl}/v1/models/${params.model}/predictions`;
};
var REPLICATE_CONFIG = {
  makeBaseUrl: makeBaseUrl10,
  makeBody: makeBody10,
  makeHeaders: makeHeaders10,
  makeUrl: makeUrl10
};
var SAMBANOVA_API_BASE_URL = "https://api.sambanova.ai";
var makeBaseUrl11 = () => {
  return SAMBANOVA_API_BASE_URL;
};
var makeBody11 = (params) => {
  return {
    ...params.args,
    ...params.chatCompletion ? { model: params.model } : void 0
  };
};
var makeHeaders11 = (params) => {
  return { Authorization: `Bearer ${params.accessToken}` };
};
var makeUrl11 = (params) => {
  if (params.chatCompletion) {
    return `${params.baseUrl}/v1/chat/completions`;
  }
  return params.baseUrl;
};
var SAMBANOVA_CONFIG = {
  makeBaseUrl: makeBaseUrl11,
  makeBody: makeBody11,
  makeHeaders: makeHeaders11,
  makeUrl: makeUrl11
};
var TOGETHER_API_BASE_URL = "https://api.together.xyz";
var makeBaseUrl12 = () => {
  return TOGETHER_API_BASE_URL;
};
var makeBody12 = (params) => {
  return {
    ...params.args,
    model: params.model
  };
};
var makeHeaders12 = (params) => {
  return { Authorization: `Bearer ${params.accessToken}` };
};
var makeUrl12 = (params) => {
  if (params.task === "text-to-image") {
    return `${params.baseUrl}/v1/images/generations`;
  }
  if (params.chatCompletion) {
    return `${params.baseUrl}/v1/chat/completions`;
  }
  if (params.task === "text-generation") {
    return `${params.baseUrl}/v1/completions`;
  }
  return params.baseUrl;
};
var TOGETHER_CONFIG = {
  makeBaseUrl: makeBaseUrl12,
  makeBody: makeBody12,
  makeHeaders: makeHeaders12,
  makeUrl: makeUrl12
};
var OPENAI_API_BASE_URL = "https://api.openai.com";
var makeBaseUrl13 = () => {
  return OPENAI_API_BASE_URL;
};
var makeBody13 = (params) => {
  if (!params.chatCompletion) {
    throw new Error("OpenAI only supports chat completions.");
  }
  return {
    ...params.args,
    model: params.model
  };
};
var makeHeaders13 = (params) => {
  return { Authorization: `Bearer ${params.accessToken}` };
};
var makeUrl13 = (params) => {
  if (!params.chatCompletion) {
    throw new Error("OpenAI only supports chat completions.");
  }
  return `${params.baseUrl}/v1/chat/completions`;
};
var OPENAI_CONFIG = {
  makeBaseUrl: makeBaseUrl13,
  makeBody: makeBody13,
  makeHeaders: makeHeaders13,
  makeUrl: makeUrl13,
  clientSideRoutingOnly: true
};
var name = "@huggingface/inference";
var version = "3.6.2";
var HARDCODED_MODEL_ID_MAPPING = {
  /**
   * "HF model ID" => "Model ID on Inference Provider's side"
   *
   * Example:
   * "Qwen/Qwen2.5-Coder-32B-Instruct": "Qwen2.5-Coder-32B-Instruct",
   */
  "black-forest-labs": {},
  cerebras: {},
  cohere: {},
  "fal-ai": {},
  "fireworks-ai": {},
  "hf-inference": {},
  hyperbolic: {},
  nebius: {},
  novita: {},
  openai: {},
  replicate: {},
  sambanova: {},
  together: {}
};
var inferenceProviderMappingCache = /* @__PURE__ */ new Map();
async function getProviderModelId(params, args, options = {}) {
  var _a, _b;
  if (params.provider === "hf-inference") {
    return params.model;
  }
  if (!options.task) {
    throw new Error("task must be specified when using a third-party provider");
  }
  const task = options.task === "text-generation" && options.chatCompletion ? "conversational" : options.task;
  if ((_a = HARDCODED_MODEL_ID_MAPPING[params.provider]) == null ? void 0 : _a[params.model]) {
    return HARDCODED_MODEL_ID_MAPPING[params.provider][params.model];
  }
  let inferenceProviderMapping;
  if (inferenceProviderMappingCache.has(params.model)) {
    inferenceProviderMapping = inferenceProviderMappingCache.get(params.model);
  } else {
    inferenceProviderMapping = await ((options == null ? void 0 : options.fetch) ?? fetch)(
      `${HF_HUB_URL}/api/models/${params.model}?expand[]=inferenceProviderMapping`,
      {
        headers: ((_b = args.accessToken) == null ? void 0 : _b.startsWith("hf_")) ? { Authorization: `Bearer ${args.accessToken}` } : {}
      }
    ).then((resp) => resp.json()).then((json) => json.inferenceProviderMapping).catch(() => null);
  }
  if (!inferenceProviderMapping) {
    throw new Error(`We have not been able to find inference provider information for model ${params.model}.`);
  }
  const providerMapping = inferenceProviderMapping[params.provider];
  if (providerMapping) {
    if (providerMapping.task !== task) {
      throw new Error(
        `Model ${params.model} is not supported for task ${task} and provider ${params.provider}. Supported task: ${providerMapping.task}.`
      );
    }
    if (providerMapping.status === "staging") {
      console.warn(
        `Model ${params.model} is in staging mode for provider ${params.provider}. Meant for test purposes only.`
      );
    }
    return providerMapping.providerId;
  }
  throw new Error(`Model ${params.model} is not supported provider ${params.provider}.`);
}
var HF_HUB_INFERENCE_PROXY_TEMPLATE = `${HF_ROUTER_URL}/{{PROVIDER}}`;
var tasks = null;
var providerConfigs = {
  "black-forest-labs": BLACK_FOREST_LABS_CONFIG,
  cerebras: CEREBRAS_CONFIG,
  cohere: COHERE_CONFIG,
  "fal-ai": FAL_AI_CONFIG,
  "fireworks-ai": FIREWORKS_AI_CONFIG,
  "hf-inference": HF_INFERENCE_CONFIG,
  hyperbolic: HYPERBOLIC_CONFIG,
  openai: OPENAI_CONFIG,
  nebius: NEBIUS_CONFIG,
  novita: NOVITA_CONFIG,
  replicate: REPLICATE_CONFIG,
  sambanova: SAMBANOVA_CONFIG,
  together: TOGETHER_CONFIG
};
async function makeRequestOptions(args, options) {
  const { provider: maybeProvider, model: maybeModel } = args;
  const provider = maybeProvider ?? "hf-inference";
  const providerConfig = providerConfigs[provider];
  const { task, chatCompletion: chatCompletion2 } = options ?? {};
  if (args.endpointUrl && provider !== "hf-inference") {
    throw new Error(`Cannot use endpointUrl with a third-party provider.`);
  }
  if (maybeModel && isUrl(maybeModel)) {
    throw new Error(`Model URLs are no longer supported. Use endpointUrl instead.`);
  }
  if (!maybeModel && !task) {
    throw new Error("No model provided, and no task has been specified.");
  }
  if (!providerConfig) {
    throw new Error(`No provider config found for provider ${provider}`);
  }
  if (providerConfig.clientSideRoutingOnly && !maybeModel) {
    throw new Error(`Provider ${provider} requires a model ID to be passed directly.`);
  }
  const hfModel = maybeModel ?? await loadDefaultModel(task);
  const resolvedModel = providerConfig.clientSideRoutingOnly ? (
    // eslint-disable-next-line @typescript-eslint/no-non-null-assertion
    removeProviderPrefix(maybeModel, provider)
  ) : await getProviderModelId({ model: hfModel, provider }, args, {
    task,
    chatCompletion: chatCompletion2,
    fetch: options == null ? void 0 : options.fetch
  });
  return makeRequestOptionsFromResolvedModel(resolvedModel, args, options);
}
function makeRequestOptionsFromResolvedModel(resolvedModel, args, options) {
  const { accessToken, endpointUrl, provider: maybeProvider, model, ...remainingArgs } = args;
  const provider = maybeProvider ?? "hf-inference";
  const providerConfig = providerConfigs[provider];
  const { includeCredentials, task, chatCompletion: chatCompletion2, signal } = options ?? {};
  const authMethod = (() => {
    if (providerConfig.clientSideRoutingOnly) {
      if (accessToken && accessToken.startsWith("hf_")) {
        throw new Error(`Provider ${provider} is closed-source and does not support HF tokens.`);
      }
      return "provider-key";
    }
    if (accessToken) {
      return accessToken.startsWith("hf_") ? "hf-token" : "provider-key";
    }
    if (includeCredentials === "include") {
      return "credentials-include";
    }
    return "none";
  })();
  const url = endpointUrl ? chatCompletion2 ? endpointUrl + `/v1/chat/completions` : endpointUrl : providerConfig.makeUrl({
    authMethod,
    baseUrl: authMethod !== "provider-key" ? HF_HUB_INFERENCE_PROXY_TEMPLATE.replace("{{PROVIDER}}", provider) : providerConfig.makeBaseUrl(task),
    model: resolvedModel,
    chatCompletion: chatCompletion2,
    task
  });
  const binary = "data" in args && !!args.data;
  const headers = providerConfig.makeHeaders({
    accessToken,
    authMethod
  });
  if (!binary) {
    headers["Content-Type"] = "application/json";
  }
  const ownUserAgent = `${name}/${version}`;
  const userAgent = [ownUserAgent, typeof navigator !== "undefined" ? navigator.userAgent : void 0].filter((x) => x !== void 0).join(" ");
  headers["User-Agent"] = userAgent;
  const body = binary ? args.data : JSON.stringify(
    providerConfig.makeBody({
      args: remainingArgs,
      model: resolvedModel,
      task,
      chatCompletion: chatCompletion2
    })
  );
  let credentials;
  if (typeof includeCredentials === "string") {
    credentials = includeCredentials;
  } else if (includeCredentials === true) {
    credentials = "include";
  }
  const info = {
    headers,
    method: "POST",
    body,
    ...credentials ? { credentials } : void 0,
    signal
  };
  return { url, info };
}
async function loadDefaultModel(task) {
  if (!tasks) {
    tasks = await loadTaskInfo();
  }
  const taskInfo = tasks[task];
  if (((taskInfo == null ? void 0 : taskInfo.models.length) ?? 0) <= 0) {
    throw new Error(`No default model defined for task ${task}, please define the model explicitly.`);
  }
  return taskInfo.models[0].id;
}
async function loadTaskInfo() {
  const res = await fetch(`${HF_HUB_URL}/api/tasks`);
  if (!res.ok) {
    throw new Error("Failed to load tasks definitions from Hugging Face Hub.");
  }
  return await res.json();
}
function removeProviderPrefix(model, provider) {
  if (!model.startsWith(`${provider}/`)) {
    throw new Error(`Models from ${provider} must be prefixed by "${provider}/". Got "${model}".`);
  }
  return model.slice(provider.length + 1);
}
async function request(args, options) {
  var _a;
  const { url, info } = await makeRequestOptions(args, options);
  const response = await ((options == null ? void 0 : options.fetch) ?? fetch)(url, info);
  if ((options == null ? void 0 : options.retry_on_error) !== false && response.status === 503) {
    return request(args, options);
  }
  if (!response.ok) {
    const contentType = response.headers.get("Content-Type");
    if (["application/json", "application/problem+json"].some((ct) => contentType == null ? void 0 : contentType.startsWith(ct))) {
      const output = await response.json();
      if ([400, 422, 404, 500].includes(response.status) && (options == null ? void 0 : options.chatCompletion)) {
        throw new Error(
          `Server ${args.model} does not seem to support chat completion. Error: ${JSON.stringify(output.error)}`
        );
      }
      if (output.error || output.detail) {
        throw new Error(JSON.stringify(output.error ?? output.detail));
      } else {
        throw new Error(output);
      }
    }
    const message = (contentType == null ? void 0 : contentType.startsWith("text/plain;")) ? await response.text() : void 0;
    throw new Error(message ?? "An error occurred while fetching the blob");
  }
  if ((_a = response.headers.get("Content-Type")) == null ? void 0 : _a.startsWith("application/json")) {
    return await response.json();
  }
  return await response.blob();
}
function getLines(onLine) {
  let buffer;
  let position;
  let fieldLength;
  let discardTrailingNewline = false;
  return function onChunk(arr) {
    if (buffer === void 0) {
      buffer = arr;
      position = 0;
      fieldLength = -1;
    } else {
      buffer = concat(buffer, arr);
    }
    const bufLength = buffer.length;
    let lineStart = 0;
    while (position < bufLength) {
      if (discardTrailingNewline) {
        if (buffer[position] === 10) {
          lineStart = ++position;
        }
        discardTrailingNewline = false;
      }
      let lineEnd = -1;
      for (; position < bufLength && lineEnd === -1; ++position) {
        switch (buffer[position]) {
          case 58:
            if (fieldLength === -1) {
              fieldLength = position - lineStart;
            }
            break;
          case 13:
            discardTrailingNewline = true;
          case 10:
            lineEnd = position;
            break;
        }
      }
      if (lineEnd === -1) {
        break;
      }
      onLine(buffer.subarray(lineStart, lineEnd), fieldLength);
      lineStart = position;
      fieldLength = -1;
    }
    if (lineStart === bufLength) {
      buffer = void 0;
    } else if (lineStart !== 0) {
      buffer = buffer.subarray(lineStart);
      position -= lineStart;
    }
  };
}
function getMessages(onId, onRetry, onMessage) {
  let message = newMessage();
  const decoder = new TextDecoder();
  return function onLine(line, fieldLength) {
    if (line.length === 0) {
      onMessage == null ? void 0 : onMessage(message);
      message = newMessage();
    } else if (fieldLength > 0) {
      const field = decoder.decode(line.subarray(0, fieldLength));
      const valueOffset = fieldLength + (line[fieldLength + 1] === 32 ? 2 : 1);
      const value = decoder.decode(line.subarray(valueOffset));
      switch (field) {
        case "data":
          message.data = message.data ? message.data + "\n" + value : value;
          break;
        case "event":
          message.event = value;
          break;
        case "id":
          onId(message.id = value);
          break;
        case "retry":
          const retry = parseInt(value, 10);
          if (!isNaN(retry)) {
            onRetry(message.retry = retry);
          }
          break;
      }
    }
  };
}
function concat(a, b) {
  const res = new Uint8Array(a.length + b.length);
  res.set(a);
  res.set(b, a.length);
  return res;
}
function newMessage() {
  return {
    data: "",
    event: "",
    id: "",
    retry: void 0
  };
}
async function* streamingRequest(args, options) {
  var _a, _b;
  const { url, info } = await makeRequestOptions({ ...args, stream: true }, options);
  const response = await ((options == null ? void 0 : options.fetch) ?? fetch)(url, info);
  if ((options == null ? void 0 : options.retry_on_error) !== false && response.status === 503) {
    return yield* streamingRequest(args, options);
  }
  if (!response.ok) {
    if ((_a = response.headers.get("Content-Type")) == null ? void 0 : _a.startsWith("application/json")) {
      const output = await response.json();
      if ([400, 422, 404, 500].includes(response.status) && (options == null ? void 0 : options.chatCompletion)) {
        throw new Error(`Server ${args.model} does not seem to support chat completion. Error: ${output.error}`);
      }
      if (typeof output.error === "string") {
        throw new Error(output.error);
      }
      if (output.error && "message" in output.error && typeof output.error.message === "string") {
        throw new Error(output.error.message);
      }
    }
    throw new Error(`Server response contains error: ${response.status}`);
  }
  if (!((_b = response.headers.get("content-type")) == null ? void 0 : _b.startsWith("text/event-stream"))) {
    throw new Error(
      `Server does not support event stream content type, it returned ` + response.headers.get("content-type")
    );
  }
  if (!response.body) {
    return;
  }
  const reader = response.body.getReader();
  let events = [];
  const onEvent = (event) => {
    events.push(event);
  };
  const onChunk = getLines(
    getMessages(
      () => {
      },
      () => {
      },
      onEvent
    )
  );
  try {
    while (true) {
      const { done, value } = await reader.read();
      if (done) {
        return;
      }
      onChunk(value);
      for (const event of events) {
        if (event.data.length > 0) {
          if (event.data === "[DONE]") {
            return;
          }
          const data = JSON.parse(event.data);
          if (typeof data === "object" && data !== null && "error" in data) {
            const errorStr = typeof data.error === "string" ? data.error : typeof data.error === "object" && data.error && "message" in data.error && typeof data.error.message === "string" ? data.error.message : JSON.stringify(data.error);
            throw new Error(`Error forwarded from backend: ` + errorStr);
          }
          yield data;
        }
      }
      events = [];
    }
  } finally {
    reader.releaseLock();
  }
}
function pick(o, props) {
  return Object.assign(
    {},
    ...props.map((prop) => {
      if (o[prop] !== void 0) {
        return { [prop]: o[prop] };
      }
    })
  );
}
function typedInclude(arr, v) {
  return arr.includes(v);
}
function omit(o, props) {
  const propsArr = Array.isArray(props) ? props : [props];
  const letsKeep = Object.keys(o).filter((prop) => !typedInclude(propsArr, prop));
  return pick(o, letsKeep);
}
function preparePayload(args) {
  return "data" in args ? args : {
    ...omit(args, "inputs"),
    data: args.inputs
  };
}
async function audioClassification(args, options) {
  const payload = preparePayload(args);
  const res = await request(payload, {
    ...options,
    task: "audio-classification"
  });
  const isValidOutput = Array.isArray(res) && res.every((x) => typeof x.label === "string" && typeof x.score === "number");
  if (!isValidOutput) {
    throw new InferenceOutputError("Expected Array<{label: string, score: number}>");
  }
  return res;
}
function base64FromBytes(arr) {
  if (globalThis.Buffer) {
    return globalThis.Buffer.from(arr).toString("base64");
  } else {
    const bin = [];
    arr.forEach((byte) => {
      bin.push(String.fromCharCode(byte));
    });
    return globalThis.btoa(bin.join(""));
  }
}
async function automaticSpeechRecognition(args, options) {
  const payload = await buildPayload(args);
  const res = await request(payload, {
    ...options,
    task: "automatic-speech-recognition"
  });
  const isValidOutput = typeof (res == null ? void 0 : res.text) === "string";
  if (!isValidOutput) {
    throw new InferenceOutputError("Expected {text: string}");
  }
  return res;
}
var FAL_AI_SUPPORTED_BLOB_TYPES = ["audio/mpeg", "audio/mp4", "audio/wav", "audio/x-wav"];
async function buildPayload(args) {
  if (args.provider === "fal-ai") {
    const blob = "data" in args && args.data instanceof Blob ? args.data : "inputs" in args ? args.inputs : void 0;
    const contentType = blob == null ? void 0 : blob.type;
    if (!contentType) {
      throw new Error(
        `Unable to determine the input's content-type. Make sure your are passing a Blob when using provider fal-ai.`
      );
    }
    if (!FAL_AI_SUPPORTED_BLOB_TYPES.includes(contentType)) {
      throw new Error(
        `Provider fal-ai does not support blob type ${contentType} - supported content types are: ${FAL_AI_SUPPORTED_BLOB_TYPES.join(
          ", "
        )}`
      );
    }
    const base64audio = base64FromBytes(new Uint8Array(await blob.arrayBuffer()));
    return {
      ..."data" in args ? omit(args, "data") : omit(args, "inputs"),
      audio_url: `data:${contentType};base64,${base64audio}`
    };
  } else {
    return preparePayload(args);
  }
}
async function textToSpeech(args, options) {
  const payload = args.provider === "replicate" ? {
    ...omit(args, ["inputs", "parameters"]),
    ...args.parameters,
    text: args.inputs
  } : args;
  const res = await request(payload, {
    ...options,
    task: "text-to-speech"
  });
  if (res instanceof Blob) {
    return res;
  }
  if (res && typeof res === "object") {
    if ("output" in res) {
      if (typeof res.output === "string") {
        const urlResponse = await fetch(res.output);
        const blob = await urlResponse.blob();
        return blob;
      } else if (Array.isArray(res.output)) {
        const urlResponse = await fetch(res.output[0]);
        const blob = await urlResponse.blob();
        return blob;
      }
    }
  }
  throw new InferenceOutputError("Expected Blob or object with output");
}
async function audioToAudio(args, options) {
  const payload = preparePayload(args);
  const res = await request(payload, {
    ...options,
    task: "audio-to-audio"
  });
  return validateOutput(res);
}
function validateOutput(output) {
  if (!Array.isArray(output)) {
    throw new InferenceOutputError("Expected Array");
  }
  if (!output.every((elem) => {
    return typeof elem === "object" && elem && "label" in elem && typeof elem.label === "string" && "content-type" in elem && typeof elem["content-type"] === "string" && "blob" in elem && typeof elem.blob === "string";
  })) {
    throw new InferenceOutputError("Expected Array<{label: string, audio: Blob}>");
  }
  return output;
}
function preparePayload2(args) {
  return "data" in args ? args : { ...omit(args, "inputs"), data: args.inputs };
}
async function imageClassification(args, options) {
  const payload = preparePayload2(args);
  const res = await request(payload, {
    ...options,
    task: "image-classification"
  });
  const isValidOutput = Array.isArray(res) && res.every((x) => typeof x.label === "string" && typeof x.score === "number");
  if (!isValidOutput) {
    throw new InferenceOutputError("Expected Array<{label: string, score: number}>");
  }
  return res;
}
async function imageSegmentation(args, options) {
  const payload = preparePayload2(args);
  const res = await request(payload, {
    ...options,
    task: "image-segmentation"
  });
  const isValidOutput = Array.isArray(res) && res.every((x) => typeof x.label === "string" && typeof x.mask === "string" && typeof x.score === "number");
  if (!isValidOutput) {
    throw new InferenceOutputError("Expected Array<{label: string, mask: string, score: number}>");
  }
  return res;
}
async function imageToText(args, options) {
  var _a;
  const payload = preparePayload2(args);
  const res = (_a = await request(payload, {
    ...options,
    task: "image-to-text"
  })) == null ? void 0 : _a[0];
  if (typeof (res == null ? void 0 : res.generated_text) !== "string") {
    throw new InferenceOutputError("Expected {generated_text: string}");
  }
  return res;
}
async function objectDetection(args, options) {
  const payload = preparePayload2(args);
  const res = await request(payload, {
    ...options,
    task: "object-detection"
  });
  const isValidOutput = Array.isArray(res) && res.every(
    (x) => typeof x.label === "string" && typeof x.score === "number" && typeof x.box.xmin === "number" && typeof x.box.ymin === "number" && typeof x.box.xmax === "number" && typeof x.box.ymax === "number"
  );
  if (!isValidOutput) {
    throw new InferenceOutputError(
      "Expected Array<{label:string; score:number; box:{xmin:number; ymin:number; xmax:number; ymax:number}}>"
    );
  }
  return res;
}
function getResponseFormatArg(provider) {
  switch (provider) {
    case "fal-ai":
      return { sync_mode: true };
    case "nebius":
      return { response_format: "b64_json" };
    case "replicate":
      return void 0;
    case "together":
      return { response_format: "base64" };
    default:
      return void 0;
  }
}
async function textToImage(args, options) {
  const payload = !args.provider || args.provider === "hf-inference" || args.provider === "sambanova" ? args : {
    ...omit(args, ["inputs", "parameters"]),
    ...args.parameters,
    ...getResponseFormatArg(args.provider),
    prompt: args.inputs
  };
  const res = await request(payload, {
    ...options,
    task: "text-to-image"
  });
  if (res && typeof res === "object") {
    if (args.provider === "black-forest-labs" && "polling_url" in res && typeof res.polling_url === "string") {
      return await pollBflResponse(res.polling_url, options == null ? void 0 : options.outputType);
    }
    if (args.provider === "fal-ai" && "images" in res && Array.isArray(res.images) && res.images[0].url) {
      if ((options == null ? void 0 : options.outputType) === "url") {
        return res.images[0].url;
      } else {
        const image = await fetch(res.images[0].url);
        return await image.blob();
      }
    }
    if (args.provider === "hyperbolic" && "images" in res && Array.isArray(res.images) && res.images[0] && typeof res.images[0].image === "string") {
      if ((options == null ? void 0 : options.outputType) === "url") {
        return `data:image/jpeg;base64,${res.images[0].image}`;
      }
      const base64Response = await fetch(`data:image/jpeg;base64,${res.images[0].image}`);
      return await base64Response.blob();
    }
    if ("data" in res && Array.isArray(res.data) && res.data[0].b64_json) {
      const base64Data = res.data[0].b64_json;
      if ((options == null ? void 0 : options.outputType) === "url") {
        return `data:image/jpeg;base64,${base64Data}`;
      }
      const base64Response = await fetch(`data:image/jpeg;base64,${base64Data}`);
      return await base64Response.blob();
    }
    if ("output" in res && Array.isArray(res.output)) {
      if ((options == null ? void 0 : options.outputType) === "url") {
        return res.output[0];
      }
      const urlResponse = await fetch(res.output[0]);
      const blob = await urlResponse.blob();
      return blob;
    }
  }
  const isValidOutput = res && res instanceof Blob;
  if (!isValidOutput) {
    throw new InferenceOutputError("Expected Blob");
  }
  if ((options == null ? void 0 : options.outputType) === "url") {
    const b64 = await res.arrayBuffer().then((buf) => Buffer.from(buf).toString("base64"));
    return `data:image/jpeg;base64,${b64}`;
  }
  return res;
}
async function pollBflResponse(url, outputType) {
  const urlObj = new URL(url);
  for (let step = 0; step < 5; step++) {
    await delay(1e3);
    console.debug(`Polling Black Forest Labs API for the result... ${step + 1}/5`);
    urlObj.searchParams.set("attempt", step.toString(10));
    const resp = await fetch(urlObj, { headers: { "Content-Type": "application/json" } });
    if (!resp.ok) {
      throw new InferenceOutputError("Failed to fetch result from black forest labs API");
    }
    const payload = await resp.json();
    if (typeof payload === "object" && payload && "status" in payload && typeof payload.status === "string" && payload.status === "Ready" && "result" in payload && typeof payload.result === "object" && payload.result && "sample" in payload.result && typeof payload.result.sample === "string") {
      if (outputType === "url") {
        return payload.result.sample;
      }
      const image = await fetch(payload.result.sample);
      return await image.blob();
    }
  }
  throw new InferenceOutputError("Failed to fetch result from black forest labs API");
}
async function imageToImage(args, options) {
  let reqArgs;
  if (!args.parameters) {
    reqArgs = {
      accessToken: args.accessToken,
      model: args.model,
      data: args.inputs
    };
  } else {
    reqArgs = {
      ...args,
      inputs: base64FromBytes(
        new Uint8Array(args.inputs instanceof ArrayBuffer ? args.inputs : await args.inputs.arrayBuffer())
      )
    };
  }
  const res = await request(reqArgs, {
    ...options,
    task: "image-to-image"
  });
  const isValidOutput = res && res instanceof Blob;
  if (!isValidOutput) {
    throw new InferenceOutputError("Expected Blob");
  }
  return res;
}
async function preparePayload3(args) {
  if (args.inputs instanceof Blob) {
    return {
      ...args,
      inputs: {
        image: base64FromBytes(new Uint8Array(await args.inputs.arrayBuffer()))
      }
    };
  } else {
    return {
      ...args,
      inputs: {
        image: base64FromBytes(
          new Uint8Array(
            args.inputs.image instanceof ArrayBuffer ? args.inputs.image : await args.inputs.image.arrayBuffer()
          )
        )
      }
    };
  }
}
async function zeroShotImageClassification(args, options) {
  const payload = await preparePayload3(args);
  const res = await request(payload, {
    ...options,
    task: "zero-shot-image-classification"
  });
  const isValidOutput = Array.isArray(res) && res.every((x) => typeof x.label === "string" && typeof x.score === "number");
  if (!isValidOutput) {
    throw new InferenceOutputError("Expected Array<{label: string, score: number}>");
  }
  return res;
}
var SUPPORTED_PROVIDERS = ["fal-ai", "novita", "replicate"];
async function textToVideo(args, options) {
  if (!args.provider || !typedInclude(SUPPORTED_PROVIDERS, args.provider)) {
    throw new Error(
      `textToVideo inference is only supported for the following providers: ${SUPPORTED_PROVIDERS.join(", ")}`
    );
  }
  const payload = args.provider === "fal-ai" || args.provider === "replicate" || args.provider === "novita" ? { ...omit(args, ["inputs", "parameters"]), ...args.parameters, prompt: args.inputs } : args;
  const res = await request(payload, {
    ...options,
    task: "text-to-video"
  });
  if (args.provider === "fal-ai") {
    const { url, info } = await makeRequestOptions(args, { ...options, task: "text-to-video" });
    return await pollFalResponse(res, url, info.headers);
  } else if (args.provider === "novita") {
    const isValidOutput = typeof res === "object" && !!res && "video" in res && typeof res.video === "object" && !!res.video && "video_url" in res.video && typeof res.video.video_url === "string" && isUrl(res.video.video_url);
    if (!isValidOutput) {
      throw new InferenceOutputError("Expected { video: { video_url: string } }");
    }
    const urlResponse = await fetch(res.video.video_url);
    return await urlResponse.blob();
  } else {
    const isValidOutput = typeof res === "object" && !!res && "output" in res && typeof res.output === "string" && isUrl(res.output);
    if (!isValidOutput) {
      throw new InferenceOutputError("Expected { output: string }");
    }
    const urlResponse = await fetch(res.output);
    return await urlResponse.blob();
  }
}
async function featureExtraction(args, options) {
  const res = await request(args, {
    ...options,
    task: "feature-extraction"
  });
  let isValidOutput = true;
  const isNumArrayRec = (arr, maxDepth, curDepth = 0) => {
    if (curDepth > maxDepth)
      return false;
    if (arr.every((x) => Array.isArray(x))) {
      return arr.every((x) => isNumArrayRec(x, maxDepth, curDepth + 1));
    } else {
      return arr.every((x) => typeof x === "number");
    }
  };
  isValidOutput = Array.isArray(res) && isNumArrayRec(res, 3, 0);
  if (!isValidOutput) {
    throw new InferenceOutputError("Expected Array<number[][][] | number[][] | number[] | number>");
  }
  return res;
}
async function fillMask(args, options) {
  const res = await request(args, {
    ...options,
    task: "fill-mask"
  });
  const isValidOutput = Array.isArray(res) && res.every(
    (x) => typeof x.score === "number" && typeof x.sequence === "string" && typeof x.token === "number" && typeof x.token_str === "string"
  );
  if (!isValidOutput) {
    throw new InferenceOutputError(
      "Expected Array<{score: number, sequence: string, token: number, token_str: string}>"
    );
  }
  return res;
}
async function questionAnswering(args, options) {
  const res = await request(args, {
    ...options,
    task: "question-answering"
  });
  const isValidOutput = Array.isArray(res) ? res.every(
    (elem) => typeof elem === "object" && !!elem && typeof elem.answer === "string" && typeof elem.end === "number" && typeof elem.score === "number" && typeof elem.start === "number"
  ) : typeof res === "object" && !!res && typeof res.answer === "string" && typeof res.end === "number" && typeof res.score === "number" && typeof res.start === "number";
  if (!isValidOutput) {
    throw new InferenceOutputError("Expected Array<{answer: string, end: number, score: number, start: number}>");
  }
  return Array.isArray(res) ? res[0] : res;
}
async function sentenceSimilarity(args, options) {
  const res = await request(prepareInput(args), {
    ...options,
    task: "sentence-similarity"
  });
  const isValidOutput = Array.isArray(res) && res.every((x) => typeof x === "number");
  if (!isValidOutput) {
    throw new InferenceOutputError("Expected number[]");
  }
  return res;
}
function prepareInput(args) {
  return {
    ...omit(args, ["inputs", "parameters"]),
    inputs: { ...omit(args.inputs, "sourceSentence") },
    parameters: { source_sentence: args.inputs.sourceSentence, ...args.parameters }
  };
}
async function summarization(args, options) {
  const res = await request(args, {
    ...options,
    task: "summarization"
  });
  const isValidOutput = Array.isArray(res) && res.every((x) => typeof (x == null ? void 0 : x.summary_text) === "string");
  if (!isValidOutput) {
    throw new InferenceOutputError("Expected Array<{summary_text: string}>");
  }
  return res == null ? void 0 : res[0];
}
async function tableQuestionAnswering(args, options) {
  const res = await request(args, {
    ...options,
    task: "table-question-answering"
  });
  const isValidOutput = Array.isArray(res) ? res.every((elem) => validate(elem)) : validate(res);
  if (!isValidOutput) {
    throw new InferenceOutputError(
      "Expected {aggregator: string, answer: string, cells: string[], coordinates: number[][]}"
    );
  }
  return Array.isArray(res) ? res[0] : res;
}
function validate(elem) {
  return typeof elem === "object" && !!elem && "aggregator" in elem && typeof elem.aggregator === "string" && "answer" in elem && typeof elem.answer === "string" && "cells" in elem && Array.isArray(elem.cells) && elem.cells.every((x) => typeof x === "string") && "coordinates" in elem && Array.isArray(elem.coordinates) && elem.coordinates.every(
    (coord) => Array.isArray(coord) && coord.every((x) => typeof x === "number")
  );
}
async function textClassification(args, options) {
  var _a;
  const res = (_a = await request(args, {
    ...options,
    task: "text-classification"
  })) == null ? void 0 : _a[0];
  const isValidOutput = Array.isArray(res) && res.every((x) => typeof (x == null ? void 0 : x.label) === "string" && typeof x.score === "number");
  if (!isValidOutput) {
    throw new InferenceOutputError("Expected Array<{label: string, score: number}>");
  }
  return res;
}
function toArray(obj) {
  if (Array.isArray(obj)) {
    return obj;
  }
  return [obj];
}
async function textGeneration(args, options) {
  if (args.provider === "together") {
    args.prompt = args.inputs;
    const raw = await request(args, {
      ...options,
      task: "text-generation"
    });
    const isValidOutput = typeof raw === "object" && "choices" in raw && Array.isArray(raw == null ? void 0 : raw.choices) && typeof (raw == null ? void 0 : raw.model) === "string";
    if (!isValidOutput) {
      throw new InferenceOutputError("Expected ChatCompletionOutput");
    }
    const completion = raw.choices[0];
    return {
      generated_text: completion.text
    };
  } else if (args.provider === "hyperbolic") {
    const payload = {
      messages: [{ content: args.inputs, role: "user" }],
      ...args.parameters ? {
        max_tokens: args.parameters.max_new_tokens,
        ...omit(args.parameters, "max_new_tokens")
      } : void 0,
      ...omit(args, ["inputs", "parameters"])
    };
    const raw = await request(payload, {
      ...options,
      task: "text-generation"
    });
    const isValidOutput = typeof raw === "object" && "choices" in raw && Array.isArray(raw == null ? void 0 : raw.choices) && typeof (raw == null ? void 0 : raw.model) === "string";
    if (!isValidOutput) {
      throw new InferenceOutputError("Expected ChatCompletionOutput");
    }
    const completion = raw.choices[0];
    return {
      generated_text: completion.message.content
    };
  } else {
    const res = toArray(
      await request(args, {
        ...options,
        task: "text-generation"
      })
    );
    const isValidOutput = Array.isArray(res) && res.every((x) => "generated_text" in x && typeof (x == null ? void 0 : x.generated_text) === "string");
    if (!isValidOutput) {
      throw new InferenceOutputError("Expected Array<{generated_text: string}>");
    }
    return res == null ? void 0 : res[0];
  }
}
async function* textGenerationStream(args, options) {
  yield* streamingRequest(args, {
    ...options,
    task: "text-generation"
  });
}
async function tokenClassification(args, options) {
  const res = toArray(
    await request(args, {
      ...options,
      task: "token-classification"
    })
  );
  const isValidOutput = Array.isArray(res) && res.every(
    (x) => typeof x.end === "number" && typeof x.entity_group === "string" && typeof x.score === "number" && typeof x.start === "number" && typeof x.word === "string"
  );
  if (!isValidOutput) {
    throw new InferenceOutputError(
      "Expected Array<{end: number, entity_group: string, score: number, start: number, word: string}>"
    );
  }
  return res;
}
async function translation(args, options) {
  const res = await request(args, {
    ...options,
    task: "translation"
  });
  const isValidOutput = Array.isArray(res) && res.every((x) => typeof (x == null ? void 0 : x.translation_text) === "string");
  if (!isValidOutput) {
    throw new InferenceOutputError("Expected type Array<{translation_text: string}>");
  }
  return (res == null ? void 0 : res.length) === 1 ? res == null ? void 0 : res[0] : res;
}
async function zeroShotClassification(args, options) {
  const res = toArray(
    await request(args, {
      ...options,
      task: "zero-shot-classification"
    })
  );
  const isValidOutput = Array.isArray(res) && res.every(
    (x) => Array.isArray(x.labels) && x.labels.every((_label) => typeof _label === "string") && Array.isArray(x.scores) && x.scores.every((_score) => typeof _score === "number") && typeof x.sequence === "string"
  );
  if (!isValidOutput) {
    throw new InferenceOutputError("Expected Array<{labels: string[], scores: number[], sequence: string}>");
  }
  return res;
}
async function chatCompletion(args, options) {
  const res = await request(args, {
    ...options,
    task: "text-generation",
    chatCompletion: true
  });
  const isValidOutput = typeof res === "object" && Array.isArray(res == null ? void 0 : res.choices) && typeof (res == null ? void 0 : res.created) === "number" && typeof (res == null ? void 0 : res.id) === "string" && typeof (res == null ? void 0 : res.model) === "string" && /// Together.ai and Nebius do not output a system_fingerprint
  (res.system_fingerprint === void 0 || res.system_fingerprint === null || typeof res.system_fingerprint === "string") && typeof (res == null ? void 0 : res.usage) === "object";
  if (!isValidOutput) {
    throw new InferenceOutputError("Expected ChatCompletionOutput");
  }
  return res;
}
async function* chatCompletionStream(args, options) {
  yield* streamingRequest(args, {
    ...options,
    task: "text-generation",
    chatCompletion: true
  });
}
async function documentQuestionAnswering(args, options) {
  const reqArgs = {
    ...args,
    inputs: {
      question: args.inputs.question,
      // convert Blob or ArrayBuffer to base64
      image: base64FromBytes(new Uint8Array(await args.inputs.image.arrayBuffer()))
    }
  };
  const res = toArray(
    await request(reqArgs, {
      ...options,
      task: "document-question-answering"
    })
  );
  const isValidOutput = Array.isArray(res) && res.every(
    (elem) => typeof elem === "object" && !!elem && typeof (elem == null ? void 0 : elem.answer) === "string" && (typeof elem.end === "number" || typeof elem.end === "undefined") && (typeof elem.score === "number" || typeof elem.score === "undefined") && (typeof elem.start === "number" || typeof elem.start === "undefined")
  );
  if (!isValidOutput) {
    throw new InferenceOutputError("Expected Array<{answer: string, end?: number, score?: number, start?: number}>");
  }
  return res[0];
}
async function visualQuestionAnswering(args, options) {
  const reqArgs = {
    ...args,
    inputs: {
      question: args.inputs.question,
      // convert Blob or ArrayBuffer to base64
      image: base64FromBytes(new Uint8Array(await args.inputs.image.arrayBuffer()))
    }
  };
  const res = await request(reqArgs, {
    ...options,
    task: "visual-question-answering"
  });
  const isValidOutput = Array.isArray(res) && res.every(
    (elem) => typeof elem === "object" && !!elem && typeof (elem == null ? void 0 : elem.answer) === "string" && typeof elem.score === "number"
  );
  if (!isValidOutput) {
    throw new InferenceOutputError("Expected Array<{answer: string, score: number}>");
  }
  return res[0];
}
async function tabularRegression(args, options) {
  const res = await request(args, {
    ...options,
    task: "tabular-regression"
  });
  const isValidOutput = Array.isArray(res) && res.every((x) => typeof x === "number");
  if (!isValidOutput) {
    throw new InferenceOutputError("Expected number[]");
  }
  return res;
}
async function tabularClassification(args, options) {
  const res = await request(args, {
    ...options,
    task: "tabular-classification"
  });
  const isValidOutput = Array.isArray(res) && res.every((x) => typeof x === "number");
  if (!isValidOutput) {
    throw new InferenceOutputError("Expected number[]");
  }
  return res;
}
var InferenceClient = class {
  constructor(accessToken = "", defaultOptions = {}) {
    __publicField(this, "accessToken");
    __publicField(this, "defaultOptions");
    this.accessToken = accessToken;
    this.defaultOptions = defaultOptions;
    for (const [name2, fn] of Object.entries(tasks_exports)) {
      Object.defineProperty(this, name2, {
        enumerable: false,
        value: (params, options) => (
          // eslint-disable-next-line @typescript-eslint/no-explicit-any
          fn({ ...params, accessToken }, { ...defaultOptions, ...options })
        )
      });
    }
  }
  /**
   * Returns copy of InferenceClient tied to a specified endpoint.
   */
  endpoint(endpointUrl) {
    return new InferenceClientEndpoint(endpointUrl, this.accessToken, this.defaultOptions);
  }
};
var InferenceClientEndpoint = class {
  constructor(endpointUrl, accessToken = "", defaultOptions = {}) {
    accessToken;
    defaultOptions;
    for (const [name2, fn] of Object.entries(tasks_exports)) {
      Object.defineProperty(this, name2, {
        enumerable: false,
        value: (params, options) => (
          // eslint-disable-next-line @typescript-eslint/no-explicit-any
          fn({ ...params, accessToken, endpointUrl }, { ...defaultOptions, ...options })
        )
      });
    }
  }
};
var HfInference = class extends InferenceClient {
};
var INFERENCE_PROVIDERS = [
  "black-forest-labs",
  "cerebras",
  "cohere",
  "fal-ai",
  "fireworks-ai",
  "hf-inference",
  "hyperbolic",
  "nebius",
  "novita",
  "openai",
  "replicate",
  "sambanova",
  "together"
];
var snippets_exports = {};
__export(snippets_exports, {
  getInferenceSnippets: () => getInferenceSnippets
});
var templates = {
  "js": {
    "fetch": {
      "basic": 'async function query(data) {\n	const response = await fetch(\n		"{{ fullUrl }}",\n		{\n			headers: {\n				Authorization: "{{ authorizationHeader }}",\n				"Content-Type": "application/json",\n			},\n			method: "POST",\n			body: JSON.stringify(data),\n		}\n	);\n	const result = await response.json();\n	return result;\n}\n\nquery({ inputs: {{ providerInputs.asObj.inputs }} }).then((response) => {\n    console.log(JSON.stringify(response));\n});',
      "basicAudio": 'async function query(data) {\n	const response = await fetch(\n		"{{ fullUrl }}",\n		{\n			headers: {\n				Authorization: "{{ authorizationHeader }}",\n				"Content-Type": "audio/flac"\n			},\n			method: "POST",\n			body: JSON.stringify(data),\n		}\n	);\n	const result = await response.json();\n	return result;\n}\n\nquery({ inputs: {{ providerInputs.asObj.inputs }} }).then((response) => {\n    console.log(JSON.stringify(response));\n});',
      "basicImage": 'async function query(data) {\n	const response = await fetch(\n		"{{ fullUrl }}",\n		{\n			headers: {\n				Authorization: "{{ authorizationHeader }}",\n				"Content-Type": "image/jpeg"\n			},\n			method: "POST",\n			body: JSON.stringify(data),\n		}\n	);\n	const result = await response.json();\n	return result;\n}\n\nquery({ inputs: {{ providerInputs.asObj.inputs }} }).then((response) => {\n    console.log(JSON.stringify(response));\n});',
      "textToAudio": '{% if model.library_name == "transformers" %}\nasync function query(data) {\n	const response = await fetch(\n		"{{ fullUrl }}",\n		{\n			headers: {\n				Authorization: "{{ authorizationHeader }}",\n				"Content-Type": "application/json",\n			},\n			method: "POST",\n			body: JSON.stringify(data),\n		}\n	);\n	const result = await response.blob();\n    return result;\n}\n\nquery({ inputs: {{ providerInputs.asObj.inputs }} }).then((response) => {\n    // Returns a byte object of the Audio wavform. Use it directly!\n});\n{% else %}\nasync function query(data) {\n	const response = await fetch(\n		"{{ fullUrl }}",\n		{\n			headers: {\n				Authorization: "{{ authorizationHeader }}",\n				"Content-Type": "application/json",\n			},\n			method: "POST",\n			body: JSON.stringify(data),\n		}\n	);\n    const result = await response.json();\n    return result;\n}\n\nquery({ inputs: {{ providerInputs.asObj.inputs }} }).then((response) => {\n    console.log(JSON.stringify(response));\n});\n{% endif %} ',
      "textToImage": 'async function query(data) {\n	const response = await fetch(\n		"{{ fullUrl }}",\n		{\n			headers: {\n				Authorization: "{{ authorizationHeader }}",\n				"Content-Type": "application/json",\n			},\n			method: "POST",\n			body: JSON.stringify(data),\n		}\n	);\n	const result = await response.blob();\n	return result;\n}\n\nquery({ inputs: {{ providerInputs.asObj.inputs }} }).then((response) => {\n    // Use image\n});',
      "zeroShotClassification": 'async function query(data) {\n    const response = await fetch(\n		"{{ fullUrl }}",\n        {\n            headers: {\n				Authorization: "{{ authorizationHeader }}",\n                "Content-Type": "application/json",\n            },\n            method: "POST",\n            body: JSON.stringify(data),\n        }\n    );\n    const result = await response.json();\n    return result;\n}\n\nquery({\n    inputs: {{ providerInputs.asObj.inputs }},\n    parameters: { candidate_labels: ["refund", "legal", "faq"] }\n}).then((response) => {\n    console.log(JSON.stringify(response));\n});'
    },
    "huggingface.js": {
      "basic": 'import { InferenceClient } from "@huggingface/inference";\n\nconst client = new InferenceClient("{{ accessToken }}");\n\nconst output = await client.{{ methodName }}({\n	model: "{{ model.id }}",\n	inputs: {{ inputs.asObj.inputs }},\n	provider: "{{ provider }}",\n});\n\nconsole.log(output);',
      "basicAudio": 'import { InferenceClient } from "@huggingface/inference";\n\nconst client = new InferenceClient("{{ accessToken }}");\n\nconst data = fs.readFileSync({{inputs.asObj.inputs}});\n\nconst output = await client.{{ methodName }}({\n	data,\n	model: "{{ model.id }}",\n	provider: "{{ provider }}",\n});\n\nconsole.log(output);',
      "basicImage": 'import { InferenceClient } from "@huggingface/inference";\n\nconst client = new InferenceClient("{{ accessToken }}");\n\nconst data = fs.readFileSync({{inputs.asObj.inputs}});\n\nconst output = await client.{{ methodName }}({\n	data,\n	model: "{{ model.id }}",\n	provider: "{{ provider }}",\n});\n\nconsole.log(output);',
      "conversational": 'import { InferenceClient } from "@huggingface/inference";\n\nconst client = new InferenceClient("{{ accessToken }}");\n\nconst chatCompletion = await client.chatCompletion({\n    provider: "{{ provider }}",\n    model: "{{ model.id }}",\n{{ inputs.asTsString }}\n});\n\nconsole.log(chatCompletion.choices[0].message);',
      "conversationalStream": 'import { InferenceClient } from "@huggingface/inference";\n\nconst client = new InferenceClient("{{ accessToken }}");\n\nlet out = "";\n\nconst stream = await client.chatCompletionStream({\n    provider: "{{ provider }}",\n    model: "{{ model.id }}",\n{{ inputs.asTsString }}\n});\n\nfor await (const chunk of stream) {\n	if (chunk.choices && chunk.choices.length > 0) {\n		const newContent = chunk.choices[0].delta.content;\n		out += newContent;\n		console.log(newContent);\n	}  \n}',
      "textToImage": `import { InferenceClient } from "@huggingface/inference";

const client = new InferenceClient("{{ accessToken }}");

const image = await client.textToImage({
    provider: "{{ provider }}",
    model: "{{ model.id }}",
	inputs: {{ inputs.asObj.inputs }},
	parameters: { num_inference_steps: 5 },
});
/// Use the generated image (it's a Blob)`,
      "textToVideo": `import { InferenceClient } from "@huggingface/inference";

const client = new InferenceClient("{{ accessToken }}");

const image = await client.textToVideo({
    provider: "{{ provider }}",
    model: "{{ model.id }}",
	inputs: {{ inputs.asObj.inputs }},
});
// Use the generated video (it's a Blob)`
    },
    "openai": {
      "conversational": 'import { OpenAI } from "openai";\n\nconst client = new OpenAI({\n	baseURL: "{{ baseUrl }}",\n	apiKey: "{{ accessToken }}",\n});\n\nconst chatCompletion = await client.chat.completions.create({\n	model: "{{ providerModelId }}",\n{{ inputs.asTsString }}\n});\n\nconsole.log(chatCompletion.choices[0].message);',
      "conversationalStream": 'import { OpenAI } from "openai";\n\nconst client = new OpenAI({\n	baseURL: "{{ baseUrl }}",\n	apiKey: "{{ accessToken }}",\n});\n\nlet out = "";\n\nconst stream = await client.chat.completions.create({\n    provider: "{{ provider }}",\n    model: "{{ model.id }}",\n{{ inputs.asTsString }}\n});\n\nfor await (const chunk of stream) {\n	if (chunk.choices && chunk.choices.length > 0) {\n		const newContent = chunk.choices[0].delta.content;\n		out += newContent;\n		console.log(newContent);\n	}  \n}'
    }
  },
  "python": {
    "fal_client": {
      "textToImage": '{% if provider == "fal-ai" %}\nimport fal_client\n\nresult = fal_client.subscribe(\n    "{{ providerModelId }}",\n    arguments={\n        "prompt": {{ inputs.asObj.inputs }},\n    },\n)\nprint(result)\n{% endif %} '
    },
    "huggingface_hub": {
      "basic": 'result = client.{{ methodName }}(\n    inputs={{ inputs.asObj.inputs }},\n    model="{{ model.id }}",\n)',
      "basicAudio": 'output = client.{{ methodName }}({{ inputs.asObj.inputs }}, model="{{ model.id }}")',
      "basicImage": 'output = client.{{ methodName }}({{ inputs.asObj.inputs }}, model="{{ model.id }}")',
      "conversational": 'completion = client.chat.completions.create(\n    model="{{ model.id }}",\n{{ inputs.asPythonString }}\n)\n\nprint(completion.choices[0].message) ',
      "conversationalStream": 'stream = client.chat.completions.create(\n    model="{{ model.id }}",\n{{ inputs.asPythonString }}\n    stream=True,\n)\n\nfor chunk in stream:\n    print(chunk.choices[0].delta.content, end="") ',
      "documentQuestionAnswering": 'output = client.document_question_answering(\n    "{{ inputs.asObj.image }}",\n    question="{{ inputs.asObj.question }}",\n    model="{{ model.id }}",\n) ',
      "imageToImage": '# output is a PIL.Image object\nimage = client.image_to_image(\n    "{{ inputs.asObj.inputs }}",\n    prompt="{{ inputs.asObj.parameters.prompt }}",\n    model="{{ model.id }}",\n) ',
      "importInferenceClient": 'from huggingface_hub import InferenceClient\n\nclient = InferenceClient(\n    provider="{{ provider }}",\n    api_key="{{ accessToken }}",\n)',
      "textToImage": '# output is a PIL.Image object\nimage = client.text_to_image(\n    {{ inputs.asObj.inputs }},\n    model="{{ model.id }}",\n) ',
      "textToVideo": 'video = client.text_to_video(\n    {{ inputs.asObj.inputs }},\n    model="{{ model.id }}",\n) '
    },
    "openai": {
      "conversational": 'from openai import OpenAI\n\nclient = OpenAI(\n    base_url="{{ baseUrl }}",\n    api_key="{{ accessToken }}"\n)\n\ncompletion = client.chat.completions.create(\n    model="{{ providerModelId }}",\n{{ inputs.asPythonString }}\n)\n\nprint(completion.choices[0].message) ',
      "conversationalStream": 'from openai import OpenAI\n\nclient = OpenAI(\n    base_url="{{ baseUrl }}",\n    api_key="{{ accessToken }}"\n)\n\nstream = client.chat.completions.create(\n    model="{{ providerModelId }}",\n{{ inputs.asPythonString }}\n    stream=True,\n)\n\nfor chunk in stream:\n    print(chunk.choices[0].delta.content, end="")'
    },
    "requests": {
      "basic": 'def query(payload):\n    response = requests.post(API_URL, headers=headers, json=payload)\n    return response.json()\n\noutput = query({\n    "inputs": {{ providerInputs.asObj.inputs }},\n}) ',
      "basicAudio": 'def query(filename):\n    with open(filename, "rb") as f:\n        data = f.read()\n    response = requests.post(API_URL, headers={"Content-Type": "audio/flac", **headers}, data=data)\n    return response.json()\n\noutput = query({{ providerInputs.asObj.inputs }})',
      "basicImage": 'def query(filename):\n    with open(filename, "rb") as f:\n        data = f.read()\n    response = requests.post(API_URL, headers={"Content-Type": "image/jpeg", **headers}, data=data)\n    return response.json()\n\noutput = query({{ providerInputs.asObj.inputs }})',
      "conversational": 'def query(payload):\n    response = requests.post(API_URL, headers=headers, json=payload)\n    return response.json()\n\nresponse = query({\n{{ providerInputs.asJsonString }}\n})\n\nprint(response["choices"][0]["message"])',
      "conversationalStream": 'def query(payload):\n    response = requests.post(API_URL, headers=headers, json=payload, stream=True)\n    for line in response.iter_lines():\n        if not line.startswith(b"data:"):\n            continue\n        if line.strip() == b"data: [DONE]":\n            return\n        yield json.loads(line.decode("utf-8").lstrip("data:").rstrip("/n"))\n\nchunks = query({\n{{ providerInputs.asJsonString }},\n    "stream": True,\n})\n\nfor chunk in chunks:\n    print(chunk["choices"][0]["delta"]["content"], end="")',
      "documentQuestionAnswering": 'def query(payload):\n    with open(payload["image"], "rb") as f:\n        img = f.read()\n        payload["image"] = base64.b64encode(img).decode("utf-8")\n    response = requests.post(API_URL, headers=headers, json=payload)\n    return response.json()\n\noutput = query({\n    "inputs": {\n        "image": "{{ inputs.asObj.image }}",\n        "question": "{{ inputs.asObj.question }}",\n    },\n}) ',
      "imageToImage": 'def query(payload):\n    with open(payload["inputs"], "rb") as f:\n        img = f.read()\n        payload["inputs"] = base64.b64encode(img).decode("utf-8")\n    response = requests.post(API_URL, headers=headers, json=payload)\n    return response.content\n\nimage_bytes = query({\n{{ providerInputs.asJsonString }}\n})\n\n# You can access the image with PIL.Image for example\nimport io\nfrom PIL import Image\nimage = Image.open(io.BytesIO(image_bytes)) ',
      "importRequests": '{% if importBase64 %}\nimport base64\n{% endif %}\n{% if importJson %}\nimport json\n{% endif %}\nimport requests\n\nAPI_URL = "{{ fullUrl }}"\nheaders = {"Authorization": "{{ authorizationHeader }}"}',
      "tabular": 'def query(payload):\n    response = requests.post(API_URL, headers=headers, json=payload)\n    return response.content\n\nresponse = query({\n    "inputs": {\n        "data": {{ providerInputs.asObj.inputs }}\n    },\n}) ',
      "textToAudio": '{% if model.library_name == "transformers" %}\ndef query(payload):\n    response = requests.post(API_URL, headers=headers, json=payload)\n    return response.content\n\naudio_bytes = query({\n    "inputs": {{ providerInputs.asObj.inputs }},\n})\n# You can access the audio with IPython.display for example\nfrom IPython.display import Audio\nAudio(audio_bytes)\n{% else %}\ndef query(payload):\n    response = requests.post(API_URL, headers=headers, json=payload)\n    return response.json()\n\naudio, sampling_rate = query({\n    "inputs": {{ providerInputs.asObj.inputs }},\n})\n# You can access the audio with IPython.display for example\nfrom IPython.display import Audio\nAudio(audio, rate=sampling_rate)\n{% endif %} ',
      "textToImage": '{% if provider == "hf-inference" %}\ndef query(payload):\n    response = requests.post(API_URL, headers=headers, json=payload)\n    return response.content\n\nimage_bytes = query({\n    "inputs": {{ providerInputs.asObj.inputs }},\n})\n\n# You can access the image with PIL.Image for example\nimport io\nfrom PIL import Image\nimage = Image.open(io.BytesIO(image_bytes))\n{% endif %}',
      "zeroShotClassification": 'def query(payload):\n    response = requests.post(API_URL, headers=headers, json=payload)\n    return response.json()\n\noutput = query({\n    "inputs": {{ providerInputs.asObj.inputs }},\n    "parameters": {"candidate_labels": ["refund", "legal", "faq"]},\n}) ',
      "zeroShotImageClassification": 'def query(data):\n    with open(data["image_path"], "rb") as f:\n        img = f.read()\n    payload={\n        "parameters": data["parameters"],\n        "inputs": base64.b64encode(img).decode("utf-8")\n    }\n    response = requests.post(API_URL, headers=headers, json=payload)\n    return response.json()\n\noutput = query({\n    "image_path": {{ providerInputs.asObj.inputs }},\n    "parameters": {"candidate_labels": ["cat", "dog", "llama"]},\n}) '
    }
  },
  "sh": {
    "curl": {
      "basic": "curl {{ fullUrl }} \\\n    -X POST \\\n    -H 'Authorization: {{ authorizationHeader }}' \\\n    -H 'Content-Type: application/json' \\\n    -d '{\n{{ providerInputs.asCurlString }}\n    }'",
      "basicAudio": "curl {{ fullUrl }} \\\n    -X POST \\\n    -H 'Authorization: {{ authorizationHeader }}' \\\n    -H 'Content-Type: audio/flac' \\\n    --data-binary @{{ providerInputs.asObj.inputs }}",
      "basicImage": "curl {{ fullUrl }} \\\n    -X POST \\\n    -H 'Authorization: {{ authorizationHeader }}' \\\n    -H 'Content-Type: image/jpeg' \\\n    --data-binary @{{ providerInputs.asObj.inputs }}",
      "conversational": `curl {{ fullUrl }} \\
    -H 'Authorization: {{ authorizationHeader }}' \\
    -H 'Content-Type: application/json' \\
    -d '{
{{ providerInputs.asCurlString }},
        "stream": false
    }'`,
      "conversationalStream": `curl {{ fullUrl }} \\
    -H 'Authorization: {{ authorizationHeader }}' \\
    -H 'Content-Type: application/json' \\
    -d '{
{{ providerInputs.asCurlString }},
        "stream": true
    }'`,
      "zeroShotClassification": `curl {{ fullUrl }} \\
    -X POST \\
    -d '{"inputs": {{ providerInputs.asObj.inputs }}, "parameters": {"candidate_labels": ["refund", "legal", "faq"]}}' \\
    -H 'Content-Type: application/json' \\
    -H 'Authorization: {{ authorizationHeader }}'`
    }
  }
};
var PYTHON_CLIENTS = ["huggingface_hub", "fal_client", "requests", "openai"];
var JS_CLIENTS = ["fetch", "huggingface.js", "openai"];
var SH_CLIENTS = ["curl"];
var CLIENTS = {
  js: [...JS_CLIENTS],
  python: [...PYTHON_CLIENTS],
  sh: [...SH_CLIENTS]
};
var hasTemplate = (language, client, templateName) => {
  var _a, _b;
  return ((_b = (_a = templates[language]) == null ? void 0 : _a[client]) == null ? void 0 : _b[templateName]) !== void 0;
};
var loadTemplate = (language, client, templateName) => {
  var _a, _b;
  const template = (_b = (_a = templates[language]) == null ? void 0 : _a[client]) == null ? void 0 : _b[templateName];
  if (!template) {
    throw new Error(`Template not found: ${language}/${client}/${templateName}`);
  }
  return (data) => new Template(template).render({ ...data });
};
var snippetImportPythonInferenceClient = loadTemplate("python", "huggingface_hub", "importInferenceClient");
var snippetImportRequests = loadTemplate("python", "requests", "importRequests");
var HF_PYTHON_METHODS = {
  "audio-classification": "audio_classification",
  "audio-to-audio": "audio_to_audio",
  "automatic-speech-recognition": "automatic_speech_recognition",
  "document-question-answering": "document_question_answering",
  "feature-extraction": "feature_extraction",
  "fill-mask": "fill_mask",
  "image-classification": "image_classification",
  "image-segmentation": "image_segmentation",
  "image-to-image": "image_to_image",
  "image-to-text": "image_to_text",
  "object-detection": "object_detection",
  "question-answering": "question_answering",
  "sentence-similarity": "sentence_similarity",
  summarization: "summarization",
  "table-question-answering": "table_question_answering",
  "tabular-classification": "tabular_classification",
  "tabular-regression": "tabular_regression",
  "text-classification": "text_classification",
  "text-generation": "text_generation",
  "text-to-image": "text_to_image",
  "text-to-speech": "text_to_speech",
  "text-to-video": "text_to_video",
  "token-classification": "token_classification",
  translation: "translation",
  "visual-question-answering": "visual_question_answering",
  "zero-shot-classification": "zero_shot_classification",
  "zero-shot-image-classification": "zero_shot_image_classification"
};
var HF_JS_METHODS = {
  "automatic-speech-recognition": "automaticSpeechRecognition",
  "feature-extraction": "featureExtraction",
  "fill-mask": "fillMask",
  "image-classification": "imageClassification",
  "question-answering": "questionAnswering",
  "sentence-similarity": "sentenceSimilarity",
  summarization: "summarization",
  "table-question-answering": "tableQuestionAnswering",
  "text-classification": "textClassification",
  "text-generation": "textGeneration",
  "text2text-generation": "textGeneration",
  "token-classification": "tokenClassification",
  translation: "translation"
};
var snippetGenerator = (templateName, inputPreparationFn) => {
  return (model, accessToken, provider, providerModelId, opts) => {
    var _a;
    if (model.pipeline_tag && ["text-generation", "image-text-to-text"].includes(model.pipeline_tag) && model.tags.includes("conversational")) {
      templateName = (opts == null ? void 0 : opts.streaming) ? "conversationalStream" : "conversational";
      inputPreparationFn = prepareConversationalInput;
    }
    const inputs = inputPreparationFn ? inputPreparationFn(model, opts) : { inputs: getModelInputSnippet(model) };
    const request2 = makeRequestOptionsFromResolvedModel(
      providerModelId ?? model.id,
      { accessToken, provider, ...inputs },
      { chatCompletion: templateName.includes("conversational"), task: model.pipeline_tag }
    );
    let providerInputs = inputs;
    const bodyAsObj = request2.info.body;
    if (typeof bodyAsObj === "string") {
      try {
        providerInputs = JSON.parse(bodyAsObj);
      } catch (e) {
        console.error("Failed to parse body as JSON", e);
      }
    }
    const params = {
      accessToken,
      authorizationHeader: (_a = request2.info.headers) == null ? void 0 : _a.Authorization,
      baseUrl: removeSuffix(request2.url, "/chat/completions"),
      fullUrl: request2.url,
      inputs: {
        asObj: inputs,
        asCurlString: formatBody(inputs, "curl"),
        asJsonString: formatBody(inputs, "json"),
        asPythonString: formatBody(inputs, "python"),
        asTsString: formatBody(inputs, "ts")
      },
      providerInputs: {
        asObj: providerInputs,
        asCurlString: formatBody(providerInputs, "curl"),
        asJsonString: formatBody(providerInputs, "json"),
        asPythonString: formatBody(providerInputs, "python"),
        asTsString: formatBody(providerInputs, "ts")
      },
      model,
      provider,
      providerModelId: providerModelId ?? model.id
    };
    return inferenceSnippetLanguages.map((language) => {
      return CLIENTS[language].map((client) => {
        if (!hasTemplate(language, client, templateName)) {
          return;
        }
        const template = loadTemplate(language, client, templateName);
        if (client === "huggingface_hub" && templateName.includes("basic")) {
          if (!(model.pipeline_tag && model.pipeline_tag in HF_PYTHON_METHODS)) {
            return;
          }
          params["methodName"] = HF_PYTHON_METHODS[model.pipeline_tag];
        }
        if (client === "huggingface.js" && templateName.includes("basic")) {
          if (!(model.pipeline_tag && model.pipeline_tag in HF_JS_METHODS)) {
            return;
          }
          params["methodName"] = HF_JS_METHODS[model.pipeline_tag];
        }
        let snippet = template(params).trim();
        if (!snippet) {
          return;
        }
        if (client === "huggingface_hub") {
          const importSection = snippetImportPythonInferenceClient({ ...params });
          snippet = `${importSection}

${snippet}`;
        } else if (client === "requests") {
          const importSection = snippetImportRequests({
            ...params,
            importBase64: snippet.includes("base64"),
            importJson: snippet.includes("json.")
          });
          snippet = `${importSection}

${snippet}`;
        }
        return { language, client, content: snippet };
      }).filter((snippet) => snippet !== void 0);
    }).flat();
  };
};
var prepareDocumentQuestionAnsweringInput = (model) => {
  return JSON.parse(getModelInputSnippet(model));
};
var prepareImageToImageInput = (model) => {
  const data = JSON.parse(getModelInputSnippet(model));
  return { inputs: data.image, parameters: { prompt: data.prompt } };
};
var prepareConversationalInput = (model, opts) => {
  return {
    messages: (opts == null ? void 0 : opts.messages) ?? getModelInputSnippet(model),
    ...(opts == null ? void 0 : opts.temperature) ? { temperature: opts == null ? void 0 : opts.temperature } : void 0,
    max_tokens: (opts == null ? void 0 : opts.max_tokens) ?? 500,
    ...(opts == null ? void 0 : opts.top_p) ? { top_p: opts == null ? void 0 : opts.top_p } : void 0
  };
};
var snippets = {
  "audio-classification": snippetGenerator("basicAudio"),
  "audio-to-audio": snippetGenerator("basicAudio"),
  "automatic-speech-recognition": snippetGenerator("basicAudio"),
  "document-question-answering": snippetGenerator("documentQuestionAnswering", prepareDocumentQuestionAnsweringInput),
  "feature-extraction": snippetGenerator("basic"),
  "fill-mask": snippetGenerator("basic"),
  "image-classification": snippetGenerator("basicImage"),
  "image-segmentation": snippetGenerator("basicImage"),
  "image-text-to-text": snippetGenerator("conversational"),
  "image-to-image": snippetGenerator("imageToImage", prepareImageToImageInput),
  "image-to-text": snippetGenerator("basicImage"),
  "object-detection": snippetGenerator("basicImage"),
  "question-answering": snippetGenerator("basic"),
  "sentence-similarity": snippetGenerator("basic"),
  summarization: snippetGenerator("basic"),
  "tabular-classification": snippetGenerator("tabular"),
  "tabular-regression": snippetGenerator("tabular"),
  "table-question-answering": snippetGenerator("basic"),
  "text-classification": snippetGenerator("basic"),
  "text-generation": snippetGenerator("basic"),
  "text-to-audio": snippetGenerator("textToAudio"),
  "text-to-image": snippetGenerator("textToImage"),
  "text-to-speech": snippetGenerator("textToAudio"),
  "text-to-video": snippetGenerator("textToVideo"),
  "text2text-generation": snippetGenerator("basic"),
  "token-classification": snippetGenerator("basic"),
  translation: snippetGenerator("basic"),
  "zero-shot-classification": snippetGenerator("zeroShotClassification"),
  "zero-shot-image-classification": snippetGenerator("zeroShotImageClassification")
};
function getInferenceSnippets(model, accessToken, provider, providerModelId, opts) {
  var _a;
  return model.pipeline_tag && model.pipeline_tag in snippets ? ((_a = snippets[model.pipeline_tag]) == null ? void 0 : _a.call(snippets, model, accessToken, provider, providerModelId, opts)) ?? [] : [];
}
function formatBody(obj, format) {
  switch (format) {
    case "curl":
      return indentString(formatBody(obj, "json"));
    case "json":
      return JSON.stringify(obj, null, 4).split("\n").slice(1, -1).join("\n");
    case "python":
      return indentString(
        Object.entries(obj).map(([key, value]) => {
          const formattedValue = JSON.stringify(value, null, 4).replace(/"/g, '"');
          return `${key}=${formattedValue},`;
        }).join("\n")
      );
    case "ts":
      return formatTsObject(obj).split("\n").slice(1, -1).join("\n");
    default:
      throw new Error(`Unsupported format: ${format}`);
  }
}
function formatTsObject(obj, depth) {
  depth = depth ?? 0;
  if (typeof obj !== "object" || obj === null) {
    return JSON.stringify(obj);
  }
  if (Array.isArray(obj)) {
    const items = obj.map((item) => {
      const formatted = formatTsObject(item, depth + 1);
      return `${" ".repeat(4 * (depth + 1))}${formatted},`;
    }).join("\n");
    return `[
${items}
${" ".repeat(4 * depth)}]`;
  }
  const entries = Object.entries(obj);
  const lines = entries.map(([key, value]) => {
    const formattedValue = formatTsObject(value, depth + 1);
    const keyStr = /^[a-zA-Z_$][a-zA-Z0-9_$]*$/.test(key) ? key : `"${key}"`;
    return `${" ".repeat(4 * (depth + 1))}${keyStr}: ${formattedValue},`;
  }).join("\n");
  return `{
${lines}
${" ".repeat(4 * depth)}}`;
}
function indentString(str) {
  return str.split("\n").map((line) => " ".repeat(4) + line).join("\n");
}
function removeSuffix(str, suffix) {
  return str.endsWith(suffix) ? str.slice(0, -suffix.length) : str;
}
export {
  HfInference,
  INFERENCE_PROVIDERS,
  InferenceClient,
  InferenceClientEndpoint,
  InferenceOutputError,
  audioClassification,
  audioToAudio,
  automaticSpeechRecognition,
  chatCompletion,
  chatCompletionStream,
  documentQuestionAnswering,
  featureExtraction,
  fillMask,
  imageClassification,
  imageSegmentation,
  imageToImage,
  imageToText,
  objectDetection,
  questionAnswering,
  request,
  sentenceSimilarity,
  snippets_exports as snippets,
  streamingRequest,
  summarization,
  tableQuestionAnswering,
  tabularClassification,
  tabularRegression,
  textClassification,
  textGeneration,
  textGenerationStream,
  textToImage,
  textToSpeech,
  textToVideo,
  tokenClassification,
  translation,
  visualQuestionAnswering,
  zeroShotClassification,
  zeroShotImageClassification
};
//# sourceMappingURL=@huggingface_inference.js.map
